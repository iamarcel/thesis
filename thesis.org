# -*- org-confirm-babel-evaluate: nil -*-
#+TITLE: Autonomous Production of Gestures on a Social Robot using Deep Learning
#+AUTHOR: Marcel Samyn
#+COLUMNS: %4TODO(To Do) %40ITEM(Task)  %12EFFORT(Effort){:}
#+OPTIONS: tasks:nil ':t H:5
#+LATEX_CLASS: report-noparts
#+LATEX_HEADER_EXTRA: \usepackage{animate}
#+LATEX_HEADER: \input{ugent.tex}
#+LATEX_HEADER: \input{glossary.tex}

#+BEGIN_SRC emacs-lisp :exports none :session python-env
   ;; If you have the pipenv package, this initalializes the environment so that
   ;; Python packages are all present.
   (pipenv-mode)

   ;; Set to t to export some figures as animations in the PDF
   (setq do-fancy-export nil)
#+END_SRC

#+NAME: pgf_figure
#+BEGIN_SRC latex :var var_name="" var_caption="" :exports none
  \begin{figure}
    \centering
    \adjustbox{max width=1.2\linewidth, center}{%
      \input{./img/var_name.pgf}%
    }
    \caption{\label{fig:var_name} var_caption}
  \end{figure}
#+END_SRC



* Notes :noexport:
  
** DONE Leg uit in de thesis: in het begin is het belangrijk dat we heel monotone beelden gebruiken
   CLOSED: [2018-07-28 za 08:14]

** DONE Voorlopig is het waarschijnlijk best dat je de monologen gebruikt. Vermeld de use-case:monoloog voor een publiek
   CLOSED: [2018-07-28 za 08:16]

** DONE Leg uit hoe je nieuwe trainingsdata kan maken
   CLOSED: [2018-07-28 za 10:00]

** CANCELLED Erken probleem in de clustering: de gebaren die er uit komen zin /gemiddelden/ (lauwe gebaren)
   CLOSED: [2018-07-28 za 10:00]
   - State "CANCELLED"  from              [2018-07-28 za 10:00] \\
     Geen gemiddelden maar echte elementen van de dataset
   Iconische gebaren verdwijnen met deze methode uit het repertoire van de robot.

** DONE Vermeld dat bepaalde heel belangrijke elementen die we willen hebben, worden uitgemiddeld
   CLOSED: [2018-07-28 za 10:18]

*** Mogelijkheid: dataset biasen met extra trainignsdata

*** In RNN kan je bijvoorbeeld een veel hardere gradient met voor die specifieke woorden geven (1/0.03)

** DONE Meet de afstanden van de clusters tot de ground truth (niet alleen klassen)
   CLOSED: [2018-07-28 za 10:19] DEADLINE: <2018-07-02 ma>
   Toon dat die afstand kleiner wordt. Dus twee evaluaties:

   1. Toon dat er geen bug zit in je algoritme, dat ze wel dichter komen bij de trainingsdata
   2. Tonen met mensen

** Vergelijk gelijkaardige zinnen
   maak een 100-tal paren van zinnen die wel/niet op elkaar lijken en vergelijk
   dat met de output van je algoritme.
   
   Bijvoorbeeld: cluster uw zinnen en kijk of daar iets in zit

** Vraag voor mezelf: hoe kan ik meer tussentijds cijfers geven over hoe goed het werkt?

** DONE 2-10 juli is buffer voor het extra werk dat Tony mee geeft
   CLOSED: [2018-07-28 za 10:19]

** Data storage
   - src
   - data
     - clusters.json: { class: frames[] }
* Development                                                      :noexport:
** DONE [#A] Maak precieze planning wat je nog moet doen
   CLOSED: [2018-06-21 do 11:51]
** DONE [#A] Stuur planning door
   CLOSED: [2018-06-21 do 18:27] DEADLINE: <2018-06-21 do>
** DONE Create evaluation questionnaire
   CLOSED: [2018-06-26 di 08:18] DEADLINE: <2018-06-27 wo> SCHEDULED: <2018-06-25 ma>--<2018-06-26 di>
   :LOGBOOK:
   CLOCK: [2018-07-03 di 14:34]--[2018-07-03 di 15:06] =>  0:32
   CLOCK: [2018-06-25 ma 15:47]--[2018-06-25 ma 17:53] =>  2:06
   CLOCK: [2018-06-25 ma 09:25]--[2018-06-25 ma 11:57] =>  2:32
   :END:
*** DONE Create a script to generate a TTS audio clip
    CLOSED: [2018-06-26 di 08:18]
    :PROPERTIES:
    :Effort:   3:00
    :END:
*** DONE Create comparison video (x6)
    CLOSED: [2018-06-26 di 08:18]
    :PROPERTIES:
    :Effort:   1:00
    :END:
    :LOGBOOK:
    CLOCK: [2018-06-23 za 13:24]--[2018-06-23 za 15:12] =>  1:48
    :END:
**** DONE Pick a random subtitle
     CLOSED: [2018-06-23 za 15:12]
**** DONE Generate TTS audio
     CLOSED: [2018-06-23 za 15:12]
**** DONE Record video clips
     CLOSED: [2018-06-23 za 15:12]
***** DONE Play back original gesture
      CLOSED: [2018-06-23 za 15:12]
***** DONE Play back NAO's generated gesture
      CLOSED: [2018-06-23 za 15:12]
***** DONE Play back chosen cluster
      CLOSED: [2018-06-23 za 15:12]
**** DONE Merge video clips
     CLOSED: [2018-06-23 za 15:12]
**** DONE Add audio clip to video
     CLOSED: [2018-06-23 za 15:12]
**** DONE Add subtitles to video
     CLOSED: [2018-06-23 za 15:12]
*** DONE Upload videos
    CLOSED: [2018-06-26 di 08:18]
    :PROPERTIES:
    :Effort:   1:00
    :END:
*** DONE Create questions for all videos
    CLOSED: [2018-06-26 di 08:18]
    - Embedded video
    - Score each
    - Which do you prefer?

**** TODO Duplicate previous question
**** TODO Replace video
*** TODO Add question: attention check
** DONE Try out the Java clustering algorithm
   CLOSED: [2018-06-26 di 16:18]
   :LOGBOOK:
   CLOCK: [2018-06-26 di 08:18]--[2018-06-26 di 12:04] =>  3:46
   :END:
** DONE Try other ways of clustering
   CLOSED: [2018-07-07 za 16:22]
   :LOGBOOK:
   CLOCK: [2018-07-03 di 09:08]--[2018-07-03 di 12:15] =>  3:07
   CLOCK: [2018-07-02 ma 19:16]--[2018-07-02 ma 20:30] =>  1:14
   CLOCK: [2018-07-02 ma 17:12]--[2018-07-02 ma 18:14] =>  1:02
   CLOCK: [2018-07-02 ma 16:39]--[2018-07-02 ma 16:45] =>  0:06
   CLOCK: [2018-06-27 wo 08:06]--[2018-06-27 wo 11:24] =>  3:18
   CLOCK: [2018-06-26 di 18:15]--[2018-06-26 di 18:26] =>  0:11
   CLOCK: [2018-06-26 di 16:18]--[2018-06-26 di 17:33] =>  1:15
   :END:
** DONE Send out questionnaire
   CLOSED: [2018-07-27 vr 18:44]
** DONE Record a video of a live NAO gesturing
   CLOSED: [2018-07-27 vr 18:45]
** TODO Record video of NAO telling a story
** DONE Improve sequence model
   CLOSED: [2018-07-20 vr 15:04]
   Hmm. Adding droput in the sequence decoder, right after the initial RNN cell,
   increases the max loss by 10x. Even if the dropout is 0. It then produces
   output independent of the subtitle, though. Hmm, maybe I did something wrong
   in the inference loop function.

*** DONE Add a mask dimension to the data
    CLOSED: [2018-07-20 vr 15:04]
*** DONE Stop predicting when mask says so
    CLOSED: [2018-07-20 vr 15:04]
*** DONE Eigen embedding
    CLOSED: [2018-07-20 vr 15:04]
**** DONE Maak een per-woord vocab
     CLOSED: [2018-06-19 di 15:26]
**** DONE embed woorden in vocab
     CLOSED: [2018-06-21 do 08:24]
**** DONE Encode die sequentie
     CLOSED: [2018-06-21 do 08:24]
**** DONE Decode + geef tussen-states mee aan decoder
     CLOSED: [2018-07-20 vr 15:04]

* TODO Preface
  :PROPERTIES:
  :UNNUMBERED: t
  :Effort:   2:00
  :END:

* Abstract
  :PROPERTIES:
  :UNNUMBERED: t
  :Effort:   2:00
  :END:

  When humanoid robots interact with people in an intuitive manner, these robots
  are perceived as more likable, communicate more effectively and do not need
  training to work with. When they adapt to the social norms of humans,
  interaction happens seamlessly. Body language is a crucial component of
  human-human communication and implementing nonverbal cues for robots is a key
  research goal in the field of human-robot interaction.

  Existing methods for gesture synthesis either require detailed manual
  descriptions of gestures or are generated randomly and thus lack context,
  meaning or intention. This thesis approaches this task from the angle of
  machine learning and uses a deep-learning approach to automatically learn and
  generate gestures for a given part of text to be pronounced by the robot.
  
  First, a dataset is constructed from online video sources by using the
  available subtitles and using pose estimation techniques to extract the
  gestures. Second, a deep learning model is trained to predict these gestures,
  either directly or by predicting one of several gesture clusters whom are
  created with an unsupervised clustering algorithm. Finally, since human
  perception is difficult to measure quantitatively, a survey was conducted to
  evaluate respondent's perception of humanness on the synthesized gestures.

  Results show that TODO. While these results are preliminary, this method could
  lay the foundation for future developments, tackles the intial hurdle of a
  lack of training data and shows next steps in the path ahead.
  
** TODO Add in results
* Introduction
  :PROPERTIES:
  :Effort:   1:00
  :END:
  :LOGBOOK:
  CLOCK: [2018-07-07 za 16:23]--[2018-07-07 za 17:23] =>  1:00
  :END:

   For at least 2.6 million years, humans have been making tools. The ability to
   create sophisticated tools allowed us to find and process more energy-rich
   food sources which gave us the energy to evolve bigger, more slowly growing
   brains. That, in turn, improved our ability to invent new and more complex
   tools, creating the feedback loop that quickly made humans the dominant
   animal species on earth cite:lieberman12.

   The story of human toolmaking started with the humble Pointy Rock. The Pointy
   Rock was crafted by smacking together two less pointy rocks repeatedly so
   that at least one of them would start to chip off and become sharper. While
   crude compared to today's strict engineering practices and tolerances, the
   Pointy Rock helped our ancestors a tremendous amount. With this tool, one
   could kill prey, break bones to access valuable marrow and pulverize plants
   so they could be more easily digested.

   Since that point, humans have never stopped building tools. The desire to
   craft things that make our lives easier is deeply embedded in the human
   condition. In an ever-accelerating feedback loop, people have built tools
   with tools to build better tools.

   A prime motivator for making increasingly complex machines was (and still is)
   the desire for machines that could run completely autonomously. In the past
   few millenia machines that operated autonomously had been invented, like
   water clocks and Japanese Karakuri glspl:automaton, but these were mostly
   for display purposes and of little practical value. Widespread, real-life use
   of automation only started with the introduction of mechanized spinning
   machines in the 1780s during the industrial revolution
   cite:britannica-industrial-revolution. These machines could operate
   autonomously with no or limited interaction with people and laid the
   foundation for the dramatic increase in productivity of the past few
   centuries.

   One more fundamental feature---the ability to /reprogram/ these
   machines---came with the Unimate, a machine that would today be distinctly
   recognizable as an industrial machine cite:robotics_unimate. It was the first
   digitally operated, programmable machine and this ability for it to be
   reprogrammed is why the Unimate can be confidently identified as a /robot/.
   Industrial and commercial robots are widespread today and excel at various
   tasks like car assembly, package sorting and vacuuming. More recently, robots
   that can perform perform complex human tasks like driving make headlines in
   technology news cite:wired-self-driving.

   Though industrial and specialized robots are useful, nothing captures our
   attention more than a human-like machine. The idea of artificial humans can
   be found in centuries-old legends like the Greek Talos, a bronze man that
   defended Crete or the clay golems of the Jews and Nordics. But it was not
   until the 1930s that real robots entered our popular culture. It started with
   not much more than the ancient glspl:automaton, simple humanlike machines
   that could perform a few "tricks", but development steadily continued and
   robots learned to walk, talk and interact with their environment.

   One of the most popular robots today, Pepper from Softbank Robotics (see
   cref:fig:pepper), can make eye contact, read emotions from people's faces and
   adapt its behavior based on the moods of people it is talking with. Notable
   about these features is that they are /social/, they are built so that the
   robot can interact with the people in its environment in an active manner.
   Next to the challenges of locomotion (i.e., not falling and moving at an
   acceptable speed) and making robots /look/ like humans, making robots
   /behave/ like humans is a tremendous challenge.

*** DONE Reference for when automation started
    CLOSED: [2018-06-09 za 08:35]

** Social Robots
   
   #+CAPTION: label:fig:pepper Softbank's social robot Pepper, one of the most popular advanced robots today.
   #+NAME: fig:pepper
   #+ATTR_LATEX: :width 0.323\textwidth :float wrap :placement {L}[2cm]{0.4\textwidth}
   [[file:./img/pepper.jpg]]

   Social robots can interact and communicate with humans by following the
   behavioral norms that their conversational partners expect cite:bartneckil.
   The power of these kinds of robots lies in the fact that the prerequisites
   for fluent interaction are essentially reversed: the human no longer needs to
   learn how to interact with the machine; rather, the machine learned how
   people naturally operate so that they can interact without any special skills
   or training.

   These robots have the potential to become our assistants and trusted
   sidekicks. In the form of humanoid nurses, smart toys or even small creatures
   to be carried around cite:breazeal04_desig, these machines would understand
   us intuitively, anticipate our needs and seamlessly integrate in our social
   world.

   Modern robots are starting to look more humanlike and gaining basic human
   capabilities like the ability to walk, speak, see, listen and move objects.
   However, these are still rather technical foundations and a lot more effort
   is needed to let these machines communicate effortlessly with people.
   Creating the ability to interact socially is not easy. Concepts like body
   language and emotion---that have evolved over for millions of years in humans
   and are still an active research topic in psychology---have to be programmed
   into computers who are inherently built to act in a rational, logical and
   determinstic manner.

   However, that is not to say that this is impossible or far away in the
   future. Many robots exist today which vary in approach and ability to be
   social. Developed at the Massachusetts Institute of Technology by a team led
   by Dr.Â Cynthia Breazeal, the robotic head /Kismet/ was one of the earliest
   examples of a social robot. The developers knew that building a robot that
   behaves realistically like a human adult would be impossible at that point so
   Kismet was designed to appear and behave more like a baby. It could hear and
   speak but interpreted the /emotion/ of what was being said and spoke in a
   kind of proto-language similar to infants. This way, people interacting with
   Kismet naturally talked slower and were more expressive in their voice:
   Kismet managed to intuitively define the social context in which it could
   operate well. The goal of allowing interaction without training the user was
   achieved while the robot could still use its method of communication, in a
   way that was almost unnoticed by the people interacting with Kismet
   cite:breazeal04_desig. This appeared to work: people formed an emotional
   connection to the robot and enjoyed interacting with it.
   
   #+CAPTION: label:fig:kismet Kismet is a social robot that presents itself as an infant, to which people intuitively react by being more expressive in their voice and talking more slowly. This is precisely what Kismet's computer system needed to work well.
   #+ATTR_LATEX: :width 0.323\textwidth :float wrap :placement {R}[2cm]{0.5\textwidth}
   [[file:./img/kismet.jpg]]

   Two of the most popular social robots today are SoftBank's NAO and Pepper
   cite:softbank-robotics. These robots can understand and talk to people,
   recognize their emotion and are used in a broad range of places like
   introductory classes for STEM education and hotel lobbies. NAO is about 60
   centimeters high and can walk on his feet, while Pepper is 1.20 meters high
   and moves around using three wheels under its "skirt" (See cref:fig:pepper).

   Robots like NAO and Pepper try to be a part of our world by being present in
   public places around groups of people while still being very clear about
   their identity as a robot. There are also social robots that take this one
   step further where they actually try to appear indistinguishable from
   humans. So far, these robots are still in what is called the /uncanny
   valley/---a very high level of realism that is eerie because it is not yet
   /exactly/ human-like.

   It takes little effort to appreciate the complexity and amount of mechanisms
   at play when people communicate with each other. We can infer meaning and
   intention in a split second, quickly learn and reason inductively and adapt
   our communication style to our conversation partner. Now, imitating a few
   million years' worth of evolution is no small undertaking but the closer we
   get to communicating in a human-like way, the better we will be able to work
   with machines cite:adalgeirsson10_mebot,huang13_model_evaluat_narrat_gestur_human_robot.

** Why Gesture?

   Building machines which are modeled after human form and behavior is called
   /antropomorphic design/. This is important to support an intuitive and
   meaningful interaction with humans cite:breazeal04_desig and a key component
   of antropomorphism is animacy or aliveness
   cite:bartneck08_measur_instr_anthr_animac_likeab. People's perception of
   animacy is greatly influenced by the amount and type of motion they perceive
   in an object---as shown, for example, in Heider and Simmel's work
   cite:heider44_exper_study_appar_behav. Indeed, motion is a prerequisite for a
   perceived notion of aliveness.

   In situations with both virtual agents and humanoid robots it has been shown
   that speech-accompanying non-verbal behaviors have a positive effect on
   antropomorphism, likeability and future contact intentions---key objectives
   in the field of Human Robot Interaction (HRI)
   cite:bremner16_iconic_gestur_robot_avatar_recog,salem13_to_err_is_human,adalgeirsson10_mebot.
   Congruent gesture improves task performance
   cite:kramer16_nonverbal_mimicry,mamode13_cooper but even incongruent
   gesturing increases people's evaluation of a robot's human-like qualities
   cite:huang13_model_evaluat_narrat_gestur_human_robot.

   A speaker's gestures bear little structure nor are they produced or
   interpreted consciously, yet they still convey information between the
   collocutors. Gesturing is in fact beneficial to both the speaker and the
   listener: it helps the speaker think and helps the listener understand this
   thinking---even for people who are not trained in understanding these
   gestures cite:goldin-meadow99_role_gestur_commun_think,mcneill95_hand.

   This presents opportunities to significantly improve the quality of
   communication between humans and machines. First, human-like motion improves
   people's perception of the robot. Second, gesturing can provide additional
   information that is not conveyed in speech and improve the quality of
   communication. Third, communicating on an intuitive level reduces the need
   for training people who need to work with these robots.

** Current State of Robot Gesture Synthesis
   label:sec:state-robot-synthesis

   To understand the state of current gesture synthesis technologies, one can
   look at both gesture synthesis in robot and in virtual agents. Translating
   the motion of such an agent to a live robot is challenging but possible
   cite:Salem2012.

   Three desirable properties for an effective gesture synthesis are proposed:

   *Continuity.* The avatar keeps moving. If a humanoid robot or avatar is
   motionless even for a small amount of time, people can think it is crashing
   and thus stop seeing the avatar as a being that is alive.

   *Variability.* The avatar should be able to perform gestures for any
   interaction.

   *Congruence.* The gestures performed should have some relationship to the
   semantics of the text that is  being spoken. For example, extreme cases like
   nodding while the avatar says "no" should be avoided.

   In current research and industry, these are popular approaches for gesture
   synthesis:

   /The gestures are pre-recorded or otherwise pre-determined./ This could be by
   manually animating the robot for specific sentences or by annotating text
   files with the gestures which should be performed and when
   cite:neff08_gestur_model_animat_based_probab,Kipp2007,kopp04_synth_multim_utter_conver_agent.
   This can produce natural results but is very labor-intensive and not suited
   for the large amount of interactions a humanoid robot might have. This method
   succeeds at /Continuity/ and /Congruence/ but fails for the /Variability/
   requirement. Additionally, creating these animations or annotations is
   costly.

   /Gestures are generated randomly./ They might be chosen from a repertoire of
   movements and then stiched together or be completely random altogether.
   Often, this method introduces noticeable stuttering and might produce
   gestures that are inconsistent with the content of the spoken text, which
   is confusing to the person listening. An improvement for this method is
   adding fixed motions for specific keywords, which introduces the problems
   of pre-recording again. Random gestures allow /Variability/ but have
   difficulty with /Congruence/ and /Continuity/.

   /Gestures are generated from a set of rules cite:ng-thow-hing10_synch./ The
   gesture synthesis system analyzes the content of the text that will be
   pronounced and chooses a category of gesture for each text part. Then,
   category-specific rules are applied (such as matching for a keyword or parts
   of words) with some randomness to generate the final gestures. In principle,
   this system can allow all three desired properties but at a high cost for
   creating the gesture generation rules. To create this kind of system, it is
   necessary to perform social studies that examine how humans gesture and try
   to extract general rules.

   Neither of these solutions are ideal. In a truly social robot, the gesture
   synthesis system should be able to generate these gestures for arbitrary text
   (so that the robot can be reprogrammed) and still look natural---just like
   humans can say things they have never said before and still look alive.

   So how /do/ people gesture? What can we learn from research in psychology
   that could help us build a better system for gesture synthesis? Do we need
   different approaches to synthesize the different kinds of gestures people
   perform?

** How People Gesture

   In his classical work on human gesturing, McNeill argues that gesture and
   speech are created in concert; they are neither used as an addition to
   speech, nor a translation of it, nor are both modalities produced independently
   cite:mcneill95_hand.

   As mentioned previously speech-accompanying gesture is largely unstructured,
   but not completely. Some gestures /are/ interpreted consciously, for example
   when pointing at the location of an object that is being talked about.
   McNeill proposes four categories of gesture cite:cassell_1998,mcneill95_hand:

   - Iconic gestures :: literally depict an object or action that is being
        described. For example, spreading your arms while saying "big" or
        standing on your toes while explaining a ballet move.
   - Metaphoric gestures :: also represent something but not directly, for
        example, making a rolling motion with the hands while saying "the
        meeting went on and on."
   - Deictic gestures :: reference positions in space. For example, pointing at
        the bus stop when directing someone.
   - Beat gestures :: are not closely related to the content of the
                      communication but rather are used to emphasize words or to
                      clarify the structure of a sentence. For example, holding
                      the hands together left from the body in the first part of
                      a sentence, then moving both of them to the right as you
                      transitions to the second part of the sentence.

   Iconic and metaphoric gestures are perhaps the most straightforward of these
   from the point of view of gesture synthesis. In order to produce these, one
   could build a "gesture dictionary" that associates specific words or parts of
   sentences with gestures. To add more variability, some randomness could be
   added in the form of alternative gestures or noise. Note however that these
   types of gestures especially can vary across culture: a "V for victory" with
   the palm facing the gesturer is considered offensive in British culture
   cite:archer97.

   Deictic gestures reveal information that is not present in speech and
   generating these gestures would thus require semantic information along with
   the words that are being spoken. Once that information is present, though,
   generating deictic gestures is straightforward.

   Beat gestures make up the biggest part of all gestures (almost half, followed
   closely by iconic gestures) cite:mcneill95_hand but do not directly
   correspond to the content of the communication, making these difficult to
   generate procedurally. Subsequently, this type of gesture has not been
   focused on much in gesture synthesis research. Yet in order to build a robot
   that would move naturally it seems reasonable to start with the most-occuring
   type of gesture---perhaps this category alone is enough in order to make the
   robot seem alive.

** Synthesizing Gestures with Deep Learning

   # Imagining the ideal gesture synthesis system in a robot, it could then have
   # the following architecture: both speech and gesture are generated
   # simultaneously, with access to information about the robot's intent and
   # contextual information like, possibly, the positions of objects to be pointed
   # at, cultural background or the emotional context. It could then use beat
   # gestures as a baseline for its movement and combine those with iconic,
   # metaphoric and deictic gestures using information from the context to make
   # more precise gestures.
   
   The nature of this task is in some sense very similar to that of other
   problems where intuitive human abilities are to be imitated like speech
   synthesis, bipedal locomotion and image recognition. In all of these tasks,
   machine learning-based approaches have proven to be very successful
   cite:hintin-need-ml so adopting a similar approach here seems promising.

   Especially the generation of beat gestures might benefit from a deep learning
   approach: a neural network could learn a general sense of how people move,
   which can be used as a starting point for other methods to add their more
   specific gestures to (like deictic ones) or it might even be able to learn
   iconic and metaphoric gestures given enough of the right data is present.
   
   Based on this premise---the power of machine learning---this thesis proposes
   a novel system for gesture synthesis, creates a proof of concept and reports
   on the initial results. This system uses a deep learning-based approach to
   synthesize gestures for a robot to perform while it is talking based on the
   content of its spoken words.

   Cref:chap:literature-research covers a more detailed analysis of gesture
   synthesis methods, explores recent advances in machine learning and provides
   background information on the technologies and methods used throughout the
   process. Cref:chap:method explains the process used in this project in
   detail, while cref:chap:evaluation evaluates the results. Finally,
   cref:chap:conclusion concludes this thesis and provides opportunities for
   future work.
    
** TODO Checklist
   - [ ] Context: Where does this fit in the state of the art?
   - [ ] Need: Why should it be done?
   - [ ] Task: What was done?
   - [ ] Object: What does the document cover?

* Literature Research
  label:chap:literature-research

  Every great dish starts with a set of fresh, high-quality ingredients. This
  chapter takes a walk through the proverbial grocery store and collects the
  elements needed to build this system. After a quick look at existing recipes
  for gesture synthesis in
  cref:sec:gesture-synthesis-robots,sec:gesture-synthesis-va, all the components
  necessary for this project are sourced and examined. First, in
  cref:sec:ml-research, recent advances in machine learning and, in particular,
  deep learning methods, are explored to find which types of architectures could
  be able to produce the results we are looking for. Then,
  cref:sec:research-dataset attempts to find the right dataset but will find
  that this does not exist yet, resulting in a search for a method to create it.
  Finally, cref:sec:research-clustering looks at an alternative approach that
  might dramatically simplify the problem.

** Gesture Synthesis in Robots
   :PROPERTIES:
   :Effort:   0:30
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-07 za 17:23]--[2018-07-07 za 18:06] =>  0:43
   :END:
   label:sec:gesture-synthesis-robots

   Cref:sec:state-robot-synthesis evaluated the current gesture synthesis
   systems qualitatively, concluding that the methods investigated lack in one
   or more of the desired properties of continuity, variability and congruence.
   Below, the inner workings of two gesture synthesis---those of the Kismet and
   SoftBank robots---are explained in more detail.

   Kismet's range of gestures is limited: it can only move its face actuators
   and move its head with the neck motors. Its movements are organized into
   /skills/, each of which is a finite state machine of positions where a
   transition is a certain gls:motion-primitive, a unit of gesture. These skills
   and the transitions between them are activated by the robot's other
   behavioral systems and external stimuli as specified in the finite state
   machine cite:breazeal04_desig.
   
   While the task of gesture generation can be applied to any robot, this thesis
   focuses on SoftBank's robots NAO and Pepper because they were easily
   accessible to work with. SoftBank provides developers a Python API and
   software package /Choregraphe/ cite:softbank_tools which includes a visual
   programming environment and robot simulator. This way, the results can be
   tested on a virtual robot quickly. Performing these gestures on a physical
   robot is as simple as changing the connection from the simulator to the real
   robot.

   SoftBank's robots all use the same software framework and API, /NAOqi/
   cite:softbank_naoqi. This framework includes a few modules that regulate
   their autonomous life cite:naoqi_autonomous_life:

   - ALAutonomousBlinking :: makes the robot blink its eyes (flash the LEDs
        around its eyes).
   - ALBackgroundMovement :: makes the robot make slight movements when it is
        idle and runs a breathing animation.
   - ALBasicAwareness :: makes the robot look at people's faces when it sees
        them, hears them or notices when they touch it.
   - ALListeningMovement :: makes the robot move slightly when it is listening.
   - ALSpeakingMovement :: controls how the robot moves when it is talking.
        There are two modes for this module: /random/ launches random animations
        and /contextual/ launches specific animations for certain keywords and
        fills in the rest with random animations.

   Note that not all of these run simultaneously. For example, the
   /BackgroundMovement/'s breathing animation does not run when the
   /ListeningMovement/ or /SpeakingMovement/ is active.

   A developer has some control over these movements, like enabling and
   disabling them or changing the mode of speaking movement, but these systems
   are fairly limited in their expressive capability. In public appearances of
   SoftBank robots, their movements are often animated manually and thus do not
   use these autonomous capabilities.

*** DONE Systems in NAO(qi), Kismet
    CLOSED: [2018-06-21 do 08:25]
** Gesture Synthesis in Virtual Agents
   :PROPERTIES:
   :Effort:   0:15
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-10 di 09:51]--[2018-07-10 di 10:15] =>  0:24
   CLOCK: [2018-07-09 ma 17:35]--[2018-07-09 ma 18:11] =>  0:36
   :END:
   label:sec:gesture-synthesis-va

   One of the most advanced gesture synthesis systems is the Articulated
   Communication Engine (ACE) cite:kopp04_synth_multim_utter_conver_agent. In
   this system, one annotates the text to be spoken with gestures and how they
   should be timed in an XML language called MURML (see
   cref:fig:ace-murml-example for an example). The ACE system combines the
   information from the text-to-speech engine with the gestures and information
   given in the speech/gesture definition, allowing it to create movements that
   are well-timed with the text being spoken (for example, stretching the arm
   while saying "there"). All the gestures that appear in a specification are
   combined so that the whole looks like a singular movement.
   
   The gestures produced by ACE are continuous and precise. However, they
   require extensive metadata accompanying the speech. When no behavior
   specification is defined, the avatar does not move. This makes the ACE system
   useful when a high level of precision is required, such as for deictic
   gestures, but less for free-form text.
   
   #+NAME: fig:ace-murml-example
   #+CAPTION: label:fig:ace-murml-example Example of MURML multi-modal gesture and speech specification.
   #+LABEL: fig:ace-murml-example
   #+attr_latex: :width center
   #+BEGIN_SRC xml :exports code
     <utterance>
       <specification>
         And now take <time id="t1"/> this bar <time id="t2" chunkborder="true"/>
         and make it <time id="t3"/> this big. <time id="t4"/>
       </specification>

       <behaviorspec id="gesture_1">
         <gesture>
           <affiliate onset="t1" end="t2" focus="this"/>
           <function name="refer_to_loc">
             <param name="refloc" value="$Loc-Bar_1"/>
           </function>
         </gesture>
       </behaviorspec>

       <behaviorspec id="gesture_2">
         <gesture>
           <affiliate onset="t3" end="t4"/>
           <constraints>
             <symmetrical dominant="right_arm" symmetry="SymMS">
               <parallel>
                 <static slot="HandShape" value="BSflat (FBround all o) (ThCpart o)"/>
                 <static slot="ExtFingerOrientation" value="DirA"/>
                 <static slot="PalmOrientation" value="DirL"/>
                 <static slot="HandLocation" value="LocLowerChest LocCenterRight LocNorm"/>
               </parallel>
             </symmetrical>
           </constraints>
         </gesture>
       </behaviorspec>
     </utterance>
   #+END_SRC

   The BodySpeech system was developed to remove the need to specify which
   gestures have to be chosen. It uses an audio clip of a recorded voice,
   analyzes its intensity in segmented parts of speech, chooses from a set of
   motion-captured gestures the one that most closely aligns with that part of
   speech and then blends between those movements cite:Fernandez:2013.
   
   Interesting references for realistic gesture synthesis systems can be found
   in 3D animated movies or video games. Movies are mostly manually animated but
   provide a point of reference---not only in the sense that the people in these
   movies move in a way we recognize as being human, but also in how these
   movements are often purposely not precisely imitated from humans. Animators
   understand some principles of aliveness and manipulate or exaggerate gestures
   to convey emotional content.
   
   Modern video games present many in-game avatars that have to move in a
   realistic manner yet not all move in the same way. This means they face
   similar challenges and try to generate animation instead of expensive motion
   capture by actors. In practice, these avatars often have a few keypoints
   animated manually. The animation for the rest of the avatar is then generated
   using physics-based engines that take into account the biomechanics of humans
   cite:deepmotion_avatar, or a base animation is created manually or via motion
   capture and some variations are generated automatically
   cite:2013-SCA-diverse.

*** DONE ACE, video game engines
    CLOSED: [2018-06-21 do 08:25]
** Recent Advances in Machine Learning
   :PROPERTIES:
   :Effort:   3:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-10 Tue 18:53]--[2018-07-10 Tue 19:07] =>  0:14
   CLOCK: [2018-07-10 di 16:34]--[2018-07-10 Tue 18:24] =>  1:50
   CLOCK: [2018-07-10 di 15:05]--[2018-07-10 di 15:34] =>  0:29
   CLOCK: [2018-07-10 di 11:10]--[2018-07-10 di 12:00] =>  0:50
   CLOCK: [2018-07-10 di 10:26]--[2018-07-10 di 10:54] =>  0:28
   :END:
   label:sec:ml-research

   Over the past ten years, tremendous progress has been made in the field of
   machine learning. With the invention of effective training algorithms such as
   the backpropagation algorithm and stochastic gradient descent, along with the
   exploitation of GPUs, we now have the capability to process more data orders
   of magnitude more quickly with algorithms that are more effective
   cite:nvidia-ai-computing.
   
   With these improvements in performance the possibility arrived to train large
   neural networks, machine learning algorithms that automatically learn
   abstract representations of their data. This alleviated the need for manual
   feature engineering, which is time-consuming and requires extensive domain
   knowledge. Many complex problems such as object recognition, speech synthesis
   and machine translation are dominantly being tackled with deep neural
   networks cite:lecun15_deep_learn.

*** Neural Networks
    
    Neural networks are large structures of deceptively simple components, often
    called /neurons/, /nodes/ or /cells/ that from the foundation of the most
    powerful machine learning methods to date. Mathematically a neural network
    is nothing more than a specific kind of function, with a set of parameters
    that can be optimized, where the goal is to approximate another---yet
    unknown---function. In this case, the neural network should approximate "the
    function that returns a gesture for a given subtitle." The way such networks
    are visualized, though, is what warrants their name.

    A neuron, the building block of a neural network, is visualized in
    cref:fig:neuron. It performs two operations $f$ and $g$, respectively a
    linear combination of some trainable (i.e., that will be optimized)
    /weights/ and its inputs (in this case with weights $\vec{w}$ and input
    $\vec{x} = x_i, i \in \{0, 1, 2, 3, 4\}$) and a non-linear /activation
    function/ $g$ (often the Rectified Linear Unit or ReLU, $x \mapsto \max(0,
    x)$). Often, the activation function is not shown separately but is instead
    assumed to be a part of $f$.
    
    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}[h]
        \adjustbox{center}{%
          \begin{tikzpicture}

            \node [block, center coordinate] (neuron) at (\gridsize, -2) {$f$};
            \node [variable block, above = \smallgridsize of neuron] (weights) {$\vec{w}$};
            \draw [->] (weights) -- (neuron);

            \foreach \i in {0, 1, 2, 3, 4} {
              \node (i-\i) at (0, -\i\smallgridsize) {$x_\i$};
              \draw[->] (i-\i) -- (neuron);
            }

            \draw[->] (neuron) -- ++(2\smallgridsize, 0) node (activation) [block] {$g$};
            \draw[->] (activation) -- ++(2\smallgridsize, 0);
          \end{tikzpicture}%
        }
        \caption{\label{fig:neuron} A neuron is the simple bulding block of a neural network.}
      \end{figure}
    #+END_SRC
    
    One /layer/ of neurons is then a set of neurons that share the same inputs
    but have different weights. In a deep neural network, a large amount of
    layers are present where the outputs of the neurons in one layer are used as
    inputs in the next layer.

*** Recurrent Neural Networks

    The last two problems mentioned earlier, speech synthesis and machine
    translation, require an extension of "plain" neural networks because they
    must produce a /sequence/ of features (e.g., sound samples or words) which
    can have a variable length.

    In order to be able to read or produce a sequence, the cells in the neural
    network need some kind of memory. The simplest way to do this is to give
    cells two inputs and outputs: the usual input/output and a state vector.
    The cell then computes:

    1. The state, based on the input and the previous state
    2. The current output, based on the state

    Both these computations are, as in other neural networks, linear
    combinations with some learned weights. This computation is repeated for
    each element in the sequence with the same weights. A network composed of
    this kind of cells is called a gls:rnn. The simplest one only has a single
    cell and performs a single "loop" at each time step, using as state input
    the state output of the previous step.

    In order to see how this roughly corresponds with a regular (deep) neural
    network and how it is implemented in practice, remark that in practice,
    these networks are /unrolled/ in time: instead of a looping construct, all
    iterations are explicitly "written down" so that the network no is now deep
    and narrow instead of truly recurrent. All the nodes in each layer then
    share the same weights.

    Cref:fig:rnn shows a high-level diagram of an gls:rnn, where two time
    steps are unrolled. In this figure and further diagrams, the convention will
    be adopted that 
    #+latex: \tikz[baseline]{\node[block,inline spacing,anchor=base]{grey rounded rectangles};} 
    represent operations and 
    #+latex: \tikz[baseline]{\node[variable block,inline spacing,anchor=base]{orange rectangles};} 
    represent variables. When the data type is annotated in between operators,
    the variable styling will often be omitted for clarity.
    
    Simple glspl:rnn have difficulty learning long-term relationships because of a
    problem called the /vanishing gradient problem/ that also occurs in very
    deep neural networks. The LSTM (Long Short-Term Memory) cell solves this
    problem by splitting the state in two parts: one part is the output for
    every step and another part is a more long-term part that is only changed
    linearly with a filtered set of values from the first part cite:colah-lstm.
    The GRU (Gated Recurrent Unit) is a variation on the LSTM structure which
    appears to perform better on smaller datasets
    cite:chung14_empir_evaluat_gated_recur_neural.

    #+begin_src latex :exports results :results output
      \begin{figure}[h]
      \centering

      \begin{tikzpicture}
      \matrix [row sep=0.8cm, column sep=1.2cm] {
        \node (y_t-2) {}; &
        \node (y_t-1) {$y_{t-1}$}; &
        \node (y_t) {$y_{t}$}; &
        &
        \\
        \node (s_t-2) {$\cdots$}; &
        \node (rnn_t-1) [block] {RNN Cell}; &
        \node (rnn_t) [block] {RNN Cell}; &
        \node (rnn_next) {$\cdots$}; &
        \\
        &
        \node (x_t-1) {$x_{t-1}$}; &
        \node (x_t) {$x_{t}$}; &
        \node (x_t+1) {}; &
        \\
      };

      \path[->]
        (x_t-1) edge[thick] (rnn_t-1)
        (x_t) edge[thick] (rnn_t)

        (rnn_t-1) edge[thick] (y_t-1)
        (rnn_t) edge[thick] (y_t)

        (s_t-2) edge[thick] node[above] {$s_{t-2}$} (rnn_t-1)
        (rnn_t-1) edge[thick] node[above] {$s_{t-1}$} (rnn_t)
        (rnn_t) edge[thick] node[above] {$s_t$} (rnn_next)
        ;
      \end{tikzpicture}
      \caption{\label{fig:rnn}A time slice of a \glsfirst{rnn}.
        At each time step $t$, the network reads the current input $y_t$ and uses
        the state of the previous time step $s_{t-1}$ to compute the current output
        $y_t$ and current state $s_t$.}
      \end{figure}
    #+end_src

**** DONE Figure: Basic RNN
     CLOSED: [2018-07-10 Tue 17:57]
*** The Encoder-Decoder Architecture
    
    Glspl:rnn are good at predicting the next time step or steps in a sequence,
    making them ideal for tasks such as text autocompletion, but they can also
    be used for more complex tasks. 

    An encoder-decoder architecture consists of two recurrent neural networks.
    The first---the encoder---is used to "read" an input sequence. The state
    from the final time step is then interpreted as a representation of the
    entire sequence---often referred to as the gls:thought-vector. This
    gls:thought-vector then serves as input to a second gls:rnn that again
    outputs a sequence but of a different kind. This architecture is used in
    sequence-to-sequence problems where there is no one-to-one mapping between
    the steps in the source sequence and steps in the destination sequence. In
    machine translation, for example, the number of words in a sentence in
    different languages can differ, as well as the word order.
    
    Encoder-decoder architectures can also be used to solve problems which do
    not transform a sequence to another sequence. For example, in image caption
    generation, the encoder is a convolutional neural network that "interprets"
    and image, while the decoder network is a sequential network that generates
    a sequence of words describing the image.
    
    #+begin_src latex :exports results :results output
      \begin{figure}[h]
      \centering
      \usetikzlibrary{shapes,arrows,backgrounds,fit}

      \begin{tikzpicture}[->, shorten >=1pt, auto, node distance=2cm, semithick, font=\headingfont]

      \node (input) {};
      \node (encoder) [block, right=4cm of input] {Encoder};
      \draw [dash pattern=on 10pt off 5pt on 16pt off 5pt on 13pt off 5pt on 8pt off 5pt] (input) -- (encoder);

      \node (thought-vector) [variable block, above of=encoder, align=center] {Thought Vector};
      \draw (encoder) -> (thought-vector);

      \node (decoder) [block, above of=thought-vector] {Decoder};
      \draw (thought-vector) -> (decoder);
      \node (output) [right=4cm of decoder] {};
      \draw [dash pattern=on 2pt off 5pt on 7pt off 5pt on 3pt off 5pt on 4pt off 5pt on 1pt off 5pt on 1pt off 5pt] (decoder) -- (output);

      \end{tikzpicture}
      \caption{\label{fig:encoder-decover}A high-level overview of the encoder-decoder
        architecture that reads a variable-length input sequence and outputs another
        sequence. Often, both the encoder and the decoder networks are \glspl{rnn}.}
      \end{figure}
    #+end_src
    
*** Text Embedding

    When the input and output of an encoder-decoder network are the same, this
    stucture is called an /autoencoder/. An autoencoder can be trained without
    supervision and learns to create an internal representation (the
    gls:thought-vector) which is of much smaller dimensionality than the
    original data. This encoder part can then be reused as the first step in a
    supervised problem. An autoencoder can be used to compute a more efficient
    and meaningful representation of some input or to compute a fixed-length
    representation of a sequence, when the encoder and decoder networks are
    recurrent neural networks.

    One of the most widely used applications of this encoder-decoder
    architecture is /text embedding/, which is the process of creating a vector
    representation for parts of text (characters, parts of words, words or
    sentences). The ~word2vec~ algorithm is a popular implementation that embeds
    words in a vector space, where the positional relationship in this vector
    space is related to the semantic relationship between words. For example,
    the authors found that the operation vec("king") $-$ vec("man") $+$
    vec("woman") resulted in the vector which was closest to the vector
    representation of the word "queen"
    cite:mikolov13_effic_estim_word_repres_vector_space.
    
    Since it is reasonable to assume that sentences with a similar semantic
    meaning would result in similar gestures, a text embedding could be used as
    a first step to process the input sentence before generating a gesture. The
    advantage of this step is that existing pre-trained models are available.
    This could increase the effectiveness of the gesture synthesis process,
    especially if only a small dataset can be collected as training data. 

*** High-Level Interfaces
    
    The recent increase in popularity of and interest in deep learning methods
    could not have been possible without the introduction of tools that allow
    programmers to use these techniques without having to know precisely how the
    algorithms work or how to create programs that work efficiently on GPUs. The
    reason why the previous sections did not include extensive mathematical
    descriptions of the techniques explained is simply because it is not
    necessary to know these details when implementing these methods. While a
    foundational understanding is necessary to understand why these methods
    work, where they could fail and how they can be evaluated, modern frameworks
    change the skills that developers and researchers need from understanding
    mathematics (even though the mathematics behind neural networks are
    relatively simple), optimization techniques and multi-threaded GPU
    programming to knowing how to use the interfaces presented by these new
    frameworks and libraries.
    
    The most popular framework for deep learning (by far) today is TensorFlow
    cite:tensorflow,dl-rankings. At its core, this is an engine that builds a
    /computation graph/ based on the operations that need to be performed. This
    decouples the code the programmer writes from the execution of this code and
    allows the engine to execute the computations on a variety of hardware and
    optimize them for each type specifically. Because the framework knows
    exactly which computations are happening, it can automatically differentiate
    them in order to perform optimization methods like gradient descent without
    additional effort. On top of this engine, a large amount of machine learning
    and deep learning functions and algorithms have been implemented so they can
    be used relatively easily. A lot of machine learning research is done with
    TensorFlow, meaning that even very recently developed algorithms are often
    available for use. A small amount of its components are explained below.
    
    #+CAPTION: label:fig:tensorboard-screenshot Screenshot of TensorBoard, the graphical utility to visualize learning metrics and debug models, included with TensorFlow.
    #+ATTR_LATEX: :width 0.5\textwidth :float wrap :placement {R}[2cm]{0.5\textwidth}
    [[file:./img/tensorboard-screenshot.png]]
    
    The Dataset and Estimator APIs cite:tf-estimators,tf-datasets set a standard
    for handling data and organizing learning models, respectively. The Dataset
    API allows reading from a variety of sources such as /TFRecords/
    (TensorFlow's binary format for datasets), performing all the necessary
    transformations and inserting it into the computational graph efficiently.
    The Estimator API wraps a model, allows it to run on a multitude of
    computers (perhaps distributed) and handles building the graph, running
    training and evaluation, saving the results and reporting summaries.
    
    TensorBoard is a graphical interface included with TensorFlow that
    automatically shows graphs of various learning metrics, visualizes the
    computational graph and can show representations of embedding layers.
    Cref:fig:tensorboard-screenshot shows a screenshot of this interface.
    
    Finally, the TensorFlow team recently announced a new library /TensorFlow
    Hub/ that is now part of the TensorFlow ecosystem, which allows access to
    pre-trained models with a very simple API cite:introducing-tfhub. This
    library includes built-in access to a variety of text embedding modules,
    including the ~word2vec~ algorithm and the /universal sentence encoder/
    which processes greater-than-word length text cite:tfhub-text.
    
** The Dataset
   :PROPERTIES:
   :Effort:   1:00
   :END:
   label:sec:research-dataset

   The dataset is a crucial component of any machine learning project. In this
   case, the model should be trained to predict gestures from a sentence as
   input. This means the dataset should contain these input-output pairs: text
   as input and gestures as output.

   Gestures will be represented as sequences of /poses/, which are single frames
   with the position of a person's joints. This is the format used in motion
   tracking systems and can easily be represented on a virtual avatar.
   
   #+BEGIN_SRC latex :exports results
     \begin{figure}
       \adjustbox{max width=1.2\linewidth,center}{\input{./img/pose-format-comparison.pgf}}
       \caption{\label{fig:pose-format-comparison} Example of a \emph{pose}, a
         collection of joint positions. The left side shows the format used in the
         H36M dataset\cite{h36m_pami} and the 3D Pose
         Baseline\cite{martinez17_simpl_yet_effec_basel_human_pose_estim} projects,
         the right side is formatted according to the one used by OpenPose
         \cite{cao16_realt_multi_person_pose_estim}.}
     \end{figure}
   #+END_SRC
   
   A real robot however often requires a different type of input; the NAOqi API
   only provides the ability to directly specify the joint position of its
   wrists and torso cite:naoqi_cartesian_control. In order to manipulate the
   arms more precisely, the robot expects the joint angles instead
   cite:naoqi_joint_control. The simplest way to calculate these joint angles is
   to measure the angles between the joints in their positional representation.
   By using SoftBank's specification of these angles, they can be directly
   measured on a pose in order to move the robot to this position
   cite:naoqi_joints. As this representation will be used in the learning model,
   a more detailed description is deferred until cref:sec:preprocessing.
   
   Next to this difference in data format, it should be noted that these robots
   do not have the range of motion of humans, nor can they move their joints as
   quickly as people. This is a hard constraint on the extent to which human
   gesture can be imitated by a robot. The NAOqi API allows developers to
   specify the desired joint angles at every moment, which the robot will
   fulfill to its best ability.

   In order to avoid issues with the robot's balance, only the pose data of the
   upper body will be used to control the robot. SoftBank's Pepper robot has a
   hip joint which can be controlled without the risk of it toppling over
   (Pepper has a wheeled base), but NAO walks on its feet so controlling the
   legs to move its hips is risky.
   
   There are various datasets available of human motion, such as the Human3.6M
   and CMU Panopticon datasets
   cite:h36m_pami,Joo_2017_TPAMI,PoseletsICCV09,Shahroudy_2016_CVPR. However,
   these were created with the intent of training pose estimation or activity
   recognition techniques, resulting in datasets that are diverse in the kind of
   movements but have no or few samples of people who are talking. These
   datasets do not include subtitles for the text being spoken and lack audio
   tracks.
   
   As the dataset required for this project is not readily available, one will
   need to be created. Below, the elements for an approach to build a dataset
   from freely available videos and existing pose estimation projects are
   outlined.
   
*** Video Collection

    YouTube cite:youtube is one of the most popular websites and contains video
    footage from a wide variety of people in all kinds of environments and
    performing many activities. Videos with English spoken text are
    automatically transcribed which means that the subtitles for many videos are
    available. This means an adequate amount of video footage of people talking
    and gesturing would likely be available.

    To download videos with their subtitles, ~youtube-dl~ can be used which is a
    command line utility that can download video from a variety of sources
    including YouTube cite:youtube_dl.

    It is unlikely that entire videos will be usable so some pre-processing will
    need to be done on the downloaded videos. In particular, the parts of the
    videos that have suitable footage will need to be selected and the video
    will need to be split up in sentences with the corresponding footage.
    ~ffprobe~ is a command line utility that is part of the ~ffmpeg~ multimedia
    framework and can be used to detect scene changes (for example, when the
    footage cuts to another camera angle) cite:ffprobe. This can be used to aid
    the selection of footage, because pose estimation will be unstable across
    hard cuts and it is usually the case that a scene is either completely
    usable or completely unusable.

*** Pose Estimation

    Pose estimation is the task of processing an image or image sequence and
    extracting information about the pose of the person or people in that image
    (sequence). That is, the position of a person's joints (for example, the
    left knee, right wrist etc.) are estimated on the image.

    Some recent projects have had good results in estimating the (2D) positions
    of joints in images. The Stacked Hourglass
    cite:newell16_stack_hourg_networ_human_pose_estim, OpenPose
    cite:cao16_realt_multi_person_pose_estim and AlphaPose cite:fang16_rmpe
    networks have state-of-the-art results and have their source code freely
    available. All of these systems internally use convolutional neural networks
    to process their input images. The OpenPose and AlphaPose networks can
    detect multiple people in an image and do not have scaling or centering
    constraints, as opposed to the Stacked Hourglass algorithm.

    These networks estimate the two-dimensional position of joints in an image.
    To control a robot, however, the three-dimensional position of these poses
    (or the angles between them) is needed. There are two ways to approach
    three-dimensional pose estimation: either one first estimates the pose in
    2D, then "lifts" this into three dimensions, or one directly estimates the
    3D poses from an image.
    
    #+begin_src latex :exports (if (eq do-fancy-export t) "results" "none")
      \begin{figure}
        \adjustbox{center}{\animategraphics[loop,autoplay,width=0.5\textwidth]{12}{./img/openpose-demo/frame-}{0}{42}}
        \caption{\label{fig:openpose-demo} Example of 2D pose detections by OpenPose
          \cite{cao16_realt_multi_person_pose_estim}.}
      \end{figure}
    #+end_src
    
    #+begin_src latex :exports (if (eq do-fancy-export t) "none" "results")
      \begin{figure}
        \adjustbox{max width=0.75\textwidth,center}{\includegraphics{./img/openpose-demo.png}}
        \caption{\label{fig:openpose-demo} Example of 2D pose detections by OpenPose
          \cite{cao16_realt_multi_person_pose_estim}.}
      \end{figure}
    #+end_src

    It is possible to estimate 3D poses straight from monocular images
    cite:mehta16_monoc_human_pose_estim_in,simo-serra13_joint_model_pose_estim_singl_image,
    however, the source code of these projects is not available. For the VNect
    cite:mehta17_vnect project, an unofficial TensorFlow implementation is
    available cite:vnect_tensorflow but it did not produce results that were as
    good as using the official implementation of the other approach---lifting
    poses from 2D to 3D.
    
    The "3d-pose-baseline" project is what the authors consider to be a baseline
    for 2D-to-3D lifting of poses
    cite:martinez17_simpl_yet_effec_basel_human_pose_estim; it is a simple
    neural network but appeared to work well on the initial testing data. The
    code is available on GitHub and written with TensorFlow so it could be
    adapted for use within the rest of this project.

** Time Series Clustering
   :PROPERTIES:
   :Effort:   0:30
   :END:
   label:sec:research-clustering

   Instead of directly predicting poses, the problem of gesture synthesis can be
   much simplified if we break down movement into a sequence of motion
   primitives. This way, a two-step process appears:

   1. Extract glspl:motion-primitive from the pose data
   2. Predict glspl:motion-primitive from parts of text

   To extract these glspl:motion-primitive from the data, an unsupervised clustering
   algorithm could be used to find clusters of (subsequences of) gestures.
   Clustering, even with a large amount of features, is a well-understood
   problem and even one of the first techniques taught in most introductions to
   machine learning. /Time series/ clustering, however, introduces its own
   challenges cite:zolhavarieh14_review_subseq_time_series_clust. In order to be
   able to identify these glspl:motion-primitive, the algorithm needs to be able to
   look at small parts of these sequences and find similar-looking subsequences
   in other pose animations. One can compare this to anomaly detection, albeit
   with more labels than the two of "normal" and "exceptional".

   A first approach might be using a sliding window with some fixed time length
   and finding close matches across the dataset. However, sliding window
   approaches for clustering subsequences seem to be mostly meaningless
   cite:keoghil_clust.

   Previous work has been done on activity clustering of motion capture data
   cite:zhou13_hierar_align_clust_analy_tempor, though here the difference
   between different activities (e.g., walking versus sitting) is much more
   pronounced than different gestures and the authors noted that it did not
   perform well for smaller, more subtle movements.

   To perform clustering on /whole/ time series, there are multiple methods.
   Noting that clustering in its most general form comes down to grouping
   samples so that the samples within a group are close to each other while
   samples between groups are far away from each other, the key element of a
   clustering method is its distance metric
   cite:zolhavarieh14_review_subseq_time_series_clust. The classic Dynamic Time
   Warping (DTW) distance metric can be used to compare time series of different
   lengths and is implemented in the ~dtwclust~ R package cite:r-dtwclust
   which provides this and other clustering metrics. Additionally, this package
   includes a few methods to extract a medioid of these clusters, which can be
   used as the representation for a gesture to be played back on the robot.

** Conclusion

   Current recipes for gesture synthesis either make random moves or play back
   predetermined gestures based on annotated text but the ingredients for a more
   powerful gesture synthesis method are available. Glspl:rnn can handle
   sequential data and have previously successfully been used in tasks like
   machine translation and text-to-speech synthesis. A dataset of gestures
   performed while talking is not available but can be produced by taking
   internet videos and using pose estimation tools to extract the gestures
   performed. The problem can be greatly simplified and presented as a
   classification problem if poses can be represented as glspl:motion-primitive,
   but techniques to find these in an unsupervised way are not accessible yet
   and clustering entire clips might not yield good results. Pose estimation
   methods represent poses in terms of the location of joints but NAOqi robots
   expect them in terms of joint /angles/.
   
   With that, the /mise en place/ is finished. These components can now be
   assembled into a pipeline that performs all the steps necessary to build a
   model synthesizes gestures based on a subtitle and play them back on a robot.

* A Modern Method for Gesture Synthesis
  SCHEDULED: <2018-07-02 ma>--<2018-07-15 zo>
  label:chap:method

  The ingredients collected in cref:chap:literature-research need to be combined
  in the right order to build a system for gesture synthesis. This chapter
  explains how the pipeline was created that connects the pieces and builds a
  system to create a dataset, build a gesture prediction model and perform these
  gestures on a robot. Cref:sec:dataset walks through the steps needed to create
  the dataset, introducing an application designed to simplify finding suitable
  videos, showing how these video clips are processed and explaining how the
  data is prepared for use in the prediction model. This model is explained in
  cref:sec:pose-prediction, where two methods to process the features
  and two methods to generate the gestures are shown. Finally, cref:sec:playback
  closes this chapter by performing the resulting gestures on a live robot.
  Cref:fig:pipeline shows an overview of all the components involved and how
  data flows between them.

  #+BEGIN_SRC latex :exports results :results output
    \begin{figure}
      \adjustbox{center}{\begin{tikzpicture}
        \matrix[default matrix] {
          \node (video) [variable block] {Video}; &
          \node (video-picker) [block] {Video Picker}; &
          \node (detect-pose) [block] {Pose Estimation}; &
          \node (lift-pose) [block] {Pose Lifting}; \\
          \node (angle-conversion) [block, align=center] {Position $\rightarrow$ Angle\\{\small conversion}}; &
          \node (dataset) [variable block, rectangle split, rectangle split parts = 3] {
            Dataset
            \nodepart{second} {\small Subtitles}
            \nodepart{third}  {\small Angle-Based Gestures}
          }; &
          \node (clustering) [block, anchor=south] at (0, 0.5\smallgridsize) {Clustering}; &
          \node (sequence-predictor) [block, anchor=north, align=center] at (0, -0.5\smallgridsize) {Gesture Prediction\\{\small Sequence Decoder}};
          \node (classification-predictor) [block, align=center, anchor=south] at (0, 0.5\smallgridsize) {Gesture Prediction\\{\small Classification Decoder}}; &
          \node (gesture) [variable block] {Gesture}; \\
        };

        \path[->, above, outer sep=0.2\smallgridsize, every node/.append style={rounded rectangle, fill=white, fill opacity=0.6, text opacity=1, tight spacing}]
          (video) edge (video-picker)
          (video-picker) edge node {Images} (detect-pose)
          (detect-pose) edge node {2D Poses} (lift-pose)
          (angle-conversion) edge (dataset)

          (dataset.second east) edge (clustering)
          (dataset.third east) edge (classification-predictor)
          (clustering) edge node {Gesture Classes} (classification-predictor)

          (dataset.second east) edge (sequence-predictor)
          (dataset.third east) edge (sequence-predictor)
          (classification-predictor) edge (gesture)

          (sequence-predictor) edge (gesture)
          ;

        \draw[->]
          (lift-pose.east)
          .. controls ++(\smallgridsize, -2\smallgridsize)
             and ($(angle-conversion.west) + (-\smallgridsize, 3\smallgridsize)$) ..
          node [fill=white, tight spacing] {3D Poses}
          (angle-conversion.west);

        % \draw node [tight spacing, right=6pt of angles.east] {Angles};
        % \draw node (subtitle) [variable block, tight spacing, anchor=center, below=0.25\smallgridsize of clustering] {Subtitle};
        % \draw[->] (subtitle) -- (sequence-predictor);
        % \draw[->] (subtitle) -- (classification-predictor);

      \end{tikzpicture}}
      \caption{\label{fig:pipeline} Overview of this project's pipeline. It starts
        by processing a video to become part of the dataset, the result of which is
        then used to predict a gesture based on some given text.}
    \end{figure}
  #+END_SRC

  \paragraph{A Note on the Source Code and Development Environment}

  As this research is designed to be built upon, having easily accessible source
  code and data is important. All the code used in this project is made
  available on the GitHub repository https://github.com/iamarcel/thesis along
  with detailed usage instructions. The source code for this report and the
  scripts used to generate the images, diagrams and graphs are also available.
  
  Most of the code is written in Python and Pipenv cite:pipenv is used to manage
  dependencies automatically. Docker container specifications are also available
  that install the system-level dependencies necessary for some components and
  compile OpenPose automatically. Installation instructions for computers either
  with or without NVIDIA graphics card are available.

** Creating the Dataset
   label:sec:dataset

   Most people do not need to explicitly be taught how to behave in a humanlike
   manner. Robots, however, have less luck and need explicit programming or, in
   our case, a large collection of carefully crafted examples. These examples
   should allow a deep learning model to predict a gesture based on a piece of
   text as input---which is referred to as the /subtitle/---so this is a pair of
   a subtitle and corresponding gesture or a reference to the
   gls:motion-primitive.
   
   In the first step a set of /clips/ will be collected that form the elements
   of the dataset. A clip is a part of the video corresponding to one line in
   its subtitles. The following steps will extract information like the gesture
   performed by the subject in the video, but the whole of the video and other
   associated information will still be referred to as the "clip". This means
   that each of the steps below will add information to the clip. An overview of
   the fields of a clip is shown in cref:fig:clip-structure.
   
   #+BEGIN_EXPORT latex
   \begin{figure}[h]
     \adjustbox{center}{
       \begin{tikzpicture}
         \draw node (clip) [variable block, 
             rectangle split, rectangle split parts = 8,
             font = \small,
             minimum width = 2\gridsize, rectangle split part align = left] {
           {\large Clip}
           \nodepart{two}   {ID}
           \nodepart{three} {Video Reference (Start and End Frames)}
           \nodepart{four}  {Subtitle}
           \nodepart{five}  {Poses, 2D Points}
           \nodepart{six}   {Poses, 3D Points}
           \nodepart{seven} {Poses, Angles}
           \nodepart{eight} {Gesture Class}
         };
       \end{tikzpicture}
     }
     \caption{\label{fig:clip-structure} The unit of data in the dataset is the
       \emph{clip}, which has the fields as shown here.}
   \end{figure}
   #+END_EXPORT

   Having defined the data structure, the time has come to start browsing
   through the wealth of videos available on YouTube (for science).

*** The Video Picker
    :PROPERTIES:
    :Effort:   1:00
    :END:
    label:sec:video-picker

    While there is indeed a lot of video material available on YouTube, the
    requirements for the dataset are very specific:

    - The clip should be of a person talking
    - The person should talk English and subtitles should be available
    - The person should be visible in its entirety (as will be explained below,
      this is necessary for further steps in the pipeline)
    - The clip should be a single contiguous shot, i.e., the video cannot cut
      to a shot from another angle

    Whole videos that fulfill these needs are scarce but since the data has to
    be cut into clips, videos can be processed to extract only the parts that
    fulfill these requirements. The Video Picker application built assists in
    the process of finding good parts of a video and saving its data.

    When a video with suitable parts is found on YouTube and downloaded using
    ~youtube-dl~ cite:youtube_dl, it is first examined by the scene detection
    algorithm in ~ffprobe~ cite:ffprobe. Usually, a person is similarly framed
    throughout a single scene (most scenes do not pan or zoom to the point that
    parts of the person are cut off) so the next step can use these results from
    the scene detection algorithm and run semi-automatically when a suitable
    scene is found, saving all the clips in a single scene.
    
    The term /scene/ as used in the scene detection algorithm can be confusing.
    Since this tool detects sudden changes between video frames, it rather
    detects /shots/ (video sequences recorded continuously by one camera). In
    films, a scene is often comprised of multiple shots. Thus, from now on, the
    term "shot" will be used. Also remark that, since the speaker can say
    multiple sentences during a single shot, a shot will contain multiple clips.

    Then, the video is opened in Video Picker. The video picker is a GUI (see
    cref:fig:video-picker for a screenshot) in which the user can scrub through
    the video or navigate by shot. When he has found a suitable shot, he can
    point at the person of interest with the mouse cursor and start recording
    it. Since the pose detection algorithm can detect multiple people in the
    image (it could recognize people in the audience, for example) the user
    needs to point his cursor closest to the person he is interested in. This
    will be used later to filter out only the person of interest.

    The video picker then starts extracting the clips from the current shot.
    Every clip is saved in a JSON Lines format cite:jsonlines (where every line
    in the file is a JSON-formatted object; this is much faster than reading and
    parsing an entire JSON object at once). The image frames are extracted
    automatically by the application and saved in a specific folder. The
    information in the JSON object just created can be used to find these
    images. The user can also explicitly pick a single clip or stop extracting
    when the shot has changed but the scene detection algorithm had not detected
    that change. This happens, for example, when there is a smooth transition
    between shots.

    Below the surface, the video picker is a Python application using the GTK+
    cite:gtk and GStreamer cite:gstreamer frameworks for building the GUI and
    playing back the video respectively.
    
    Due to the relatively strict conditions for usable videos (mainly the fact
    that the person speaking should be fully visible), most of the videos used
    in this project are of people who are presenting on a stage, e.g.,
    presenters of TED talks. This will necessarily result in a set of gestures
    that might not completely correspond to the gestures one performs in a
    dialogue with another person. On the other hand, this limited "gesture
    vocabulary" will make it easier to train a machine learning model and the
    resulting gestures will likely still look natural. With a more diverse set
    of gesturing styles, more data would be needed to model a general type of
    gesturing.

    It would likely be possible to train this pipeline on a different set of
    specialized gestures such as gestures of someone telling fairy tales, which
    the model could then learn to imitate. This proof of concept, though, is
    focused on the use case for a monologue in front of an audience.
    
    #+caption: label:fig:video-picker Screenshot of the Video Picker application. This allows the user to select usable clips from videos and extracts their frames and subtitles for further processing.
    #+attr_latex: :width 1.2\textwidth,center
    [[file:./img/video-picker-screenshot.png]]

**** DONE Figure: screenshot
     CLOSED: [2018-07-20 vr 09:32]
*** Detecting 2D Poses with OpenPose
    :PROPERTIES:
    :Effort:   0:30
    :END:

    Once the video clips are collected, the next step is to perform 2D pose
    estimation on the extracted image frames and saving those results to the
    clips. The authors of OpenPose included a sample application that, once
    compiled, can read a directory of images and write the poses in each image
    to a JSON file. This "demo" program is run on the output directory of the
    images Video Picker extracted and afterwards, the pose data from OpenPose is
    added to the database of clips.

    #+BEGIN_SRC latex :exports results
      \begin{figure}[h]
        \centering
        \adjustbox{max width=1.2\linewidth,center}{\input{./img/sanity-check-openpose.pgf}}
        \caption{\label{fig:sanity_check_openpose} âSanity checkâ for OpenPose 2D detections, showing a source video frame and the extracted pose information.}
      \end{figure}
    #+END_SRC
    
    The JSON files generated by OpenPose store a list of all the people detected
    in the frame, with the position and confidence score for each joint. This
    joint data is stored in a vector of the $x$ position, $y$ position and
    confidence score for each joint in the order as shown in
    cref:fig:pose-format-comparison cite:openpose_output.
    Cref:fig:sanity_check_openpose shows an example result from this step of the
    pipeline. This "sanity check" was performed on a subset of the captured
    clips in order to verify whether the data was sent to and processed by
    OpenPose correctly.

    OpenPose provides no stability or continuity of detected people across
    frames, i.e., people can disappear or appear over time and the order in
    which they are specified can change throughout frames. Thus, before using
    these results as input for the next step, all but one person are discarded
    according to the region of interest annotation that was made in Video
    Picker. Additionally, to ensure data quality, clips are discarded when, in
    at least one frame in the clip, the mean confidence for all joints is lower
    than $60\%$.

**** DONE Figure: sanity check - example of OpenPose detection
     CLOSED: [2018-07-20 vr 10:35]

*** Lifting Poses to 3D
    :PROPERTIES:
    :Effort:   2:00
    :END:

    Now that the 2D gestures are extracted, the next step is to lift the poses
    into three-dimensional space. The /3D Pose Baseline/ project had its source
    code and trained model available online so this was used as a starting
    point. Some modifications were made in order to use it in this pipeline.

    The first modification is made because the pose data format that 3D Pose
    Baseline expects is different from the one OpenPose outputs: they use the
    Human3.6M and COCO format respectively. The Human3.6M pose model has its
    joints ordered differently, does not have the eye and ear joints but does
    define hip, top-of-head and spine (at chest height) joints. These differences were visualized in
    cref:fig:pose-format-comparison.
    
    Another 2D pose estimation framework, the Stacked Hourglass project
    cite:newell16_stack_hourg_networ_human_pose_estim, uses the same skeleton
    structure as 3D Pose Baseline and also has its source code available (in Lua
    and Torch). When testing this out, however, the results where not nearly as
    good as those from OpenPose. The Stacked Hourglass network can only detect a
    single person and requires precise annotation of the person's center and
    size in an image, which would also make the data collection step more
    difficult.
    
    While the ideal solution for the incompatibility between pose formats would
    be to re-train the 3D Pose Baseline model using 2D data from OpenPose, that
    would require processing their entire training set with OpenPose and then
    training it, which would take too much time. Instead, a rough direct
    conversion was made. Before passing the 2D detections as input to 3D Pose
    Baseline, their points were reordered and the following points were added,
    based on some simple vector calculations:
    
    - Hip :: Center of left and right hip
    - Head (top of head) :: Half the distance between the Neck/Nose and the
         Thorax joints above the Neck/Nose.
    - Spine (chest height) :: Half the distance between Thorax and Hip below
         the Thorax.

    The second modification is necessary to use this 3D Pose Baseline for making
    predictions. While the authors' code allowed running the training and
    validation steps, there was no code present to run the inference step, i.e.,
    predicting 3D poses for new 2D detections. Additionally, a smoothing step
    was added to reduce jittering between the frame-per-frame 2D predictions
    from OpenPose.
    
    After these two changes the 3D Pose Baseline code is usable but does not yet
    produce results that were of adequate quality. Sometimes poses are
    completely incorrect or deformed throughout a clip.
    Cref:fig:crappy-3d-detections shows some samples of these low-quality
    results. Three issues were found rooted in the data.
    
    #+CALL: pgf_figure(var_name="crappy-3d-detections", var_caption="Samples of unusable results from the 3D Pose Baseline, who were discarded from the final dataset.")

    First, a missing point in the 2D detection has large effects on the results
    in 3D. When only joints in the upper body are detected by OpenPose, the
    lifted 3D pose is useless. Second, since the data is captured from multiple
    people, their size and body shape differs. Finally, the people in the 3D
    space are oriented in different directions.
    
    These effects would result in the model learning useless features like the
    body shape or orientation of the people. Thus, before using the data in a
    machine learning model, it has to be cleaned and normalized first.
    
    #+BEGIN_SRC latex :exports results
      \begin{figure}[htbp]
        \centering
        \adjustbox{max width=1.2\linewidth,center}{\input{./img/sanity-check-3d.pgf}}
        \caption{\label{fig:sanity_check_3d} Sanity check for the 2D to 3D pose conversion.}
      \end{figure}
    #+END_SRC
    
    #+BEGIN_SRC latex :exports results
      \begin{figure}[htbp]
        \centering
        \adjustbox{max width=1.2\linewidth,center}{\input{./img/sanity-check-pipeline.pgf}}
        \caption{\label{fig:sanity_check_pipeline} Sanity check for the entire pipeline.
           the image frame used as source, the middle shows the results from the OpenPose 2D
           pose estimation and the right shows the results from lifting that 2D pose into 3D
           and performing a slight rotation. Above the figure is the corresponding subtitle for
           this clip.}
      \end{figure}
    #+END_SRC

**** DONE Figure: sanity check - 3D skeletons
     CLOSED: [2018-07-20 vr 14:00]

**** DONE Figure: sanity check - video > 2D detection > 3D skeleton (+ subtitle)
     CLOSED: [2018-07-20 vr 14:37]
     
**** DONE Figure: difference in skeleton structures
     CLOSED: [2018-07-28 za 08:56]

*** Cleaning and Normalizing the Data
    label:sec:preprocessing

    The 3D poses are processed in two steps:
    
    1. *Cleaning* throws away corrupt poses and attempts to correct small
       errors.
    2. *Normalizing* formats the poses so they are independent of body shape and
       orientation.

**** Cleaning
    Three classes of errors occurred in the results from 3D Pose Baseline:

    - Point error: in a single or a few frames, one or more joints were not
      detected in 2D and have erroneous positions in the 3D output.
    - Clip error: not enough points were detected in 2D, resulting in an
      unusable 3D skeleton.
    - Leaning: a person appears to be leaning forward while it should not.

    Each clip is processed on a frame-by-frame basis and the distance of each
    joint with that joint in the previous frame is examined. When this distance
    exceeds a threshold (here 30% of a person's height), the position of that
    joint is replaced with the position from the previous frame. When more than
    4 joints have to be corrected this way, the clip is considered low quality
    and not used anymore.

    Then, the leaning issue is corrected for by setting an allowed range for
    the angle that the spine makes with the upward axis. If this angle is
    exceeded, all the points of the upper body are rotated so that they lie
    within this range. This leaning problem likely appeared because of the
    imprecise placement of the Spine joint that was not present in the OpenPose
    results and estimated (see the previous step).

**** Normalizing
    At this point, a pose is represented by the Cartesian coordinates of each
    joint in three-dimensional space. Even when every skeleton is centered
    around the hip, the height is set to unity and all skeletons are oriented
    in the same direction (by rotating the body so that the hip is aligned with
    the perpendicular axis), there are still two problems: people's body type
    differs significantly and the space containing all possible poses (i.e.,
    the entire 3D Cartesian space for every joint) is too large.

    Since the end goal is to play back gestures on a NAO robot, the choice was
    made to convert the data format to one that is directly compatible with the
    NAOqi SDK that is used to control this robot. Controlling the pose of a NAO
    robot is done by setting the angles of its actuators, so these angles could
    be measured from the position representation of the 3D poses.
    Cref:fig:nao-angles shows the definition of these angles and
    cref:tab:pose-to-angle shows the details of how they can be calculated from
    the Cartesian coordinates. The joints mentioned in cref:tab:pose-to-angle
    are interpreted as vectors, which cref:fig:pose-angles visualizes.

    Note that when axes are specified, this is in the reference coordinate
    system of the poses returned from 3D Pose Baseline, not the coordinate
    system used in the NAOqi software.

    #+NAME: tab:pose-to-angle
    #+CAPTION: label:tab:pose-to-angle Details of joint position to angle conversion
    | Angle name     | Method                                                         |
    |----------------+----------------------------------------------------------------|
    | HipRoll        | Angle around $-z$ axis, from chest (upwards) to $-y$ axis      |
    | HipPitch       | Angle around $x$ axis, from chest to $-y$ axis                 |
    | RShoulderPitch | Angle around $x$ axis, from right upper arm to chest $- \pi/2$ |
    | RShoulderRoll  | Angle of right upper arm with $yz$ plane $+ \pi/10$            |
    | RElbowRoll     | Angle between right upper arm and right elbow                  |
    | LShoulderPitch | Angle around $x$ axis, from left upper arm to chest $- \pi/2$  |
    | LShoulderRoll  | Angle of left upper arm with $yz$ plane $- \pi/10$             |
    | LElbowRoll     | Angle between left upper arm and left elbow (negative)         |
    | HeadPitch      | Angle around $x$ axis, from nose to head $- \pi/4$             |
    | HeadYaw        | Angle around $-y$ axis, from $-z$ axis to nose                 |

    Note that these are only the angles for the upper body. The other joints and
    angles are ignored because they are not used here to generate gestures.
    
    At this point, all the data is ready and in the right format to be learned.
    However, before building a model, perhaps one assumption can make this task
    a lot simpler.
    
    #+BEGIN_SRC latex
      \begin{figure}
        \begin{tabular}{ >{\centering\arraybackslash} m{70mm} >{\centering\arraybackslash} m{70mm} }
          \includegraphics[width=65mm]{./img/nao-angles-arm-l.png} & \includegraphics[width=65mm]{./img/nao-angles-arm-r.png} \\
          (a) Left arm angles & (b) Right arm angles \\[18pt]
          \includegraphics[width=65mm]{./img/nao-angles-head.png} & \includegraphics[width=40mm]{./img/nao-axes.png} \\
          (c) Head angles & (d) Reference frame
        \end{tabular}
        \caption{\label{fig:nao-angles} Angle definitions and reference frame for
          Cartesian coordinates of the NAO robot. The 3D pose data is converted from
          Cartesian coordinates into a representation based on these angles.}
      \end{figure}
    #+END_SRC
    
    #+BEGIN_SRC latex :exports results
      \begin{figure}[htbp]
        \centering
        \adjustbox{max width=1.2\linewidth,center}{\includegraphics{./img/pose-vectors.png}}
        \caption{\label{fig:pose-angles} Visualization of the vector interpretation of
          body joints. Joints on the back side are shaded for clarity. The reference
          coordinate frame for the 3D Pose Baseline poses is also shown (not to
          scale); the $x$, $y$ and $z$ axes are colored in red, green and blue
          respectively.}
      \end{figure}
    #+END_SRC

**** TODO Figure: leaning
     :PROPERTIES:
     :Effort:   0:30
     :END:
**** DONE Figure: NAO skeleton and angles
     CLOSED: [2018-07-20 vr 15:19]
     :PROPERTIES:
     :Effort:   0:30
     :END:
**** DONE Figure: Vectors used in pose, for directions
     CLOSED: [2018-07-20 vr 17:09]
     :PROPERTIES:
     :Effort:   1:00
     :END:
**** DONE Figure: Difference in axes
     CLOSED: [2018-07-20 vr 17:15]
     :PROPERTIES:
     :Effort:   0:15
     :END:
*** Finding Motion Primitives
   
    Even if pose data is stored as a limited set of angles, the output space is
    continuous and quite large. This makes it difficult to train a machine
    learning model with only a small amount of data. Would it be possible to
    vastly reduce the model complexity by turning it into a classification
    problem? How would the results compare?

    Gesture synthesis can be interpreted as a classification problem if the
    space of possible movements is reduced to a sequence of predefined /motion
    primitives/, assuming a gesture can be split up in such a sequence. Instead
    of producing a continuous sequence of angles, the model could classify a
    sentence under a gls:motion-primitive and then concatenate these
    glspl:motion-primitive into a coherent, continuous whole. This approach
    poses two more questions:

    - Can we make this look continuous? Continuity was one of the main
      objectives but given the discrete nature of a sequence of motion
      primitives, this appears not to be trivial.
    - Is it possible to extract a set of these glspl:motion-primitive from our
      dataset? I.e., can we cluster our dataset into motion patterns?

    The first question might have a straightforward answer as the NAOqi software
    has a built-in animation module that can interpolate between points. It is,
    however, difficult to evaluate beforehand if this results in (qualitatively)
    natural motion. The second question needs deeper investigation and
    experimentation.
   
**** Time Series Clustering
     :LOGBOOK:
     CLOCK: [2018-07-21 za 08:48]--[2018-07-21 za 09:01] =>  0:13
     :END:

     These glspl:motion-primitive can be extracted from the captured gestures
     by performing unsupervised clustering on the dataset. The range of
     algorithms available is determined by the properties of this dataset:

     - It is a collection in which samples are time series
     - The samples have varying lengths
     - The samples are multi-dimensional (one dimension for each joint)
     - The desired clusters are subsequences of these samples

     Suitable algorithms to perform unsupervised, multi-dimensional clustering on
     subsets across multiple samples, with an implementation readily available,
     were not found by the author so the implementation in this project clusters
     across /whole/ samples instead of subsequences.

     As mentioned in cref:sec:research-clustering, the ~dtwclust~ R package
     allows experimenting with different distance metrics. Those that support
     sequences of varying lengths are described below.

     *Dynamic Time Warping (DTW) distance.* To calculate the DTW distance between
     two sequences $a_i, i \in \{1,\ldots,n\}$ and $b_j, j \in \{1,\ldots,m\}$,
     the following steps are taken:

     1. Calculate the pairwise Euclidian distance between every pair of points
        $(a_i, b_j)$ and store it in a matrix $M_{i,j} = d(a_i, b_j)$, where $d:
        \mathbb{R}^k \times \mathbb{R}^k \rightarrow \mathbb{R}$ is the
        \(k\)-dimensional Euclidean distance function.
     2. Find the shortest path from $M_{0,0}$ to $M_{n,m}$, where the total weight
        of the path is the sum of the elements on this path. Every step in this
        path can only increase one of or both of the matrix' indices by one.

     This shortest path, in terms of \((i, j)\)-pairs, is called the /alignment/
     and the sum of the elements of this path is the DTW distance
     cite:sarda2017comparing.
    
     *Truangular Global Alignment Kernel (GAK) distance.* GAK methods interpret
     the distance measurement in a kernel space, similar to the process often
     used in Support Vector Machines. With a GAK, it is relatively simple to add
     a penalty to certain paths. In particular, the Triangular GAK with parameter
     $T$ weights elements of the alignment by their distance to the matrix
     diagonal and discards elements further than $T$ from the diagonal. This
     greatly reduces the computation complexity---with some loss of precision, of
     course. Still, the triangular GAK seems to perform well
     cite:Cuturi:2011:FGA:3104482.3104599,sarda2017comparing.
    
     The second element of a clustering algorithm is the method of defining a
     /prototype/ or centroid of a cluster. In this case, the Partition Around
     Medioids (PAM) method is used, which always uses an element of the data as
     centroid.
    
***** DONE Explain metrics in dtwclust
      CLOSED: [2018-07-20 vr 19:32]
      :PROPERTIES:
      :Effort:   0:30
      :END:

***** TODO Figure: Euclidian distance vs. DTW distance
      :PROPERTIES:
      :Effort:   0:30
      :END:

     The standard Euclidian distance compares distance on a point-by-point basis.
     For time series, however, this metric falls short because it cannot account
     for variations in the /length/ of recurring patterns that should be
     discovered. The Dynamic Time Warping (DTW) metric solves this issue by
     skipping or repeating points in time so that the distance between two time
     series is minimized.

**** Results
     :PROPERTIES:
     :Effort:   2:00
     :END:
     :LOGBOOK:
     CLOCK: [2018-07-23 ma 14:43]--[2018-07-23 ma 14:54] =>  0:11
     CLOCK: [2018-07-23 ma 07:56]--[2018-07-23 ma 11:54] =>  3:58
     CLOCK: [2018-07-21 za 09:01]--[2018-07-21 za 10:26] =>  1:25
     :END:

     The results described here were obtained using the GAK distance metric, PAM
     centroid method and a partition in eight clusters.
     Cref:fig:clustering-results-histogram shows the distribution of clusters
     across the dataset. Two of the clusters are very small, two are very big and
     the other four have a size roughly 1/8th of the dataset.
    
     #+call: pgf_figure(var_name="clustering-results-histogram", var_caption="Distribution of gesture clusters across the dataset.")
    
     #+call: pgf_figure(var_name="cluster-centers", var_caption="A single frame from each of the clustered gestures' centroids.")
    
     #+BEGIN_SRC latex :exports results :results output
       \begin{figure}[!tbp]
         \adjustbox{max width=0.95\paperwidth,center}{%
         \begin{tabular}{m{35mm} m{35mm} m{35mm} m{35mm} m{35mm} m{35mm} }

           \multicolumn{2}{p{70mm}}{\input{./img/cluster-1-samples.pgf}} &
           \multicolumn{2}{p{70mm}}{\input{./img/cluster-2-samples.pgf}} &
           \multicolumn{2}{p{70mm}}{\input{./img/cluster-3-samples.pgf}} \\
           \multicolumn{2}{c}{(a) Cluster 1} &
           \multicolumn{2}{c}{(b) Cluster 2} &
           \multicolumn{2}{c}{(c) Cluster 3} \\

           \multicolumn{2}{p{70mm}}{\input{./img/cluster-4-samples.pgf}} &
           \multicolumn{2}{p{70mm}}{\input{./img/cluster-5-samples.pgf}} &
           \multicolumn{2}{p{70mm}}{\input{./img/cluster-6-samples.pgf}} \\
           \multicolumn{2}{c}{(d) Cluster 4} &
           \multicolumn{2}{c}{(e) Cluster 5} &
           \multicolumn{2}{c}{(f) Cluster 6} \\

           \multicolumn{3}{p{105mm}}{\adjustbox{center}{\input{./img/cluster-7-samples.pgf}}} &
           \multicolumn{3}{p{105mm}}{\adjustbox{center}{\input{./img/cluster-8-samples.pgf}}} \\
           \multicolumn{3}{c}{(g) Cluster 7} &
           \multicolumn{3}{c}{(h) Cluster 8}

         \end{tabular}}
         \caption{\label{fig:cluster-samples} Frames from four samples for each cluster.}
       \end{figure}

     #+END_SRC
    
     Cref:fig:cluster-centers shows a frame for the centroid of each of the
     clusters and cref:fig:cluster-samples shows, for each of the eight
     clusters, a frame from four random samples in that cluster. While it is
     difficult to evaluate based on single frames, looking at the animated
     version of cref:fig:cluster-samples reveals that the results from
     clustering are good in some cases and not that good in others. For example,
     one of the poses in cluster 3 (where the person's right upper arm is
     extended to the right and their lower arm is pointing downwards) would make
     more sense if it would belong to cluster 5, where two instances of similar
     gestures are present. The samples within cluster 4 and cluster 6 are all
     very similar but they could perhaps even be combined into a single cluster.
    
     The author suspects that much better results can be achieved with a larger
     dataset or if an algorithm could be implemented that can extract clusters
     from subsequences of the gestures, which would be real
     glspl:motion-primitive. For now, these results will be used for the rest
     of the project since building the entire flow that collects and processes
     data and generates gestures from that is deemed more important. As
     explained in cref:sec:video-picker, collecting more data is time-consuming
     but straightforward.
     
     A noteworthy consequence of a clustering method is the tendency to produce
     smoothed, average results instead of outliers: the cluster representatives
     are close to the average of all the elements in that cluster. While this is
     often a desired property, that might not be the case here. Humans have a
     repertoire of specific, iconic gestures such as waving while saying
     "hello." If the clustering or prediction algorithm would be able to
     reproduce these gestures this would give a high confidence in its ability
     to accurately synthesize gestures, but this averaging property impedes
     this.

     In order to still gain the ability to perform these iconic gestures, the
     dataset could be biased to include a large amount of samples for each of
     these gestures. That would both help the clustering algorithm recognize
     these gestures and help a direct gesture synthesis machine learning model
     learn these gestures. It would also be possible annotate the data and to
     bias the learning algorithm to weight these iconic gestures more heavily,
     forcing it to learn them.

***** DONE Figure: examples of extracted clusters
      CLOSED: [2018-07-23 ma 14:51]
*** Summary
    
    In summary, the dataset can be created by repeatedly adding clips from
    videos and then processing the results. 

    Processing a video is a three-step process:

    1. Find and download a suitable video. It should
       - be of a person talking in front of an audience,
       - be in English,
       - have captions, and
       - have at least one shot where the person speaking is completely in frame.
    2. Pre-process the video by running the scene detection algorithm.
    3. Process it with the Video Picker.
       1. Open the video in the Video Picker.
       2. Browse through the shots until one is found where the person is
          completely in frame.
       3. Save that shot and go back to the previous step.

    After these clips are collected, the saved frames from these videos can be
    processed with the following steps:

    1. Run OpenPose on the extracted video frames to perform pose estimation.
    2. Run 3D Pose Baseline to lift those 2D poses to 3D.
    3. Run the gesture processing script to fix or remove data, convert it to
       the angle-based representation and calculate statistics for
       normalization.
    4. Run the clustering algorithm to classify the gestures and create
       the centroids representing glspl:motion-primitive.
** Predicting Gestures 
   label:sec:pose-prediction

   With the dataset in place, a model can be built that can make predictions of
   gestures. This section shows the model used and its variations. In all cases,
   the overall architecture is that of an encoder-decoder network but there are
   two alternatives for each half.
   
   The encoder is responsible for reading the input text and interpreting that
   sequence into a thought vector. In one case, the encoder uses an gls:rnn and a
   vocabulary based on the input data while in the other case, the encoder uses
   a pretrained sequence encoder for text embedding of sentences.

   The decoder then generates the desired output based on the results from the
   encoder. In one case, the decoder will return a set of probabilities for
   output classes while in the other case, the decoder will return a sequence of
   poses, i.e., a gesture.

*** DONE Figure: graph of the model(s)
    CLOSED: [2018-07-27 vr 08:36]
    :PROPERTIES:
    :Effort:   0:30
    :END:
    :LOGBOOK:
    CLOCK: [2018-07-22 zo 10:50]--[2018-07-22 zo 11:21] =>  0:31
    :END:
    
*** The Encoder Network
    :PROPERTIES:
    :Effort:   1:00
    :END:
    :LOGBOOK:
    CLOCK: [2018-07-22 zo 11:42]--[2018-07-22 zo 12:54] =>  1:12
    :END:

    The encoder network interprets the input sentence and returns an internal
    representation to be used by the decoder network. Note that the input of
    this network is a string, a variable-length sequence of characters, so the
    encoder must be able to able to process sequential data. If a plain neural
    network would be used (where sequences are padded up until the maximum
    length of an input sequence in the dataset), it would suffer from the
    following problems:

    - The size of the network and subsequently its complexity is dependent upon
      the maximum length. This means, for example, that the network needs to be
      rebuilt if longer sequences are desired.
    - Exactly the same amount of processing needs to happen whether the input is
      short or long.
    - The network cannot learn to exploit structure in parts of the input, e.g.,
      the words "this big" appearing at the beginning or at the end result in
      completely different computations while they should probably result in the
      same gesture, just at a different point in time.

    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}
        \centering
        \begin{tikzpicture}[semithick, font=\headingfont]
          \matrix [row sep=0.8cm, column sep=1.2cm] {
            \node (thought) [variable block, align=center] {Output\\{\small(Thought Vector)}}; \\
            \node (rnn) [block] {\Gls{rnn} Cell}; \\
            \node (embedder) [block] {Word Embedding Layer}; \\
            \node (lookup) [block] {Lookup}; \\
            \node (splitter) [block] {Splitter}; \\
            \node (sentence) [variable block, align=center] {Input\\{\small(Sentence)}}; \\
          };

          \path [->, thick]
            (sentence) edge (splitter)
            (splitter) edge node[right, xshift=12pt] {Normalized words} (lookup)
            (lookup) edge node[right, xshift=12pt] {Word Indices} (embedder)
            (embedder) edge node[right, xshift=12pt] {Word Embeddings} (rnn)
            (rnn) edge (thought)
            (rnn) edge [loop right] ()
          ;

          \foreach \x in {-10pt, 10pt} {
            \path [->, thick]
              (splitter.north) edge ([xshift=\x] lookup.south)
              ([xshift=\x] lookup.north)   edge ([xshift=\x] embedder.south)
              ([xshift=\x] embedder.north) edge ([xshift=\x] rnn.south)
              ;
          }

        \end{tikzpicture}
        \caption{\label{fig:text-encoder} The encoder network that processes an input
          sentence into a thought vector. Multiple arrows are shown where sequential
          data flows.}
      \end{figure}
    #+END_SRC
   
**** \Glsentrytext{rnn}-based encoder
    
     The \gls{rnn}-based encoder processes the input text in four steps:

     1. Split the sentence into text objects.
     2. Use a vocabulary file to encode them with one-hot encoding [fn:one-hot].
     3. Create an /embedding/ of these text objects, representing them as a
        (dense) vector in a lower-dimensional feature space.
     4. Read this sequence of embeddings with an gls:rnn and return the thought
        vector.

     The first design choice in the encoder is the definition of a /text object/.
     This is the unit of text to which the sentence will be reduced. Common
     examples are words and n-grams ($n$ characters). Larger text objects need
     higher-dimensionality neural network layers since each unique instance needs
     its own representation, but they can more readily be used to model time
     dependencies. Smaller text objects like single characters only need a small
     vocabulary (in this example, each letter of the alphabet and perhaps some
     punctuation characters) but a more complex network structure is needed to
     capture time dependencies. In this case, words were chosen as text object
     because of their interpretability and popularity.

     # there are bigger things in there. yesyes. you see. when its bigger it
     # needs more space hÃ©. So yes. The first design choice from above is clearly
     # very good. (ahja, dazietiedereen). dus okay then. if we take a closer look
     # we'll see some interesting things there. euh ja. uhu.

[fn:one-hot] A one-hot encoding represents a word as an \(n\)-dimensional
    vector, where $n$ is the number of words in the vocabulary. A word is
    represented by the vector that is 1 at the index of that word in the
    vocabulary and 0 everywhere else.


     When splitting the sentence, the text is pre-processed by removing
     capitalization and special characters. The same transformation is used to
     create the vocabulary file, which contains the 512 words that appear most in
     the dataset.

     The embedding step is a simple linear layer that is trained along with the
     rest of the network. It can be expected that, after training, the embedded
     representations of words with similar gestures would be close to each other.
    
     The final step is a single-layer gls:rnn with Gated Recurrent Unit (GRU) cells,
     which perform better than plain gls:rnn cells on longer sequences and are less
     complex to train cite:chung14_empir_evaluat_gated_recur_neural. 
    
**** Pretrained encoder

     The pretrained encoder uses Google's Universal Sentence Encoder
     cite:cer18_univer_senten_encod available in pretrained form with TensorFlow
     Hub cite:tfhub. This module reads the entire sentence at once and returns a
     512-dimensional vector.

     # A /recurrent/ neural network, however, is able to handle this kind of data.
     # This kind of network introduces a "time" dimension that can vary across
     # inputs.

     # In practice, though, there is a caveat. Training data is not processed
     # sample-per-sample, rather minibatches of samples are created and all elements
     # in the minibatch pass through the network at once. This improves training
     # speed cite:TODO but re-introduces some of the problems mentioned above. While
     # inputs of different length do not have an impact on the /result/ of the
     # network, batching inputs of different lengths can slow down the training
     # process.
    
     # Recurrent neural networks have difficulty with long-term dependencies (LSTM
     # networks fare somewhat better) because of the large amount of operations in
     # between the input data and the corresponding output: the data from one time
     # step is passed through all the data from later time steps and combined into
     # the intermediate state, which needs to be "unrolled" again.
    
     # An attentional network is one in which weights act as "keys" to select
     # specific time steps of the input, implemented as a weight vector that is
     # multiplied element-wise with the inputs. This weight vector is a variable
     # that is optimized.
    
*** The Decoder Network
    label:sec:decoder
    
    Now that the hidden representation of the subtitle has been created, this
    can be decoded to predict a class or sequence of poses.
    
**** Classification decoder 
    
     The simplest decoder returns a probability for each of the classes by
     adding the following layers to the hidden representation (visualized in
     cref:fig:class-decoder):

     1. A dropout layer for regularization
     2. An intermediate fully-connected layer with ReLU activation
     3. A fully-connected layer with ReLU activation, representing the classes'
        logits

     The loss function is the softmax cross entropy with the labels in a one-hot
     encoding.
    
     #+BEGIN_SRC latex :exports results :results output
       \begin{figure}
         \centering
         \begin{tikzpicture}[semithick, font=\headingfont]
           \matrix [row sep=0.8cm, column sep=1.2cm] {
             \node (logits) [variable block, align=center] {Output\\{\small(Class Probabilities)}}; \\
             \node (dense2) [block] {Dense}; \\
             \node (dense1) [block] {Dense}; \\
             \node (dropout) [block] {Dropout}; \\
             \node (thought) [variable block, align=center] {Input\\{\small(Thought Vector)}}; \\
           };

           \path [->, thick]
             (thought) edge (dropout)
             (dropout) edge (dense1)
             (dense1) edge (dense2)
             (dense2) edge (logits)
           ;

         \end{tikzpicture}
         \caption{\label{fig:class-decoder} The class decoder decodes a thought vector
           into a probability for each class.}
       \end{figure}
     #+END_SRC
    
    
**** Sequence decoder
    
     The other decoder directly generates gestures. The challenges when
     implementing this decoder are:

     - Generating sequential data which is of a different data type than the
       input sequence (gestures are sequences of vectors in a continuous space,
       while words are discrete).
     - Learning the length of the sequence to be generated.
     - Learning dependencies between the input sequence and output sequence, when
       their length scales are different.

     Previous work on models that input and output sequences, like the ~seq2seq~
     framework for TensorFlow built by Google engineers as a general-purpose
     encoder-decoder framework cite:Britz:2017 and the Seq2seq Library built-in
     to TensorFlow cite:tf-seq2seq-library are built to generate sequences of
     symbols (such as word objects) instead of sequences of continuous values.
     Thus, while they can easily be used for various text generation tasks like
     machine translation and image captioning, they cannot generate continuous
     values like poses.
    
     In that sense, gesture synthesis is more akin to speech synthesis than
     machine translation. One advantage of gesture synthesis over speech
     synthesis is the much smaller time resolution: Google's WaveNet
     text-to-speech synthesis network, for example, generates audio at 24,000
     frames per second cite:oord17_paral_waven. As opposed to sound, motion
     appears smooth to human perception at 25 frames per second and this might
     even be reduced if a good interpolation of poses can be found. Note though
     that poses have higher dimensionality than sound: a sound sample is a
     single value while a pose is, in this case, a value per joint. Still, the
     dimensionality per unit of time for gesture synthesis is two orders of
     magnitude smaller than for raw audio synthesis.
    
     With that in mind, neither the text synthesis or speech synthesis systems
     available can be easily adapted for this task but a specialized decoder
     shall be built. Cref:fig:sequence-decoder shows a diagram of this
     decoder. The data that flows through an gls:rnn is divided into two parts:
     the input/output and the state. Both of these are updated in every time step
     of processing but the state is internal. Intuitively, one can interpret the
     state as the context while the input represents the progress of decoding.

     In the sequence decoder, the initial state is given by the thought vector as
     created by the encoder. The input is the pose from the previous time step
     but in practice, this differs depending on the network structure. During the
     training step, the ground truth pose is passed as input and during
     inference, the generated pose from the previous time step is fed back into
     the cell.
    
     Similar to previous research that animated 3D face meshes based on audio
     input cite:karras17_audio_driven_facial_animat_by, the loss function used is
     a sum of two terms: the /position loss/ and the /motion loss/. The position
     loss is the squared error between the predicted pose and the ground truth
     pose, while the motion loss measures the squared error of the difference
     between consecutive frames. This way, the network is explicitly forced to
     learn the correct speed of motion as well as the position of the joints.
     These terms, defined in terms of the network input $x$ are, respectively:

     \begin{align*}
       P: x \mapsto &\sum_{t=0}^{T(x)-1}\sum_{i=0}^{n-1} \Big[ y_i^{(t)}(x) - \hat{y}_i^{(t)}(x) \Big]^2 \\
       M: x \mapsto &\sum_{t=0}^{T(x)-1}\sum_{i=0}^{n-1} \Big[ \big(y_i^{(t)}(x) - y_i^{(t-1)}(x)\big) - 
                    \big(\hat{y}_i^{(t)}(x) - \hat{y}_i^{(t-1)}(x)\big) \Big]^2.
     \end{align*}
    
     Here, we defined $y$ and $\hat{y}$ as the functions that map the input to
     the ground truth output and the network's prediction respectively. The
     output of both of these functions is a temporal sequence of /frames/
     $y^{(t)}, t \in \{0,\ldots,T(x)-1\}$, where $T$ is the length of the ground
     truth sequence and thus dependent on $x$, and where each frame is a vector
     of $n$ joints $y^{(t)}_i, i \in \{0,\ldots,n-1\}$.

     Since the length of the time sequence in the output cannot be directly
     determined from the input, the network needs to learn this as well. To
     achieve this, an extra dimension is added to the gesture data containing
     the percentage of time left until the end of the sequence. For example, when
     a gesture is 50 frames long, the value of the vector in the 11th timestep
     for this dimension is $1 - (1/50) \cdot 10 = 0.8$. The same loss function
     can be used to learn this dimension.
    
     #+BEGIN_SRC latex :exports results :results output
       \begin{figure}
         \centering
         \begin{tikzpicture}[->, thick, x=\smallgridsize, y=\smallgridsize, font=\headingfont, center coordinate=(cell)]
           \node (cell) [block] {\Gls{rnn} Cell};

           \draw
             (cell.160)++(-2, 0) node(state) [variable block, left, align=center, yshift=0.25\smallgridsize] {Initial State\\{\small(Thought Vector)}}
             -- node(statejoincontrol)[pos=0.3]{}
                node(statejoin)[pos=1.0]{}
                (cell.160);

           \draw[->]
             (cell.200)++(-2, 0) node(input) [left] {Input}
             -- node(inputjoincontrol)[pos=0.3]{}
                node(inputjoin)[pos=1.0]{}
                (cell.200);

           \draw[->] 
             (cell.340)
             -- node(outputjoin)[pos=0]{}
                ++(2, 0) node(output)   [right] {Output};

           \draw[draw-blue]
             (input.east)++(0, -1)
             node (groundtruth) [left, align=right] {Ground Truth\\{\small(Poses)}} 
             .. controls +(1, 0) and (inputjoincontrol) .. (inputjoin.center);

           \draw[->,looseness=3] (cell.20) to[out=0, in=0] ([shift=(up:1)] cell.north) node [above] {State} to[out=180, in=180] (statejoin.center);
           \draw[looseness=3, draw-green] (outputjoin.center) to[out=0, in=0] ([shift=(down:1)] cell.south) to[out=180, in=180] (inputjoin.center);

         \end{tikzpicture}
         \caption{\label{fig:sequence-decoder} The sequence decoder decodes a thought
           vector into a sequence of poses. Green lines are connections present during
           inference, while blue lines are present during training.}
       \end{figure}
     #+END_SRC

     One property of this encoder-decoder structure that is at least intuitively
     restrictive is that the entire input sequence must be summarized as a single
     vector and then expanded again by the decoder---especially considering that
     people probably do not choose their gestures based on the summarized
     "meaning" of the sentence they are saying, but rather as a combination of
     gestures based on the words in that sentence.
    
     In response to this observation in the fields of image caption generation
     and machine translation, for example, /attention-based models/ were proposed
     as a solution
     cite:wu16_googl_neural_machin_trans_system,xu15_show_atten_tell,bahdanau14_neural_machin_trans_by_joint.
     These structures are most often used in combination with glspl:rnn and
     have more recently even been used on their own
     cite:vaswani17_atten_is_all_you_need. An attention model creates, for each
     time step in the decoder, a /context vector/ that is a weighted combination
     of the intermediate outputs from the gls:rnn in the encoder step (referred
     to as /memory/). These weights are trainable and based on the previous state
     from the gls:rnn. Then, the result is assigned to the new state of the
     recurrent network. Cref:fig:attention-decoder shows how this works
     conceptually.
     
     #+BEGIN_SRC latex :exports results :results output
       \begin{figure}
         \centering
         \begin{tikzpicture}[->, thick, x=\smallgridsize, y=\smallgridsize, node distance=\smallgridsize, font=\headingfont, center coordinate=(cell)]

           \node (cell) [block] {\Gls{rnn} Cell};
           \node (attention) [block, above=\smallgridsize of cell] {Attention Mechanism};
           \node (memory) [variable block, above=\smallgridsize of attention, align=center] {Memory\\{\small(Outputs from encoder)}};

           % Input Line
           \draw[->]
             (cell.200)++(-2, 0) node(input) [left] {Input}
             -- node(inputjoincontrol)[pos=0.3]{}
                node(inputjoin)[pos=1.0]{}
                (cell.200);

           % Output Line
           \draw[->] 
             (cell.340)
             -- node(outputjoin)[pos=0]{}
                ++(2, 0) node(output)   [right] {Output};

           % Right State Line
           \draw[->,looseness=3] (cell.20)
                to[out=0, in=0]
                node [right] {State} (attention.east);

           % Left State Line
           \draw[->,looseness=3] (attention.west)
                to[out=180, in=180]
                node [left] {State} (cell.160);

           % Input Loop
           \draw[looseness=3] (outputjoin.center) to[out=0, in=0] ([shift=(down:1)] cell.south) to[out=180, in=180] (inputjoin.center);

           % Memory Lines
           \draw[->] ([xshift=-0.75\smallgridsize] memory.south) -- ([xshift=-0.75\smallgridsize] attention.north);
           \draw[->] ([xshift=-0.5\smallgridsize] memory.south) -- ([xshift=-0.5\smallgridsize] attention.north);
           \draw[->] ([xshift=0.75\smallgridsize] memory.south) -- ([xshift=0.75\smallgridsize] attention.north);
           \draw[->] ([xshift=0.5\smallgridsize] memory.south) -- ([xshift=0.5\smallgridsize] attention.north);

           \node at ($(memory)!0.5!(attention)$) {$\cdots$};

         \end{tikzpicture}
         \caption{\label{fig:attention-decoder} The attention-based decoder calculates
           its new state by taking a weighted combination of all the outputs from the
           encoder. Details about training/inference differences and initial states
           omitted for clarity.}
       \end{figure}
     #+END_SRC

** Performing Gestures on a Robot
   :PROPERTIES:
   :Effort:   2:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-27 vr 08:39]--[2018-07-27 vr 09:18] =>  0:39
   :END:
   label:sec:playback
   
   When researching the dataset and data formats in
   cref:sec:research-dataset, it was decided that the representation of poses
   throughout the project would be based on the joint angles of the NAO robot,
   so they can directly be used on this robot.
   
   During most of the project, the results were tested on a virtual robot.
   SoftBank provides the /Choregraphe/ application cite:softbank_tools which
   provides a visual programming interface and hosts a simulated robot with a 3D
   view. 

   Softbank's robots can be controlled over the local network, either wired or
   over Wi-Fi. They provide the Python NAOqi API that adds an abstraction layer
   to make controlling the robot straightforward. The simulation in Choregraphe
   creates a virtual robot on the local machine and behaves mostly similar to
   the real robot, except that:

   - There is no network involved outside the development machine.
   - The simulation does not include gravity or other environmental physics. The
     robot will thus never fall over and its torso is always displayed upright.
   - Some of NAO's capabilities cannot be simulated.
   
   #+CAPTION: label:fig:nao-performing The NAO robot performing a generated gesture
   #+NAME: fig:nao-performing
   #+ATTR_LATEX: :width 6\smallgridsize :float wrap :placement {R}[2\smallgridsize]{6\smallgridsize}
   [[file:./img/nao-performing.png]]
   
   Of these capabilities not supported in the simulation, though, two are
   critical for this project: the gesture synthesis and the text-to-speech
   engines. This means that to compare NAOqi's existing gesture synthesis system
   with the ones created here a physical robot is needed.
   
   Controlling the robot's pose is done by setting the value of each joint's
   angle through this API. Though these angles might contain positions that are
   not reachable by the robot or move too quickly, the NAO will move in a
   best-effort way. The commands to specify position are asynchronous so they
   can be sent frame-by-frame while the robot will try to keep up as best as he
   can. Thus, the robot's pose is sent in real time, frame-by-frame.
   
   In a production environment, this would not be a good solution. Spotty
   network connections and a dependency on a separate computer for gesture
   synthesis will make it diffifult in real life situations. Eventually, the
   best solution would be that the gesture synthesis system is part of the robot
   and has direct control over its pose. For this research, though, the method
   used here works well.

*** TODO Figure: screenshot of Choregraphe
   
*** TODO Figure: photo of NAO
* Evaluation
  SCHEDULED: <2018-06-28 do>--<2018-06-29 vr>
  :LOGBOOK:
  CLOCK: [2018-07-27 vr 09:18]--[2018-07-27 vr 11:56] =>  2:38
  :END:
  label:chap:evaluation
  
  While the metrics and many "sanity checks" used up until now give some
  confidence that the gestures generated look natural, that conclusion is
  clearly subjective and should be verified with a proper test. This section
  starts by covering the evaluation and optimization of the neural network
  discussed previously, after which it explains the setup and results for a
  Turing-like test that was performed to validate the results produced
  qualitatively by sending out a survey. Before that, however, a note on finding
  a good evaluation metric is in order.

** Defining Good Results
   
   One of the biggest challenges in this project and machine learning in general
   is defining what a "good result" is. In this case it is especially
   ill-defined since human perception is involved and body language is by no
   means a formal language. The most "real" measure of success would be
   something like /"the majority of people agree that this robot gestures in a
   natural way"/---which is not a precise measure and is influenced by a large
   amount of factors we cannot control like culture differences, the physical
   shape of the robot and the text-to-speech engine it uses.
   
   Yet, without an accessible measure it is not possible to compare different
   results or have an idea about whether the outcome is actually something
   useful. Three different "levels" of metrics are used throughout the project:

   - Quick-and-dirty sanity checks :: The figures throughout cref:chap:method
        provide a quick, qualitative pass-or-fail answer that make sure the
        results are at least somewhat meaningful. This was necessary because of
        the many steps in the pipeline and to prevent needing to debug this
        entire process at once.
   - Prediction loss functions :: The gesture synthesis models are trained and
        evaluated based on their loss functions as defined in
        cref:sec:pose-prediction. These hard numbers drive the learning
        mechanisms but differ from the ideal perception-based metric. They also
        differ between decoders. In cref:sec:decoder-comparison, a single metric
        will be described to compare the decoders.
   - The survey :: By asking people's opinion, human perception is accounted for
                   and the final results can be evaluated. Unfortunately, this
                   is time-consuming for participants so the amount of
                   approaches that can be compared is very limited. It is also
                   difficult to measure the external factors as mentioned above.

   Next, the loss functions will be used to evaluate the neural network, compare
   its architectures and optimize its hyperparameters. After that, the survey
   setup and its results are described.

** Optimizing the Neural Network
  
   The gesture synthesis network has quite a few parameters, the first of which
   is the network structure itself. \Cref{sec:pose-prediction} presented
   the network as an encoder-decoder architecture and explained two encoders (a
   recurrent text encoder and a pre-trained black box encoder) and two decoders
   (a classification decoder and a sequence decoder). This results in four
   possible network architectures.
  
   There is one special case in these four architectures. Since the
   attention-based decoder model (see cref:sec:decoder) requires access to the
   intermediate results of the encoder, it cannot be used in combination with
   the black box pre-trained text encoder. In that case, the decoder falls back
   to a "regular" recurrent neural network that only uses the final output of
   the encoder to generate its predictions.
  
   The rest of this section shows the results from tweaking the hyperparameters
   of the encoder and decoder networks. In these results, the differences
   between these high-level network architectures will be shown as much as
   possible. In order to do that, we first need to find a metric that can
   compare the results from the two decoders who have different types of
   outputs.
    
*** Comparing the Classification and Sequence Decoders
    :PROPERTIES:
    :Effort:   2:00
    :END:
    label:sec:decoder-comparison

    The loss function for the classification decoder is the softmax cross
    entropy function, which measures the classification error, and for the
    sequence decoder the generated gesture is compared to the ground truth
    gesture directly. This gesture loss can be used as the shared metric when
    we generate gestures in the classification decoder by substituting the
    predicted cluster centers. (Note that this is only during evaluation, the
    training step still uses the classification loss.) This allows for a common
    ground to compare the decoders but it implicitly evaluates the clustering
    method too. Because of the limited amount of clusters, there is thus a lower
    bound for the gesture loss from the classification decoder: even if all
    clusters were predicted with perfect accuracy, there would still be a loss
    because of the "simplification" made by the clustering algorithm.

    Directly comparing the generated gestures with the ground truth data from the
    dataset implies that there is only a single gesture possible (perhaps with
    some noise) for each part of text, which is clearly not the case for humans.
    A better solution would be to use an adversarial network model that has two
    components: one that learns to predict gestures and another that learns what
    kind of gestures humans perceive as being "natural". Sadly this approach was
    determined as out of scope for the current project and is thus recommended as
    one of the next steps in future works towards gesture synthesis
    (cref:chap:conclusion).
    
    In order to compare the two decoders, this leaves us with comparing the
    generated gestures using the position-motion-loss from the sequence decoder.
    This is not a completely fair game since the minimum loss the classification
    decoder can achieve is the loss during the clustering step, which is also
    determined by the amount of clusters created.
    
    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}
        \centering
        \begin{tikzpicture}
          \begin{semilogyaxis}[
            xlabel = {Output Type},
            ylabel = {Gesture Loss},
            % ymode = log,
            symbolic x coords = {sequences, classes},
            cycle list name = default filled,
            xtick distance = 1,
            xtick = data,
            width = 2\gridsize,
            enlarge x limits = 1,
            % log ticks with fixed point,
            % y tick label style={/pgf/number format/1000 sep=\,},
          ]
            \addplot+[
              only marks,
              discard if not = {motion_loss_weight}{0.5},
            ] table [x=output_type, y=gesture_loss, col sep=comma] {./img/hp-results.csv};
          \end{semilogyaxis}
        \end{tikzpicture}
        \caption{\label{fig:classification-vs-sequence} The difference in gesture
          error for the classification and sequence decoders.}
      \end{figure}
    #+END_SRC
    
    Cref:fig:classification-vs-sequence shows the results on the validation
    set. The model with the sequence decoder clearly learns to predict the
    gestures while the classification decoder does not learn (in terms of its
    own loss function this decoder immediately starts to overfit the training
    data).
   
*** Hidden Size
    
    The /hidden size/ is defined as the size of the hidden representation in
    between the encoder and decoder architectures. As displayed in
    cref:fig:experiment-hidden-size, changing this parameter only has a small
    impact on the loss function unless, for the sequence decoder, this size
    becomes very large. 

    Note that, in this and the following figures, multiple data points are
    plotted for each value in the $x$ axis. These data points represent the
    results from various other parameters. Since some parameter values have a
    very large effect on the loss (such as a hidden size that is too large for
    the sequence decoder), these "outlier" values will often be ommited in the
    following figures. Refer to the figure captions to confirm if that is the
    case.
    
    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}
        \adjustbox{center}{
          \begin{tikzpicture}
            \begin{groupplot}[
              group style={
                group name = myplot,
                group size = 2 by 1,
                horizontal sep = 2\smallgridsize
              },
              % ybar,
              xlabel = {Hidden Size},
              ylabel = {Gesture Loss},
              xtick = data,
              ymode = log,
              restrict y to domain=-inf:50,
              width = 8\smallgridsize,
              cycle list name = default filled,
              ]

              \nextgroupplot[title = Sequences]
              \addplot+[only marks,
                discard if not = {output_type}{sequences},
                discard if not = {motion_loss_weight}{0.5},
              ] table [x=hidden_size, y=gesture_loss, col sep=comma] {./img/hp-results.csv};

              \nextgroupplot[title = Classes]
              \addplot+[only marks,
                discard if not = {output_type}{classes},
                discard if not = {motion_loss_weight}{0.5},
              ] table [x=hidden_size, y=gesture_loss, col sep=comma] {./img/hp-results.csv};

            \end{groupplot}
          \end{tikzpicture}
        }
        \caption{\label{fig:experiment-hidden-size} The the size of the hidden
          representation in the encoder-decoder architecture has little effect on the
          evaluation loss. In case of the sequence decoder, a very large hidden size
          resulted in a network that was unable to converge. (Some even larger points
          were omitted for clarity.)}
      \end{figure}
    #+END_SRC

*** Using the Pretrained Encoder

    Cref:fig:TODO shows the effect of using the pretrained encoder instead of a
    custom encoder to process the subtitle. For the classification decoder, the
    difference is small but for the sequence decoder, /not/ using the pretrained
    encoder improves the model's performance significantly. This is probably due
    to the fact that the attentional system can be used.
    
*** Motion Loss Weight

    When the gesture loss is calculated, a weighted average of the position loss
    and motion loss is taken. Important to note here is the scale difference:
    since the motion loss operates on the difference between two frames, its
    value is much smaller than the position loss. The position loss is usually
    about 10 times larger than the motion loss. Thus, choosing the value for the
    motion loss weight is a difficult task since the usual metrics cannot be
    used. This is even intuitively a difficult question to answer: for humanlike
    gestures, how much more (or less) important is moving at the right speed
    versus holding your hands at the right position?
    
    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}
        \adjustbox{center}{
          \begin{tikzpicture}
            \begin{groupplot}[
              group style={
                group name = myplot,
                group size = 2 by 1,
                horizontal sep = 2\smallgridsize
              },
              % ybar,
              xlabel = {Motion Loss Weight},
              ylabel = {Gesture Loss},
              xtick = data,
              restrict y to domain=-inf:10,
              ymode = log,
              width = 8\smallgridsize,
              cycle list name = default filled,
              ]

              \nextgroupplot[title = Sequences]
              \addplot+[only marks, restrict y to domain = -inf:1,
                % discard if not = {motion_loss_weight}{0.5},
                % discard if not = {dropout}{0.5},
                % discard if not = {embedding_size}{512},
                discard if not = {output_type}{sequences}
              ] table [x=motion_loss_weight, y=gesture_loss, col sep=comma] {./img/hp-results.csv};

              \nextgroupplot[title = Classes]
              \addplot+[only marks,
                % discard if not = {motion_loss_weight}{0.5},
                % discard if not = {dropout}{0.5},
                % discard if not = {embedding_size}{512},
                discard if not = {output_type}{classes}
              ] table [x=motion_loss_weight, y=gesture_loss, col sep=comma] {./img/hp-results.csv};

            \end{groupplot}
          \end{tikzpicture}
        }
        \caption{\label{fig:experiment-motion-loss} The gesture loss appears to
          decrease linearly with increasing motion loss weight. Note, though, that
          this is probably a result in a difference of domain: the value of the motion
          loss is usually about 10 times smaller than the position loss.}
      \end{figure}
    #+END_SRC
    
*** Dropout
    
    Dropout layers are used throughout the model to prevent overfitting. A
    dropout layer removes a certain fraction of the intermediate values forcing
    the model to learn from incomplete data. In many applications, this
    technique has been proven very useful.
    
    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}
        \adjustbox{center}{
          \begin{tikzpicture}
            \begin{groupplot}[
              group style={
                group name = myplot,
                group size = 2 by 1,
                horizontal sep = 2\smallgridsize
              },
              % ybar,
              xlabel = {Motion Loss Weight},
              ylabel = {Gesture Loss},
              xtick = data,
              ymode = log,
              width = 8\smallgridsize,
              cycle list name = default filled,
              ]

              \nextgroupplot[title = Sequences]
              \addplot+[only marks, restrict y to domain = -inf:1,
                discard if not = {output_type}{sequences},
                discard if not = {motion_loss_weight}{0.5},
              ] table [x=dropout, y=gesture_loss, col sep=comma] {./img/hp-results.csv};

              \nextgroupplot[title = Classes]
              \addplot+[only marks, restrict y to domain = -inf:0.5,
                discard if not = {output_type}{classes},
                discard if not = {motion_loss_weight}{0.5},
              ] table [x=dropout, y=gesture_loss, col sep=comma] {./img/hp-results.csv};

            \end{groupplot}
          \end{tikzpicture}
        }
        \caption{\label{fig:experiment-dropout} A larger amount of dropout has the
          effect that the loss becomes unbounded in less cases, but for the sequence
          decoder only.}
      \end{figure}
    #+END_SRC

** Looking At the Gestures

   While the results from optimizing the loss function clearly show the
   sequence-based decoder as better than the classification-based one, this
   result might not transfer to the qualitative evaluation of these gestures.
   The sequence-based indeed predicts a gesture that looks natural, it tends to
   slow down and stop quickly. At the same time, the network seems to have
   difficulty predicting the sequence length parameter, resulting in a long
   sequence that is stopped after the maximum threshold is reached. Thus, in
   practice, the results from the sequence-based decoder are very still.
   Cref:sec:survey will verify whether those observations match the opinion of
   other people.
   
   The metrics used while training show a much higher gesture loss for the
   classification-based decoder but the results look humanlike simply because
   the cluster centroids are actual samples of the data. Thus, they do not
   correspond with the way the actual gestures were performed in the dataset but
   they do look like "a moving human."
   
   In a real-life context, when including the subtitle, the robot is pronouncing
   the text wile performing its gestures. Since this project does not include
   any interaction between those engines, gestures do not match up with the
   text-to-speech engine. Previous gesture synthesis systems that could
   synchronize motion with the text required annotation but this system has the
   opportunity to learn speech pronunciation along with the gestures. Alas, that
   is not currently included. It can thus be expected that the length of the
   generated gesture will differ from the length of the TTS engine (unless the
   audio clip is manipulated to match the predicted gesture).

** Survey label:sec:survey

   A better metric to validate this method is with qualitative results from
   people, obtained through a survey. This section desribes the survey that was
   created and its results.

   An online survey was created that includes the following sections:

   1. An introduction with an explanation of the project and the task
   2. An attention checker question, where users are given the simple question
      "what is your gender?" while they are instructed, in a long block of
      text, to answer "orange penguin" instead of the true answer.
   3. Six questions where
      - A video was shown with a NAO robot performing four different animations
        for the same subtitle: the ground truth, a baseline (the default NAO
        animation), the result from the classification-based prediction and the
        result from the sequence-based prediction.
      - Users were asked to rate the human-ness of each animation on a
        five-point scale (from "stiff, robot-like" to "humanlike")
      - Users were asked which of the animations they prefer
   4. A final checker question that asked the color of the robot's shoulders

   A screenshot of this questionnaire is shown in
   cref:fig:survey-screenshot.
    
   #+caption: label:fig:survey-screenshot Screenshot of one of the main questions from the survey used to qualitatively test the results of these gesture synthesis methods.
   #+attr_latex: :width 1.2\textwidth,center
   [[file:./img/survey-screenshot.png]]

   A few respondents provided feedback about the survey and said they found it
   difficult to compare. The video had an audio track recorded from NAO's
   text-to-speech engine while it was performing its own gestures but the timing
   of this does not match up with the other gestures, since every method decides
   the output length by itself. Perhaps a better solution would be to present
   respondents with a single video per gesture synthesis system, where the speed
   of the text-to-speech system is modified to match with the generated gesture.
    
* Conclusions and Future Work
  :PROPERTIES:
  :Effort:   2:00
  :END:
  label:chap:conclusion

  Deep learning methods can be used effectively to synthesize gestures that a
  robot performs while it is talking. The pipeline introduced in this thesis
  builds a dataset from videos that are freely available on the internet,
  allowing a dataset to be created of arbitraty size, and built a
  proof-of-concept model that shows this method, which uses a recurrent neural
  network-based encoder-decoder architecture to directly synthesize gestures,
  can lead to satisfying results.

  A survey was proposed and the results provided as a baseline, who showed
  that...

  While the results shown here are preliminary, they are promising and ample
  opportunities for improvement are presented. A machine learning-based model
  such as the one presented in this thesis is likely the best road to a gesture
  synthesis system that fulfills the desirable properties of continuity,
  variability and congruence.
  
** The Outlook

   This thesis is only a starting point. It proposes a new type of method,
   rooted in modern technological developments, to synthesize gestures for
   social robots and provides a proof of concept, but a lot more work is needed
   to be able to use this in the real world. Below, a few possible improvements
   are listed, roughly in order of declining expected improvement:

   /Collect more data./ The dataset used here is very small. A larger dataset is
   probably a prerequisite for most of the other improvements.
   
   /Add samples for specific gestures to the dataset./ Explicitly adding samples
   for specific iconic gestures, like waving while saying "hello" or nodding
   while saying "yes," could teach this system to generate these gestures.
        
   /Find or create and implement a clustering algorithm that clusters
   subsequences./ One major drawback of the clustering algorithm used in this
   thesis is that it compares entire clips at once. With an algroithm that can
   cluster subsequences, true glspl:motion-primitive could be found.

   /Build a Generative Adversarial Network around the current network./ Building
   an adversarial network allows the machine learning model to train towards a
   goal that is closer to the actual goal, that is, perceived human-ness. With a
   GAN, it is no longer implied that the same input text has to produce the same
   gesture.
   
   /Combine gesture synthesis with speech synthesis./ People are quite
   sensitive to how the timing of gestures lines up with the spoken text. In
   human conversations, the content of the communication is delivered in a
   multimodal way, using both speech and gesture cooperatively. To be able to
   communicate in the same seamless manner as people, robots should have an
   integrated system that synthesizes both of these modalities simultaneously.

   /Use contextual features./ The style of gestures varies widely across
   communicative intent and people. With additional context, the robot could
   know, for example, that it needs to move more slowly when comforting another
   person or make big, enthusiastic gestures when he is trying to sell a product
   to an extroverted person.

** TODO Checklist
   - [ ] Findings: What are the main results?
   - [ ] Take-home message: What should the reader remember?
   - [ ] Future work: what is the outlook?

* References
  :PROPERTIES:
  :UNNUMBERED: t
  :END:
  
  @@latex: \addcontentsline{toc}{chapter}{References}@@
  
  \printbibliography[heading=none]
  
* Glossary
  :PROPERTIES:
  :UNNUMBERED: t
  :END:
  
  @@latex: \addcontentsline{toc}{chapter}{Glossary}@@
  
  \renewcommand{\glossarysection}[2][]{}
  \printglossaries
