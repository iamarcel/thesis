# -*- org-confirm-babel-evaluate: nil -*-
#+TITLE: Autonomous Production of Gestures on a Social Robot using Deep Learning
#+AUTHOR: Marcel Samyn
#+COLUMNS: %4TODO(To Do) %40ITEM(Task)  %12EFFORT(Effort){:}
#+OPTIONS: tasks:nil ':t
#+LATEX_CLASS: report-noparts
#+LATEX_HEADER_EXTRA: \input{ugent.tex}

#+BEGIN_SRC emacs-lisp :exports none :session python-env
  ;; If you have the pipenv package, this initalializes the environment so that
  ;; Python packages are all present.
  (pipenv-mode)
#+END_SRC

#+BEGIN_SRC latex
  \usetikzlibrary{shapes,arrows,backgrounds,fit,calc}

  \tikzset{
    center coordinate/.style={
      execute at end picture={
        \path ([rotate around={180:#1}]perpendicular cs: horizontal line through={#1},
                                    vertical line through={(current bounding box.east)})
              ([rotate around={180:#1}]perpendicular cs: horizontal line through={#1},
                                    vertical line through={(current bounding box.west)});}}}

  \tikzset{
    font=\headingfont,
    every picture/.style={/utils/exec={\headingfont}},
    x= \smallgridsize, y= \smallgridsize,
    node distance=\smallgridsize,
    semithick,
    default matrix/.style = {
      row sep = \smallgridsize,
      column sep = \smallgridsize,
      align = center,
      nodes = {
        anchor = center,
        align = center
      }
    }
  }

  \pgfplotscreateplotcyclelist{default}{
    thick, ugent-blue\\%
    thick, ugent-green\\%
    thick, ugent-orange\\%
  }

  \pgfplotsset{
    compat=newest,
    tick label style = {font=\headingfont, black!60},
    every axis label = {font=\headingfont, black},
    every axis legend/.append style = {legend cell align = left},
    legend style = {
      font=\headingfont\small, 
      draw=none, 
      text width=3\smallgridsize, 
      align=left,
      row sep=1ex,
      /tikz/every odd column/.style={yshift=0.5ex},
      /tikz/nodes={text depth=,anchor=base},
    },
    legend pos = outer north east,
    legend cell align = left,
    label style = {font=\headingfont},
    width = 12\smallgridsize,
    height = 6\smallgridsize,
    axis x line = bottom,
    axis y line = left,
    axis line style = {thick, black!60},
    every x tick/.style={thick, black!60},
    every y tick/.style={thick, black!60},
    enlargelimits = 0.1,
    cycle list name = default
  }

  \tikzstyle{draw-blue} = [
    draw=ugent-blue
  ]

  \tikzstyle{fill-blue} = [
    fill=ugent-blue!10
  ]

  \tikzstyle{draw-green} = [
    draw=ugent-green
  ]

  \tikzstyle{fill-green} = [
    fill=ugent-green!10
  ]

  \tikzstyle{draw-orange} = [
    draw=ugent-orange
  ]

  \tikzstyle{fill-orange} = [
    fill=ugent-orange!10
  ]

  \tikzstyle{draw-black} = [
    draw=black!60
  ]

  \tikzstyle{fill-black} = [
    fill=black!6
  ]

  \tikzstyle{base-block} = [
    rectangle,
    font=\headingfont,
    thick, minimum width=1.5cm, minimum height=1cm,
    inner xsep=12pt, inner ysep=6pt
  ]

  \tikzstyle{block} = [
    base-block,
    rounded rectangle,
    draw-black, fill-black
  ]

  \tikzstyle{blue-block} = [
    block, 
    draw-blue, fill-blue
  ]

  \tikzstyle{variable block} = [
    base-block,
    draw-orange, fill-orange
  ]

  \tikzstyle{inline spacing} = [
    inner xsep=4pt, inner ysep=2pt,
    minimum width=0pt, minimum height=0pt
  ]

  \tikzstyle{tight spacing} = [
    inner xsep=6pt, inner ysep=4pt,
    minimum width=0pt, minimum height=0pt
  ]

  \newacronym{rnn}{RNN}{Recurrent Neural Network}

#+END_SRC

#+NAME: pgf_figure
#+BEGIN_SRC latex :var var_name="" var_caption="" :exports none
  \begin{figure}
    \centering
    \adjustbox{center}{%
      \input{./img/var_name.pgf}%
    }
    \caption{\label{fig:var_name} var_caption}
  \end{figure}
#+END_SRC



* Notes :noexport:

** Kan uitleggen waarom je sterk verschillende trainingsdata gebruikt

** Leg uit in de thesis: in het begin is het belangrijk dat we heel monotone beelden gebruiken

** Voorlopig is het waarschijnlijk best dat je de monologen gebruikt. Vermeld de use-case:monoloog voor een publiek

** Leg uit hoe je nieuwe trainingsdata kan maken

** Erken probleem in de clustering: de gebaren die er uit komen zin /gemiddelden/ (lauwe gebaren)
   Iconische gebaren verdwijnen met deze methode uit het repertoire van de robot.

** Vermeld dat bepaalde heel belangrijke elementen die we willen hebben, worden uitgemiddeld

*** Mogelijkheid: dataset biasen met extra trainignsdata

*** In RNN kan je bijvoorbeeld een veel hardere gradient met voor die specifieke woorden geven (1/0.03)

** Meet de afstanden van de clusters tot de ground truth (niet alleen klassen)
   DEADLINE: <2018-07-02 ma>
   Toon dat die afstand kleiner wordt. Dus twee evaluaties:

   1. Toon dat er geen bug zit in je algoritme, dat ze wel dichter komen bij de trainingsdata
   2. Tonen met mensen

** Vergelijk gelijkaardige zinnen
   maak een 100-tal paren van zinnen die wel/niet op elkaar lijken en vergelijk
   dat met de output van je algoritme.
   
   Bijvoorbeeld: cluster uw zinnen en kijk of daar iets in zit

** Vraag voor mezelf: hoe kan ik meer tussentijds cijfers geven over hoe goed het werkt?

** 2-10 juli is buffer voor het extra werk dat Tony mee geeft

** Data storage
   - src
   - data
     - clusters.json: { class: frames[] }
* Development                                                      :noexport:
** DONE [#A] Maak precieze planning wat je nog moet doen
   CLOSED: [2018-06-21 do 11:51]
** DONE [#A] Stuur planning door
   CLOSED: [2018-06-21 do 18:27] DEADLINE: <2018-06-21 do>
** DONE Create evaluation questionnaire
   CLOSED: [2018-06-26 di 08:18] DEADLINE: <2018-06-27 wo> SCHEDULED: <2018-06-25 ma>--<2018-06-26 di>
   :LOGBOOK:
   CLOCK: [2018-07-03 di 14:34]--[2018-07-03 di 15:06] =>  0:32
   CLOCK: [2018-06-25 ma 15:47]--[2018-06-25 ma 17:53] =>  2:06
   CLOCK: [2018-06-25 ma 09:25]--[2018-06-25 ma 11:57] =>  2:32
   :END:
*** DONE Create a script to generate a TTS audio clip
    CLOSED: [2018-06-26 di 08:18]
    :PROPERTIES:
    :Effort:   3:00
    :END:
*** DONE Create comparison video (x6)
    CLOSED: [2018-06-26 di 08:18]
    :PROPERTIES:
    :Effort:   1:00
    :END:
    :LOGBOOK:
    CLOCK: [2018-06-23 za 13:24]--[2018-06-23 za 15:12] =>  1:48
    :END:
**** DONE Pick a random subtitle
     CLOSED: [2018-06-23 za 15:12]
**** DONE Generate TTS audio
     CLOSED: [2018-06-23 za 15:12]
**** DONE Record video clips
     CLOSED: [2018-06-23 za 15:12]
***** DONE Play back original gesture
      CLOSED: [2018-06-23 za 15:12]
***** DONE Play back NAO's generated gesture
      CLOSED: [2018-06-23 za 15:12]
***** DONE Play back chosen cluster
      CLOSED: [2018-06-23 za 15:12]
**** DONE Merge video clips
     CLOSED: [2018-06-23 za 15:12]
**** DONE Add audio clip to video
     CLOSED: [2018-06-23 za 15:12]
**** DONE Add subtitles to video
     CLOSED: [2018-06-23 za 15:12]
*** DONE Upload videos
    CLOSED: [2018-06-26 di 08:18]
    :PROPERTIES:
    :Effort:   1:00
    :END:
*** DONE Create questions for all videos
    CLOSED: [2018-06-26 di 08:18]
    - Embedded video
    - Score each
    - Which do you prefer?

**** TODO Duplicate previous question
**** TODO Replace video
*** TODO Add question: attention check
** DONE Try out the Java clustering algorithm
   CLOSED: [2018-06-26 di 16:18]
   :LOGBOOK:
   CLOCK: [2018-06-26 di 08:18]--[2018-06-26 di 12:04] =>  3:46
   :END:
** DONE Try other ways of clustering
   CLOSED: [2018-07-07 za 16:22]
   :LOGBOOK:
   CLOCK: [2018-07-03 di 09:08]--[2018-07-03 di 12:15] =>  3:07
   CLOCK: [2018-07-02 ma 19:16]--[2018-07-02 ma 20:30] =>  1:14
   CLOCK: [2018-07-02 ma 17:12]--[2018-07-02 ma 18:14] =>  1:02
   CLOCK: [2018-07-02 ma 16:39]--[2018-07-02 ma 16:45] =>  0:06
   CLOCK: [2018-06-27 wo 08:06]--[2018-06-27 wo 11:24] =>  3:18
   CLOCK: [2018-06-26 di 18:15]--[2018-06-26 di 18:26] =>  0:11
   CLOCK: [2018-06-26 di 16:18]--[2018-06-26 di 17:33] =>  1:15
   :END:
** TODO Send out questionnaire
** TODO Record a video of a live NAO gesturing
** TODO Record video of NAO telling a story
** DONE Improve sequence model
   CLOSED: [2018-07-20 vr 15:04]
   Hmm. Adding droput in the sequence decoder, right after the initial RNN cell,
   increases the max loss by 10x. Even if the dropout is 0. It then produces
   output independent of the subtitle, though. Hmm, maybe I did something wrong
   in the inference loop function.

*** DONE Add a mask dimension to the data
    CLOSED: [2018-07-20 vr 15:04]
*** DONE Stop predicting when mask says so
    CLOSED: [2018-07-20 vr 15:04]
*** DONE Eigen embedding
    CLOSED: [2018-07-20 vr 15:04]
**** DONE Maak een per-woord vocab
     CLOSED: [2018-06-19 di 15:26]
**** DONE embed woorden in vocab
     CLOSED: [2018-06-21 do 08:24]
**** DONE Encode die sequentie
     CLOSED: [2018-06-21 do 08:24]
**** DONE Decode + geef tussen-states mee aan decoder
     CLOSED: [2018-07-20 vr 15:04]

* TODO Extended Abstract
  :PROPERTIES:
  :Effort:   5:00
  :UNNUMBERED: t
  :END:

* TODO Preface
  :PROPERTIES:
  :UNNUMBERED: t
  :Effort:   2:00
  :END:
* Introduction
  :PROPERTIES:
  :Effort:   1:00
  :END:
  :LOGBOOK:
  CLOCK: [2018-07-07 za 16:23]--[2018-07-07 za 17:23] =>  1:00
  :END:

   For at least 2.6 million years, humans have been making tools. The ability to
   create sophisticated tools allowed us to find and process more energy-rich
   food sources which gave us the energy to evolve bigger, more slowly growing
   brains. That, in turn, improved our ability to invent new and more complex
   tools, creating the feedback loop that quickly made humans the dominant
   animal species on earth cite:lieberman12.

   The story of human toolmaking started with the humble Pointy Rock. The Pointy
   Rock was crafted by smacking together two less pointy rocks repeatedly so
   that at least one of them would start to chip off and become sharper. While
   crude compared to today's strict engineering practices and tolerances, the
   Pointy Rock helped our ancestors a tremendous amount. With this tool, one
   could kill prey, break bones to access valuable marrow and pulverize plants
   so they could be more easily digested.

   Since that point, humans have never stopped building tools. The desire to
   craft things that make our lives easier is deeply embedded in the human
   condition. In an ever-accelerating feedback loop, people have built tools
   with tools to build better tools.

   A prime motivator for making increasingly complex machines was (and still is)
   the desire for machines that could run completely autonomously. In the past
   few millenia machines that operated autonomously had been invented, like
   water clocks and Japanese Karakuri automatons (mechanized puppets), but these
   were mostly for display purposes and of little use. Widespread, practical use
   of automation started with the introduction of mechanized spinning machines
   in the 1780s during the industrial revolution
   cite:britannica-industrial-revolution.

   One more fundamental feature---the ability to /reprogram/ these
   machines---came with the Unimate, a machine that would today be distinctly
   recognizable as an industrial machine cite:robotics_unimate. It was the first
   digitally operated, programmable machine and this ability for it to be
   reprogrammed is why the Unimate can be confidently identified as a /robot/.
   Industrial and commercial robots are widespread today and excel at various
   tasks like car assembly, package sorting and vacuuming. More recently, robots
   that can perform perform complex human tasks like driving make headlines in
   technology news cite:wired-self-driving.

   Though industrial and specialized robots are useful, nothing captures our
   attention more than a human-like machine. The idea of artificial humans can
   be found in centuries-old legends like the Greek Talos, a bronze man that
   defended Crete or the clay golems of the Jews and Nordics. But it wasn't
   until the 1930s that real robots entered our popular culture. It started with
   not much more than ancient automata, simple humanlike machines that could
   perform a few "tricks", but development steadily continued and robots learned
   to walk, talk and interact with their environment.

   One of the most popular robots today, Pepper from Softbank Robotics, can make
   eye contact, read emotions from people's faces and adapt its behavior based
   on the moods of people he's talking with. Notable about these features is
   that they are /social/, they are built so that the robot can interact with
   the people in its environment in an active manner. Next to the challenges of
   locomotion (i.e., not falling and moving at an acceptable speed) and making
   robots /look/ like humans, making robots /behave/ like humans is a tremendous
   challenge.

*** DONE Reference for when automation started
    CLOSED: [2018-06-09 za 08:35]

** Social Robots

   Social robots can interact and communicate with humans by following the
   behavioral norms that their conversational partners expect cite:bartneckil.
   The power of these kinds of robots lies in the fact that the prerequisites
   for fluent interaction are essentially reversed: the human no longer needs to
   learn how to interact with the machine; rather, the machine learned how
   people naturally operate so that they can interact without any special skills
   or training.

   These robots have the potential to become our assistants and trusted
   sidekicks. In the form of humanoid nurses, smart toys or even small creatures
   to be carried around cite:breazeal04_desig, these machines would understand
   us intuitively, anticipate our needs and seamlessly integrate in our social
   world. They could become an integral part of this human-dominated world,
   understand us on an emotional level and carry out tasks that we cannot or
   prefer not to do.

   Modern robots are starting to look more humanlike and gaining basic human
   capabilities like the ability to walk, speak, see, listen and move objects.
   However, these are still rather technical foundations and a lot more effort
   is needed to let these machines communicate effortlessly with people.
   Creating the ability to interact socially is not easy. Concepts like body
   language and emotion---that have evolved over for millions of years in humans
   and are still an active research topic in psychology---have to be programmed
   into computers who are inherently built to act in a rational, logical and
   determinstic manner.

   However, that is not to say that this is impossible or far away in the
   future. Many robots exist today which vary in approach and ability to be
   social. Developed at the Massachusetts Institute of Technology by a team led
   by Dr. Cynthia Breazeal, the robotic head /Kismet/ was one of the earliest
   examples of a social robot. The developers knew that building a robot that
   behaves realistically like a human adult would be impossible at that point so
   Kismet was designed to appear and behave more like a baby. It could hear and
   speak but interpreted the /emotion/ of what was being said and spoke in a
   kind of proto-language similar to infants. This way, people interacting with
   Kismet naturally talked slower and were more expressive in their voice:
   Kismet managed to intuitively define the social context in which it could
   operate well. The goal of allowing interaction without training the user was
   achieved and the robot could still communicate its way of communication, in a
   way that was almost unnoticed by the people interacting with Kismet
   cite:breazeal04_desig. This appeared to work: people formed an emotional
   connection to the robot and enjoyed interacting with it.
   
   #+CAPTION: label:fig:kismet Kismet is a social robot that presents itself as an infant, to which people intuitively react by being more expressive in their voice and talking more slowly. This is precisely what Kismet's computer system needed to work well.
   #+ATTR_LATEX: :width 0.323\textwidth :float wrap :placement {R}[2cm]{0.5\textwidth}
   [[file:./img/kismet.jpg]]

   Two of the most popular social robots today are SoftBank's NAO and Pepper
   cite:softbank-robotics. These robots can understand and talk to people,
   recognize their emotion and are used in a broad range of places like
   introductory classes for STEM education and hotel lobbies. NAO is about 60
   centimeters high and can walk on his feet, while Pepper is 1.20 meters high
   and moves around using three wheels under its "skirt" (See autoref:fig:pepper).
   
   #+CAPTION: label:fig:pepper Softbank's social robot Pepper, one of the most popular advanced robots today.
   #+NAME: fig:pepper
   #+ATTR_LATEX: :width 0.323\textwidth :float wrap :placement {L}[2cm]{0.4\textwidth}
   [[file:./img/pepper.jpg]]

   Robots like NAO and Pepper try to be a part of our world by being present in
   public places around groups of people while still being very clear about
   their identity as a robot. There are also social robots that take this one
   step further where they actually try to appear indistinguishable from
   humans. So far, these robots are still in what is called the /uncanny
   valley/---a very high level of realism that is eerie because it is not yet
   /exactly/ human-like.

   It takes little effort to appreciate the complexity and amount of mechanisms
   at play when people communicate with each other. We can infer meaning and
   intention in a split second, quickly learn and reason inductively and adapt
   our communication style to our conversation partner. Now, imitating a few
   million years' worth of evolution is no small undertaking but the closer we
   get to communicating in a human-like way, the better we will be able to work
   with machines cite:adalgeirsson10_mebot,huang13_model_evaluat_narrat_gestur_human_robot.

** Why Gesture?

   Building machines which are modeled after human form and behavior is called
   /antropomorphic design/. This is important to support an intuitive and
   meaningful interaction with humans cite:breazeal04_desig and a key component
   of antropomorphism is animacy or aliveness
   cite:bartneck08_measur_instr_anthr_animac_likeab. People's perception of
   animacy is greatly influenced by the amount and type of motion they perceive
   in an object---as shown, for example, in Heider and Simmel's work
   cite:heider44_exper_study_appar_behav. Indeed, motion is a prerequisite for a
   perceived notion of aliveness.

   In situations with both virtual agents and humanoid robots it has been shown
   that speech-accompanying non-verbal behaviors have a positive effect on
   antropomorphism, likeability and future contact intentions---key objectives
   in the field of Human Robot Interaction (HRI)
   cite:bremner16_iconic_gestur_robot_avatar_recog,salem13_to_err_is_human,adalgeirsson10_mebot.
   Congruent gesture improves task performance
   cite:kramer16_nonverbal_mimicry,mamode13_cooper but even incongruent
   gesturing increases people's evaluation of a robot's human-like qualities
   cite:huang13_model_evaluat_narrat_gestur_human_robot.

   A speaker's gestures bear little structure nor are they produced or
   interpreted consciously, yet they still convey information between the
   collocutors. Gesturing is in fact beneficial to both the speaker and the
   listener: it helps the speaker think and helps the listener understand this
   thinking---even for people who are not trained in understanding these
   gestures cite:goldin-meadow99_role_gestur_commun_think,mcneill95_hand.

   This presents opportunities to significantly improve the quality of
   communication between humans and machines. First, human-like motion improves
   people's perception of the robot. Secon, gesturing can provide additional
   information that is not conveyed in speech and improve the quality of
   communication. Third, communicating on an intuitive level reduces the need
   for training people who need to work with these robots.

** Current State of Robot Gesture Synthesis label:sec-state-robot-synthesis

   To understand the state of current gesture synthesis technologies, one can
   look at both gesture synthesis in robot and in virtual agents. Translating
   the motion of such an agent to a live robot is challenging but possible
   cite:Salem2012.

   Three desirable properties for an effective gesture synthesis are proposed:

   *Continuity.* The avatar keeps moving. If a humanoid robot or avatar is
   motionless even for a small amount of time, people can think it is crashing
   and thus stop seeing the avatar as a being that is alive.

   *Variability.* The avatar should be able to perform gestures for any text
   given.

   *Congruence.* The gestures performed should have some relationship to the
   semantics of the text that is  being spoken. For example, extreme cases like
   nodding while the avatar says "no" should be avoided.

   In current research and industry, these are popular approaches for gesture
   synthesis:

   /The gestures are pre-recorded or otherwise pre-determined./ This could be by
   manually animating the robot for specific sentences or by annotating text
   files with the gestures which should be performed and when
   cite:neff08_gestur_model_animat_based_probab,Kipp2007,kopp04_synth_multim_utter_conver_agent.
   This can produce natural results but is very labor-intensive and not suited
   to the large amount of interactions a humanoid robot might have. This method
   succeeds at /Continuity/ and /Congruence/ but fails for the /Variability/
   requirement (with a significant cost for animation).

   /Gestures are generated randomly./ They might be chosen from a repertoire of
   movements and then stiched together or be completely random altogether.
   Often, this method introduces noticeable stuttering and might produce
   gestures that are inconsistent with the content of the spoken text, which
   is confusing to the person listening. An improvement for this method is
   adding fixed motions for specific keywords, which introduces the problems
   of pre-recording again. Random gestures allow /Variability/ but have
   difficulty with /Congruence/ and /Continuity/.

   /Gestures are generated from a set of rules cite:ng-thow-hing10_synch./ The
   gesture synthesis system analyzes the content of the text that will be
   pronounced and chooses a category of gesture for each text part. Then,
   category-specific rules are applied (such as matching for a keyword or parts
   of words) with some randomness to generate the final gestures. In principle,
   this system can allow all three desired properties but at a high cost for
   creating the gesture generation rules. To create this kind of system, it is
   necessary to perform social studies that examine how humans gesture and try
   to extract general rules.

   Neither of these solutions are ideal. In a truly social robot, the gesture
   synthesis system should be able to generate these gestures for arbitrary text
   (so that the robot can be reprogrammed) and still look natural---just like
   humans can say things they have never said before and still look alive.

   So how /do/ people gesture? What can we learn from research in psychology
   that could help us build a better system for gesture synthesis?

** How People Gesture

   In his classical work on human gesturing, McNeill argues that gesture and
   speech are created in concert; they are neither used as an addition to
   speech, nor a translation of it, nor are both modalities produced independently
   cite:mcneill95_hand.

   As mentioned previously speech-accompanying gesture is largely unstructured,
   but not completely. Some gestures /are/ interpreted consciously, for example
   when pointing at the location of an object that is being talked about.
   McNeill proposes four categories of gesture cite:cassell_1998,mcneill95_hand:

   - Iconic gestures :: literally depict an object or action that is being
        described. For example, spreading your arms while saying "big" or
        standing on your toes while explaining a ballet move.
   - Metaphoric gestures :: also represent something but not directly, for
        example, making a rolling motion with the hands while saying "the
        meeting went on and on."
   - Deictic gestures :: reference positions in space. For example, pointing at
        the bus stop when directing someone.
   - Beat gestures :: are not closely related to the content of the
                      communication but rather are used to emphasize words or to
                      clarify the structure of a sentence. For example, holding
                      the hands together left from the body in the first part of
                      a sentence, then moving both of them as the speaker
                      transitions to the second part of the sentence.

   Iconic and metaphoric gestures are perhaps the most straightforward of these
   from the point of view of gesture synthesis. In order to produce these, one
   could build a "gesture dictionary" that associates specific words or parts of
   sentences with gestures. To add more variability, some randomness could be
   added in the form of alternative gestures or noise. Note however that these
   types of gestures especially can vary across culture: a "V for victory" with
   the palm facing the gesturer is considered offensive in British culture
   cite:archer97.

   Deictic gestures reveal information that is not present in speech and
   generating these gestures would thus require semantic information along with
   the words that are being spoken. Once that information is present, though,
   generating deictic gestures is straightforward.

   Beat gestures make up the biggest part of all gestures (almost half, followed
   closely by iconic gestures) cite:mcneill95_hand but do not directly
   correspond to the content of the communication, making these difficult to
   generate procedurally. Subsequently, this type of gesture has not been
   focused on much in gesture synthesis research. Yet in order to build a robot
   that would move naturally it seems reasonable to start with the most-occuring
   type of gesture---perhaps this category alone is enough in order to make the
   robot seem alive.

   Imagining the ideal gesture synthesis system in a robot, it could then have
   the following structure: both speech and gesture are generated
   simultaneously, with access to information about the robot's intent and
   contextual information like, possibly, the positions of objects to be pointed
   at or cultural background. It could then use beat gestures as a baseline for
   its movement and combine those with iconic, metaphoric and deictic gestures
   using information from the context to make more precise gestures.

** The Goal: Gesture Synthesis with Deep Learning

    The goal of this project is to build a system that can synthesize gestures
    for any text, in a continuous fashion and related to the content of what is
    being communicated. An approach based on labor-intensive animation, text
    tagging or manual gesture analysis and research is not desired; it should be
    able to handle arbitrary input text and even synthesize motion for words it
    has never encountered.

    The nature of this problem is in some sense very similar to that of other
    problems where intuitive human abilities are to be imitated like speech
    synthesis, bipedal locomotion and image recognition. In all of these tasks,
    machine learning-based approached have proven to be very successful
    cite:hintin-need-ml so adopting a similar approach here seems promising.

    Especially the generation of beat gestures might benefit from a deep
    learning approach---a neural network could learn a general sense of how
    people move, which can be used as a starting point for other methods to add
    their more specific gestures (like deictic ones) to; or it might even be
    able to learn iconic and metaphoric gestures given enough of the right data
    is present.

    autoref:sec:literature-research covers a more detailed analysis of gesyure
    synthesis methods, explores recent advances in machine learning and provides
    background information on the technologies and methods used throughout the
    process. autoref:sec:method explains the process used in this project in
    detail, while autoref:sec:evaluation evaluates the results. Finally,
    autoref:sec:conclusion concludes this thesis and provides opportunities for
    future work.
    
** TODO Checklist
   - [ ] Context: Where does this fit in the state of the art?
   - [ ] Need: Why should it be done?
   - [ ] Task: What was done?
   - [ ] Object: What does the document cover?

* Literature Research label:sec:literature-research

** Gesture Synthesis in Robots
   :PROPERTIES:
   :Effort:   0:30
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-07 za 17:23]--[2018-07-07 za 18:06] =>  0:43
   :END:

   An overview of current gesture synthesis sytems was given in
   autoref:sec-state-robot-synthesis. This section will provide a deeper look at
   how two of these systems work in practice.

   Kismet's range of gestures is limited: it can only move its face actuators
   and move its head with the neck motors. Its movements are organized into
   /skills/, each of which is a finite state machine of positions where a
   transition is a certain /motion primitive/, a unit of gesture. These skills
   and the transitions between them are activated by the robot's other
   behavioral systems and external stimuli as specified in the finite state
   machine cite:breazeal04_desig.
   
   While the task of gesture generation can be applied to any robot, this thesis
   focuses on SoftBank's robots NAO and Pepper because they were easily
   accessible to work with. SoftBank provides developers a Python API and
   software package /Choregraphe/ cite:softbank_tools which includes a visual
   programming environment and robot simulator. This way, the results can be
   tested on a virtual robot quickly. Performing these gestures on a physical
   robot is as simple as changing the connection from the simulator to the real
   robot.

   SoftBank's robots all use the same software framework and API, /NAOqi/
   cite:softbank_naoqi. This framework includes a few modules that regulate
   their autonomous life cite:naoqi_autonomous_life:

   - ALAutonomousBlinking :: makes the robot blink its eyes (flash the LEDs
        around its eyes).
   - ALBackgroundMovement :: makes the robot make slight movements when it is
        idle and runs a breathing animation.
   - ALBasicAwareness :: makes the robot look at people's faces when it sees
        them, hears them or notices them when they touch it.
   - ALListeningMovement :: makes the robot move slightly when it is listening.
   - ALSpeakingMovement :: controls how the robot moves when it is talking.
        There are two modes for this module: /random/ launches random animations
        and /contextual/ launches specific animations for certain keywords and
        fills in the rest with random animations.

   Note that not all of these run simultaneously. For example, the
   /BackgroundMovement/'s breathing animation does not run when the
   /ListeningMovement/ or /SpeakingMovement/ is active.

   A developer has some control over these movements, like enabling and
   disabling them or changing the mode of speaking movement, but these systems
   are fairly limited in their expressive capability. In public appearances of
   SoftBank robots, their movements are often animated manually and thus do not
   use these autonomous capabilities.

*** DONE Systems in NAO(qi), Kismet
    CLOSED: [2018-06-21 do 08:25]
** Gesture Synthesis in Virtual Agents
   :PROPERTIES:
   :Effort:   0:15
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-10 di 09:51]--[2018-07-10 di 10:15] =>  0:24
   CLOCK: [2018-07-09 ma 17:35]--[2018-07-09 ma 18:11] =>  0:36
   :END:

   One of the most advanced gesture synthesis systems is the Articulated
   Communication Engine (ACE) cite:kopp04_synth_multim_utter_conver_agent. In
   this system, one annotates the text to be spoken with gestures and how they
   should be timed in an XML language called MURML (see
   autoref:fig:ace-murml-example). The ACE system combines the information from
   the text-to-speech engine with the gestures and information given in the
   speech/gesture definition, allowing it to create movements that are
   well-timed with the text being spoken (for example, stretching the arm while
   saying "there"). All the gestures that appear in a specification are combined
   so that the whole looks like a singular movement.
   
   The gestures produced by ACE are continuous and precise. However, they
   require extensive metadata accompanying the speech. When no behavior
   specification is defined, the avatar does not move. This makes the ACE system
   useful when a high level of precision is required, such as for deictic
   gestures, but less for free-form text.

   #+CAPTION: label:fig:ace-murml-example Example of MURML multi-modal specification (adapted from cite:salem10_gener)
   #+NAME: fig:ace-murml-example
   [[file:img/ace-murml-example.png]]

   The BodySpeech system was developed to remove the need to specify which
   gestures have to be chosen. It uses an audio clip of a recorded voice,
   analyzes its intensity in segmented parts of speech, chooses from a set of
   motion-captured gestures the one that most closely aligns with that part of
   speech and then blends between those movements cite:Fernandez:2013.
   
   Interesting reference points for realistic gesture synthesis can be found in
   3D animated movies or video games. Movies are mostly manually animated but
   provide a point of reference---not only in the sense that the people in these
   movies move in a way we recognize as being human, but also in how these
   movements are often purposely not precisely imitated from humans. Animators
   understand some principles of aliveness and manipulate or exaggerate gestures
   to convey emotional content.
   
   Modern video games present many in-game avatars that have to move in a
   realistic manner yet not all move in the same way. This means they face
   similar challenges and try to generate animation instead of extensive motion
   capture by actors. In practice, these avatars have a few keypoints animated
   manually and the animation for the rest of the avatar is generated using
   physics-based engines that take into account the biomechanics of humans
   cite:deepmotion_avatar, or a base animation is created manually or via motion
   capture and some variations are generated automatically
   cite:2013-SCA-diverse.

*** DONE ACE, video game engines
    CLOSED: [2018-06-21 do 08:25]
** Recent Advances in Machine Learning
   :PROPERTIES:
   :Effort:   3:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-10 Tue 18:53]--[2018-07-10 Tue 19:07] =>  0:14
   CLOCK: [2018-07-10 di 16:34]--[2018-07-10 Tue 18:24] =>  1:50
   CLOCK: [2018-07-10 di 15:05]--[2018-07-10 di 15:34] =>  0:29
   CLOCK: [2018-07-10 di 11:10]--[2018-07-10 di 12:00] =>  0:50
   CLOCK: [2018-07-10 di 10:26]--[2018-07-10 di 10:54] =>  0:28
   :END:

   Over the past ten years, tremendous progress has been made in the field of
   machine learning. With the invention of effective training algorithms such as
   the backpropagation algorithm and stochastic gradient descent, along with the
   exploitation of GPUs, we now have the capability to process more data orders
   of magnitude more quickly with algorithms that are more effective
   cite:nvidia-ai-computing.
   
   With these improvements in performance the possibilty arrived to train large
   neural networks, machine learning algorithms that automatically learn
   abstract representations of their data, became a possibility. This alleviated
   the need for manual feature engineering, which is time-consuming and requires
   extensive domain knowledge. Many complex problems such as object recognition,
   speech synthesis and machine translation are dominantly being tackled with
   deep neural networks cite:lecun15_deep_learn.

*** Recurrent Neural Networks

    Those last two problems require an extension of "plain" neural networks
    because they must produce a /sequence/ of features (e.g., sound samples or
    words) which can have a variable length.

    In order to be able to read or produce a sequence, the cells in the neural
    network need some kind of memory. The simplest way to do this is to give
    cells two inputs and outputs: the default input/output and a state vector.
    The cell then computes:

    1. The state, based on the input and the previous state
    2. The current output, based on the state

    Both these computations are, as in other neural networks, linear
    combinations with some learned weights. This computation is repeated for
    each element in the sequence with the same weights. A network composed of
    these kind of cells is called a \gls{rnn}, who can be represented as very
    deep neural networks where each time step (an iteration where the cell
    computation is performed) is a layer in this network and the weights are
    shared across layers. This way, recurrent neural networks are similar to
    deep neural networks.
    
    autoref:fig:rnn shows a high-level diagram of an \gls{rnn}, where two time
    steps are unrolled. In this figure and further diagrams, the convention will
    be adopted that 
    #+latex: \tikz[baseline]{\node[block,inline spacing,anchor=base]{grey rounded rectangles};} 
    represent operations and 
    #+latex: \tikz[baseline]{\node[variable block,inline spacing,anchor=base]{orange rectangles};} 
    represent variables.
    
    Simple \glspl{rnn} have difficulty learning long-term relationships because of a
    problem called the /vanishing gradient problem/ that also occurs in very
    deep neural networks. The LSTM (Long Short-Term Memory) cell solves this
    problem by splitting the state in two parts: one part is the output for
    every step and another part is a more long-term part that is only changed
    linearly with a filtered set of values from the first part cite:colah-lstm.
    The GRU (Gated Recurrent Unit) is a variation on the LSTM structure which
    appears to perform better on smaller datasets
    cite:chung14_empir_evaluat_gated_recur_neural.

    #+begin_src latex :exports results :results output
      \begin{figure}[h]
      \centering

      \begin{tikzpicture}
      \matrix [row sep=0.8cm, column sep=1.2cm] {
        \node (y_t-2) {}; &
        \node (y_t-1) {$y_{t-1}$}; &
        \node (y_t) {$y_{t}$}; &
        &
        \\
        \node (s_t-2) {$\cdots$}; &
        \node (rnn_t-1) [block] {RNN Cell}; &
        \node (rnn_t) [block] {RNN Cell}; &
        \node (rnn_next) {$\cdots$}; &
        \\
        &
        \node (x_t-1) {$x_{t-1}$}; &
        \node (x_t) {$x_{t}$}; &
        \node (x_t+1) {}; &
        \\
      };

      \path[->]
        (x_t-1) edge[thick] (rnn_t-1)
        (x_t) edge[thick] (rnn_t)

        (rnn_t-1) edge[thick] (y_t-1)
        (rnn_t) edge[thick] (y_t)

        (s_t-2) edge[thick] node[above] {$s_{t-2}$} (rnn_t-1)
        (rnn_t-1) edge[thick] node[above] {$s_{t-1}$} (rnn_t)
        (rnn_t) edge[thick] node[above] {$s_t$} (rnn_next)
        ;
      \end{tikzpicture}
      \caption{\label{fig:rnn}A time slice of a \glsfirst{rnn}.
        At each time step $t$, the network reads the current input $y_t$ and uses
        the state of the previous time step $s_{t-1}$ to compute the current output
        $y_t$ and current state $s_t$.}
      \end{figure}
    #+end_src

**** DONE Figure: Basic RNN
     CLOSED: [2018-07-10 Tue 17:57]
*** The Encoder-Decoder Architecture
    
    \Glspl{rnn} are good at predicting the next time step or steps in a sequence,
    making them ideal for tasks such as text autocompletion, but they can also
    be used for more complex tasks. 

    An encoder-decoder architecture consists of two recurrent neural networks.
    The first is used to read a source sequence and the state from the final
    time step is then interpreted as a representation of the entire
    sequence---often referred to as the /thought vector/. This thought vector
    then serves as input to a second \gls{rnn} that again outputs a sequence but of a
    different kind. This architecture is used in sequence-to-sequence problems
    where there is no one-to-one mapping between the steps in the source
    sequence and steps in the destination sequence. In machine translation, for
    example, the number of words in a sentence in different languages can
    differ, as well as the word order.
    
    Encoder-decoder architectures can also be used to solve problems which do
    not transform a sequence to another sequence. For example, in image caption
    generation, the encoder is a convolutional neural network that "interprets"
    and image, while the decoder network is a sequential network that generates
    a sequence of words describing the image.
    
    #+begin_src latex :exports results :results output
      \begin{figure}[h]
      \centering
      \usetikzlibrary{shapes,arrows,backgrounds,fit}

      \begin{tikzpicture}[->, shorten >=1pt, auto, node distance=2cm, semithick, font=\headingfont]

      \node (input) {};
      \node (encoder) [block, right=4cm of input] {Encoder};
      \draw [dash pattern=on 10pt off 5pt on 16pt off 5pt on 13pt off 5pt on 8pt off 5pt] (input) -- (encoder);

      \node (thought-vector) [variable block, above of=encoder, align=center] {Thought Vector};
      \draw (encoder) -> (thought-vector);

      \node (decoder) [block, above of=thought-vector] {Decoder};
      \draw (thought-vector) -> (decoder);
      \node (output) [right=4cm of decoder] {};
      \draw [dash pattern=on 2pt off 5pt on 7pt off 5pt on 3pt off 5pt on 4pt off 5pt on 1pt off 5pt on 1pt off 5pt] (decoder) -- (output);

      \end{tikzpicture}
      \caption{\label{fig:encoder-decover}A high-level overview of the encoder-decoder
        architecture that reads a variable-length input sequence and outputs another
        sequence. Often, both the encoder and the decoder networks are recurrent
        neural networks.}
      \end{figure}
    #+end_src
    
*** Text Embedding

    When the input and output of an encoder-decoder network are the same, this
    stucture is called an /autoencoder/. An autoencoder can be trained without
    supervision and learns to create an internal representation (the thought
    vector) which is of much smaller dimensionality than the original data. This
    encoder part can then be re-used as the first step in a supervised problem.
    An autoencoder can be used to compute a more efficient and meaningful
    representation of some input or to compute a fixed-length representation of
    a sequence, when the encoder and decoder networks are recurrent neural
    networks.

    One of the most use applications of this encoder-decoder architecture is in
    /text embedding/, which is the process of creating a vector representation
    for parts of text (characters, parts of words, words or sentences). The
    ~word2vec~ algorithm is a popular implementation that embeds words in a
    vector space, where the positional relationship in this vector space is
    related to the semantic relationship between words. For example, the
    operation $\mathrm{vec}(``king'') - \mathrm{vec}(``man'') +
    \mathrm{vec}(``woman'')$ resulted in the vector which was closest to the
    vector representation of the word "queen"
    cite:mikolov13_effic_estim_word_repres_vector_space.
    
    Since it is reasonable to assume that sentences with a similar semantic
    meaning would result in similar gestures, a text embedding could be used as
    a first step to process the input sentence before generating a gesture. The
    advantage of this step is that existing pre-trained models are available.
    This could increase the effectiveness of the gesture synthesis process,
    especially if only a small dataset can be collected as training data. 

    The TensorFlow team recently announced a new library /TensorFlow Hub/ that
    is now part of the TensorFlow ecosystem, which allows access to pre-trained
    models with a very simple API cite:introducing-tfhub. This library includes
    built-in access to a variety of text embedding modules, including the
    ~word2vec~ algorithm and the /universal sentence encoder/ which processes
    greater-than-word length text cite:tfhub-text.

** The Dataset label:sec:research-dataset
   :PROPERTIES:
   :Effort:   1:00
   :END:

   The dataset is a crucial component of any machine learning project. In this
   case, the model should be trained to predict gestures from a sentence as
   input. This means the dataset should contain these input-output pairs: text
   as input and gestures as output.

   Gestures will be represented as sequences of /poses/, which are single frames
   with the position of a person's joints. This is the format used in motion
   tracking systems and can easily be represented on a virtual avatar.
   
   #+caption: Example of a /pose/, a collection of joint positions. Specifically, this figure describes the format used in the OpenPose keypoint detection library cite:cao16_realt_multi_person_pose_estim.
   [[file:./img/openpose_keypoints.png]]

   A real robot however often requires a different type of input; the NAOqi API
   only provides the ability to directly specify the joint position of its
   wrists and torso cite:naoqi_cartesian_control. In order to manipulate the
   arms more precisely, the robot expects the joint angles instead
   cite:naoqi_joint_control. The simplest way to calculate these joint angles is
   to measure the angles between the joints in their positional representation.
   By using SoftBank's specification of these angles, they can be directly
   measured on a pose in order to move the robot to this position
   cite:naoqi_joint_control.
   
   Next to this difference in data format, it should be noted that these robots
   do not have the range of motion of humans, nor can they move their joints as
   quickly as people. This is a hard constraint on the extent to which human
   gesture can be imitated by a robot. The NAOqi API allows developers to
   specify the desired joint angles at every moment, which the robot will
   fulfill to its best ability.

   In order to avoid issues with the robot's balance, only the pose data of the
   upper body will be used to control the robot. SoftBank's Pepper robot has a
   hip joint which can be controlled without the risk of it toppling over
   (Pepper has a wheeled base), but NAO walks on its feet so controlling the
   legs to move its hips is risky.
   
   There are various datasets available of human motion, such as the Human3.6M
   and CMU Panopticon datasets
   cite:h36m_pami,Joo_2017_TPAMI,PoseletsICCV09,Shahroudy_2016_CVPR. However,
   these were created with the intent of training pose estimation or activity
   recognition techniques, resulting in datasets that are diverse in the kind of
   movements but have no or few samples of people who are talking. These
   datasets do not include subtitles for the text being spoken and lack audio
   tracks.
   
   As the dataset required for this project is not readily available, one will
   need to be created. Below, the elements for an approach to build a dataset
   from freely available videos and existing pose estimation projects are
   outlined.
   
*** Video Collection

    YouTube cite:youtube is one of the most popular websites and contains video
    footage from a wide variety of people in all kinds of environments and
    performing many activities. Videos with English spoken text are
    automatically transcribed which means that the subtitles for many videos are
    available. This means an adequate amount of video footage of people talking
    and gesturing would likely be available.

    To download YouTube videos with their subtitles, ~youtube-dl~ can be used
    which is a command line utility that can download video from a variety of
    sources including YouTube cite:youtube_dl.

    It is unlikely that entire videos will be usable so some pre-processing will
    need to be done on the downloaded videos. In particular, the parts of the
    videos that have suitable footage will need to be selected and the video
    will need to be split up in sentences with the corresponding footage.
    ~ffprobe~ is a command line utility that is part of the ~ffmpeg~ multimedia
    framework and can be used to detect scene changes (for example, when the
    footage cuts to another camera angle) cite:ffprobe. This can be used to aid
    the selection of footage, because pose estimation will be unstable across
    hard cuts and it is usually the case that a scene is either completely
    usable or completely unusable.

*** Pose Estimation

    Pose estimation is the task of processing an image or image sequence and
    extracting information about the pose of the person or people in that image
    (sequence). That is, the position of a person's joints (for example, the
    left knee, right wrist etc.) are estimated on the image.

    Some recent projects have had good results in estimating the (2D) positions
    of joints in images. The Stacked Hourglass
    cite:newell16_stack_hourg_networ_human_pose_estim, OpenPose
    cite:cao16_realt_multi_person_pose_estim and AlphaPose cite:fang16_rmpe
    networks have state-of-the-art results and have their source code freely
    available. All of these systems internally use convolutional neural networks
    to process their input images. The OpenPose and AlphaPose networks can
    detect multiple people in an image and do not have scaling or centering
    constraints, as opposed to the Stacked Hourglass algorithm.

    These networks estimate the two-dimensional position of joints in an image.
    To control a robot, however, the three-dimensional position of these poses
    (or the angles between them) is needed. There are two ways to approach
    three-dimensional pose estimation: either one first estimates the pose in
    2D, then "lifts" this into three dimensions, or one directly estimates the
    3D poses from an image.
    
    #+caption: label:fig:openpose_format Example of 2D pose detections by OpenPose cite:cao16_realt_multi_person_pose_estim.
    [[file:./img/openpose_demo.gif]]

    It is possible to estimate 3D poses straight from monocular images
    cite:mehta16_monoc_human_pose_estim_in,simo-serra13_joint_model_pose_estim_singl_image,
    however, the source code of these projects is not available. For the VNect
    project, an unofficial TensorFlow implementation is available
    cite:vnect_tensorflow but it did not produce results that were as good as
    using the official implementation of the other method: lifting poses from 2D
    to 3D.
    
    The "3d-pose-baseline" project is what the authors consider to be a baseline
    for 2D-to-3D lifting of poses
    cite:martinez17_simpl_yet_effec_basel_human_pose_estim; it is a simple
    neural network but appeared to work well on the initial testing data. The
    code is available on GitHub and written with TensorFlow so it could be
    adapted for use within the rest of this project.

** Time Series Clustering label:sec-research-clustering
   :PROPERTIES:
   :Effort:   0:30
   :END:

   Instead of directly predicting poses, the problem of gesture synthesis can be
   much simplified if we break down movement into a sequence of motion
   primitives. This way, a two-step process appears:

   1. Extract motion primitives from the pose data
   2. Predict motion primitives from parts of text

   To extract these motion primitives from the data, an unsupervised clustering
   algorithm could be used to find clusters of (subsequences of) gestures.
   Clustering, even with a large amount of features, is a well-understood
   problem and even one of the first techniques taught in most introductions to
   machine learning. /Time series/ clustering, however, introduces its own
   challenges cite:zolhavarieh14_review_subseq_time_series_clust. In order to be
   able to identify these motion primitives, the algorithm needs to be able to
   look at small parts of these sequences and find similar-looking subsequences
   in other pose animations. One can compare this to anomaly detection, albeit
   with more labels than thw two of "normal" and "exceptional".

   A first approach might be using a sliding window with some fixed time length
   and finding close matches across the dataset. However, sliding window
   approaches for clustering subsequences seem to be mostly meaningless
   cite:keoghil_clust.

   Previous work has been one on activity clustering of motion capture data
   cite:zhou13_hierar_align_clust_analy_tempor, though here the difference
   between different activities (e.g., walking versus sitting) is much more
   pronounced than different gestures and the authors noted that it did not
   perform well for smaller, more subtle movements.

   To perform clustering on /whole/ time series, there are multiple methods.
   Noting that clustering in its most general form comes down to grouping
   samples so that the samples within a group are close to each other while
   samples between groups are far away from each other, the key element of a
   clustering method is its distance metric
   cite:zolhavarieh14_review_subseq_time_series_clust. The classic Dynamic Time
   Warping (DTW) distance metric can be used to compare time series of different
   lengths and is implemented in the ~dtwclust~ R package cite:r-dtwclust
   which provides this and other clustering metrics. Additionally, this package
   includes a few methods to extract a medioid of these clusters, which can be
   used as the representation for a gesture to be played back on the robot.

** Conclusion

   This chapter provided an overview of the previous work that this thesis uses
   and builds upon. Previous work on gesture synthesis were examined in both
   robots and virtual agents; then a selected set of recent developments in
   machine learning was described, particularly in terms of language processing
   and time-series data processing; in autoref:sec:research-dataset the
   available data was examined and the tools to create a dataset for this
   problem were outlined; finally, in autoref:sec-research-clustering
   time-series clustering algorithms were explored that could simplify this
   problem by discretizing it into a set of motion primitives.

   With these starting points in place, an architecture begins to take shape
   that could complete the desired task. In the next section, these components
   will be combined in a pipeline that performs gesture synthesis.

* A Modern Approach to Gesture Synthesis label:sec:method
  SCHEDULED: <2018-07-02 ma>--<2018-07-15 zo>

  The goal of this project is to synthesize natural gestures on a robot in a
  continuous fashion, that carry some relation to what the robot is saying. This
  will be done by first building a dataset of clips ((subtitle, pose)-pairs)
  by applying pose estimation to YouTube videos, then clustering these poses and
  then building a machine learning model that learns to predict these clusters
  from a given sentence. Finally, the resulting poses will be performed on a
  live robot. An overview of this pipeline is shown in autoref:fig:pipeline.
  
  #+BEGIN_SRC latex :exports results :results output
    \begin{figure}
      \adjustbox{center}{\begin{tikzpicture}[center coordinate=(clustering)]
        \matrix[default matrix] {
          \node (video) [variable block] {Video}; &
          \node (video-picker) [block] {Video Picker}; &
          \node (detect-pose) [block] {2D Pose Estimation}; &
          \node (lift-pose) [block] {2D-to-3D Pose Lifting}; \\
          &
          \node (angles) [block] {Pose-to-Angle Conversion}; &
          \node (clustering) [block, anchor=south] at (0, 0.5\smallgridsize) {Clustering};
          \node (sequence-predictor) [block, anchor=north, align=center] at (0, -0.5\smallgridsize) {Gesture Prediction\\{\small Sequence Decoder}};
          &
          \node (classification-predictor) [block, align=center, anchor=south] at (0, 0.5\smallgridsize) {Gesture Prediction\\{\small Classification Decoder}}; &
          \node (gesture) [variable block] {Gesture}; \\
        };

        \path[->, above, outer sep=0.2\smallgridsize, every node/.append style={rounded rectangle, fill=white, fill opacity=0.6, text opacity=1, tight spacing}]
          (video) edge (video-picker)
          (video-picker) edge node {Images} (detect-pose)
          (detect-pose) edge node {2D Poses} (lift-pose)

          (angles) edge (clustering)
          (clustering) edge node {Gesture Classes} (classification-predictor)

          (angles) edge (sequence-predictor)
          (classification-predictor) edge (gesture)

          (sequence-predictor) edge (gesture)
          ;

        \draw[->]
          (lift-pose.east)
          .. controls ++(\smallgridsize, -2\smallgridsize)
             and ($(angles.west) + (-\smallgridsize, 3\smallgridsize)$) ..
          node [fill=white, tight spacing] {3D Poses}
          (angles.west);

        \draw node [tight spacing, right=6pt of angles.east] {Angles};
        \draw node (subtitle) [variable block, tight spacing, anchor=center, below=0.25\smallgridsize of clustering] {Subtitle};
        \draw[->] (subtitle) -- (sequence-predictor);
        \draw[->] (subtitle) -- (classification-predictor);

      \end{tikzpicture}}
      \caption{\label{fig:pipeline} Overview of this project's pipeline. It starts
        by processing a video to become part of the dataset, the result of which is
        then used to predict a gesture based on some given text.}
    \end{figure}
  #+END_SRC

** Development Environment
   :PROPERTIES:
   :Effort:   0:30
   :END:

   As this research is designed to be built upon, having easily accessible
   source code and data is important. All the code used in this project is made
   available on the GitHub repository https://github.com/iamarcel/thesis. The
   source for this report is also present.

   To maximize ease of use for the author and future users of this work, Pipenv
   cite:pipenv was used to manage the project's Python dependencies and is used
   for most of the code. Using OpenPose required some more system-level
   dependencies such as the NVIDIA CUDA and CuDNN libraries, so the environment
   for using OpenPose was created as a Docker container.

   Information on how to use the code is available in the source code
   repository.

** Collecting Pose Data

   Below, the process from finding video on YouTube to extracting the gesture
   data in a useful form is detailed. First, the Video Picker application that
   helps with data extraction is introduced. Then the process of extracting the
   3D poses is explained. Finally, these 3D poses will be converted to joint
   angles for further ease of use and complexity reduction.

   Throughout this section, the main unit of data will be called a "clip". A
   clip is the extracted data from the part of a video that corresponds with one
   line in its subtitles and consists of the following fields:

   - An identifier (for tracking its source, preventing duplicates and
     identifying extracted frame images, which are stored separately)
   - Start and end points in the original video
   - The subtitle
   - The 2D poses
   - The 3D poses
   - The joint angles
   - The class to which this movement belongs

   Throughout the following steps, all fields of the clip will be filled in.

*** The Video Picker label:sec:video-picker
    :PROPERTIES:
    :Effort:   1:00
    :END:

    While there is indeed a lot of video material available on YouTube, the
    requirements for the dataset are very specific:

    - The clip should be of a person talking
    - The person should talk English and subtitles should be available
    - The person should be visible in its entirety (as will be explained below,
      this is necessary for further steps in the pipeline)
    - The clip should be a single contiguous shot, i.e. the camera cannot move
      during the shot

    Whole videos that fulfill these needs are scarce but since the data has to
    be cut into clips, videos can be processed to extract only the parts that
    fulfill these requirements. The Video Picker application built assists in
    the process of finding good parts of a video and saving its data.

    When a video with suitable parts is found on YouTube and downloaded using
    ~youtube-dl~ cite:youtube_dl, it is first examined by the scene detection
    algorithm in ~ffprobe~ cite:ffprobe. Usually, a person is similarly framed
    throughout a single shot so Video Picker can run semi-automatically when a
    suitable shot is chosen and save all the clips in a single scene/shot.

    Then, the video is opened in Video Picker. The video picker is a GUI in
    which the user can scrub through the video or navigate by shot. When he has
    found a suitable shot, he can point at the person of interest with the
    mouse cursor and start recording this scene. Since the pose detection
    algorithm can detect multiple people in the scene (audience, for example)
    the user needs to point his cursor closest to the person he is interested
    in. This will be used later to filter out only the person of interest.

    The video picker then starts extracting the clips from the current shot.
    Every clip is saved in a JSON Lines format cite:jsonlines (where every line
    in the file is a JSON-formatted object; this is much faster than reading and
    parsing an entire JSON object at once) and the image frames are extracted
    and saved with ~ffmpeg~ cite:ffmpeg automatically by the application.

    The user can also explicitly pick a single clip or stop extracting when the
    shot has changed but the scene detection algorithm had not detected that
    change. This happens, for example, when there is a smooth transition between
    shots.

    Below the surface, the video picker is a Python application using the GTK+
    cite:gtk and GStreamer cite:gstreamer frameworks for building the GUI and
    playing back the video respectively.
    
    Due to the relatively strict conditions for usable videos (mainly the fact
    that the person speaking should be fully visible), most of the videos used
    in this project are of people who are presenting on a stage, e.g.,
    presenters of TED talks. This will necessarily result in a set of gestures
    that might not completely correspond to the gestures one performs in a
    dialogue with another person. On the other hand, this limited "gesture
    vocabulary" will make it easier to train a machine learning model and the
    resulting gestures will likely still look natural. It would likely be
    possible to train this pipeline on a different set of specialized gestures,
    such as gestures of someone telling fairy tales, which the model could then
    learn to imitate.
    
    #+caption: label:fig:video-picker Screenshot of the Video Picker application. This allows the user to select usable clips from videos and extracts their frames and subtitles for further processing.
    #+attr_latex: :width 1.2\textwidth,center
    [[file:./img/video-picker-screenshot.png]]

**** DONE Figure: screenshot
     CLOSED: [2018-07-20 vr 09:32]
*** Detecting 2D Poses with OpenPose
    :PROPERTIES:
    :Effort:   0:30
    :END:

    Once the video clips are collected, the next step is to perform 2D pose
    estimation on the extracted image frames and saving those results to the
    clips. The authors of OpenPose included a sample application that, once
    compiled, can read a directory of images and write the poses in each image
    to a JSON file. This "demo" program is run on the output directory of the
    video picker application and afterwards, the pose data from OpenPose is
    added to the database of clips. When adding the results from OpenPose to the
    clips, the location of interest that the user specified when extracting the
    clip in the video picker is used to select the desired person if multiple
    people were detected.

    The OpenPose output format is a list wherein the $x$ position, $y$ position
    and confidence score of each joint is specified, in the order of the pose
    model OpenPose used cite:openpose_output. This list is specified for each
    person detected in the image. If, in a clip, one or more joint positions
    have a very small confidence, that clip is not used further to avoid errant
    results later on in the process. autoref:fig:openpose_format shows the order
    of keypoints in this list.

    Note that there is no stability in the detected people, i.e., people can
    disappear or appear over time and the order in which they are specified can
    change throughout frames. This is why the user must specify the center of
    the target person while selecting video clips in the video picker.
    
    autoref:fig:sanity_check_openpose shows an example result from this step of
    the pipeline. This "sanity check" was performed on a subset of the captured
    clips in order to verify whether the data was sent to and processed by
    OpenPose correctly.
    
    #+BEGIN_SRC latex :exports results
      \begin{figure}[htbp]
        \centering
        \adjustbox{max width=1.2\linewidth,center}{\input{./img/sanity-check-openpose.pgf}}
        \caption{\label{fig:sanity_check_openpose} "Sanity check" for OpenPose 2D detections, showing a source video frame and the extracted pose information.}
      \end{figure}
    #+END_SRC

**** DONE Figure: sanity check - example of OpenPose detection
     CLOSED: [2018-07-20 vr 10:35]

*** Lifting Poses to 3D
    :PROPERTIES:
    :Effort:   2:00
    :END:

    Now that the 2D gestures are extracted, the next step is to lift the poses
    into three-dimensional space. The /3D Pose Baseline/ project had its source
    code and trained model available online so this was used as a starting
    point. Some modifications were made in order to use it in this pipeline.

    The first modification is made because the pose data format that 3D Pose
    Baseline expects is different from the one OpenPose outputs: they use the
    Human3.6M and COCO format respectively. The Human3.6M pose model has its
    joints ordered differently, does not have the eye and ear joints but does
    define hip, top-of-head and spine (at chest height) joints.
    
    Another 2D pose estimation framework, the Stacked Hourglass project
    cite:newell16_stack_hourg_networ_human_pose_estim, uses the same skeleton
    structure as 3D Pose Baseline and also has its source code available (in Lua
    and Torch). When testing this out, however, the results where not nearly as
    good as those from OpenPose. The Stacked Hourglass network can only detect a
    single person and requires precise annotation of the person's center and
    size in an image, which would also make the data collection step more
    difficult.

    While the ideal solution---when using OpenPose's 2D results---for the
    incompatibility between pose formats would be to re-train the 3D Pose
    Baseline model using 2D data from OpenPose, that would require processing
    their entire training set with OpenPose and then training it, which would
    take too much time. Instead, a rough direct conversion was made. Before
    passing the 2D detections as input to 3D Pose Baseline, their points were
    reordered and the following points were added:
    
    - Hip :: Center of left hip and left hip
    - Head (top of head) :: Half the distance between the Neck/Nose and the
         Thorax joints above the Neck/Nose.
    - Spine (chest height) :: Half the distance between Thorax and Hip below
         the Thorax.

    The second modification is necessary to use this 3D Pose Baseline for making
    predictions. While the authors' code allowed running the training and
    validation steps, there was no code present to run the inference step, i.e.,
    predicting 3D poses for new 2D detections. Additionally, the 2D predictions
    were smoothed before feeding them into this model, since OpenPose processes
    videos frame-by-frame resulting in motion that was not always smooth.
    
    After these two changes the 3D Pose Baseline code is usable but does not yet
    produce results that were of adequate quality. Sometimes poses are
    completely incorrect or deformed throughout a clip. Three issues were found
    rooted in the data.

    First, a missing point in the 2D detection has large effects on the results
    in 3D. When only joints in the upper body are detected by OpenPose, the
    lifted 3D pose is useless. Second, since the data is captured from multiple
    people, their size and body shape differs. Finally, the people in the 3D
    space are oriented in different directions.
    
    These effects would result in the model learning useless features like the
    body shape or orientation of the people. Thus, before using the data in a
    machine learning model, it has to be cleaned and normalized first.
    
    #+BEGIN_SRC latex :exports results
      \begin{figure}[htbp]
        \centering
        \adjustbox{max width=1.2\linewidth,center}{\input{./img/sanity-check-3d.pgf}}
        \caption{\label{fig:sanity_check_3d} Sanity check for the 2D to 3D pose conversion.}
      \end{figure}
    #+END_SRC
    
    #+BEGIN_SRC latex :exports results
      \begin{figure}[htbp]
        \centering
        \adjustbox{max width=1.2\linewidth,center}{\input{./img/sanity-check-pipeline.pgf}}
        \caption{\label{fig:sanity_check_pipeline} Sanity check for the entire pipeline.
           the image frame used as source, the middle shows the results from the OpenPose 2D
           pose estimation and the right shows the results from lifting that 2D pose into 3D
           and performing a slight rotation. Above the figure is the corresponding subtitle for
           this clip.}
      \end{figure}
    #+END_SRC

**** DONE Figure: sanity check - 3D skeletons
     CLOSED: [2018-07-20 vr 14:00]

**** DONE Figure: sanity check - video > 2D detection > 3D skeleton (+ subtitle)
     CLOSED: [2018-07-20 vr 14:37]
     
**** TODO Figure: difference in skeleton structures

*** Cleaning and Normalizing the Data

    The 3D poses are processed in two steps:
    
    1. *Cleaning* throws away corrupt poses and attempts to correct small
       errors.
    2. *Normalizing* formats the poses so they are independent of body shape and
       orientation.

    *Cleaning.*
    Three classes of errors occurred in the results from 3D Pose Baseline:

    - Point error: in a single or a few frames, one or more joints were not
      detected in 2D and have erroneous positions in the 3D output.
    - Clip error: not enough points were detected in 2D, resulting in an
      unusable 3D skeleton.
    - Leaning: a person appears to be leaning forward while it should not.

    Each clip is processed on a frame-by-frame basis and the distance of each
    joint with that joint in the previous frame is examined. When this distance
    exceeds a threshold (here 30% of a person's height), the position of that
    joint is replaced with the position from the previous frame. When more than
    4 joints have to be corrected this way, the clip is considered low quality
    and not used anymore.

    Then, the leaning issue is corrected for by setting an allowed range for
    the angle that the spine makes with the upward axis. If this angle is
    exceeded, all the points of the upper body are rotated so that they lie
    within this range.

    *Normalizing.*
    At this point, a pose is represented by the Cartesian coordinates of each
    joint in three-dimensional space. Even when every skeleton is centered
    around the hip, the height is set to unity and all skeletons are oriented
    in the same direction (by rotating the body so that the hip is aligned with
    the perpendicular axis), there are still two problems: people's body type
    differs significantly and the space containing all possible poses (i.e.,
    the entire 3D Cartesian space for every joint) is too large.

    Since the end goal is to play back gestures on a NAO robot, the choice was
    made to convert the data format to one that is directly compatible with the
    NAOqi SDK that is used to control this robot. Controlling the pose of a NAO
    robot is done by setting the angles of its actuators, so these angles could
    be measured from the position representation of the 3D poses.
    autoref:fig:nao-angles shows the definition of these angles and
    autoref:tab:pose-to-angle shows the details of how they can be calculated from
    the Cartesian coordinates. The joints mentioned in autoref:tab:pose-to-angle
    are interpreted as vectors, which autoref:fig:pose-angles visualizes.

    Note that when axes are specified, this is in the reference coordinate
    system of the poses returned from 3D Pose Baseline, not the coordinate
    system used in the NAOqi software.

    #+NAME: tab:pose-to-angle
    #+CAPTION: label:tab:pose-to-angle Details of joint position to angle conversion
    | Angle name     | Method                                                         |
    |----------------+----------------------------------------------------------------|
    | HipRoll        | Angle around $-z$ axis, from chest (upwards) to $-y$ axis      |
    | HipPitch       | Angle around $x$ axis, from chest to $-y$ axis                 |
    | RShoulderPitch | Angle around $x$ axis, from right upper arm to chest $- \pi/2$ |
    | RShoulderRoll  | Angle of right upper arm with $yz$ plane $+ \pi/10$            |
    | RElbowRoll     | Angle between right upper arm and right elbow                  |
    | LShoulderPitch | Angle around $x$ axis, from left upper arm to chest $- \pi/2$  |
    | LShoulderRoll  | Angle of left upper arm with $yz$ plane $- \pi/10$             |
    | LElbowRoll     | Angle between left upper arm and left elbow (negative)         |
    | HeadPitch      | Angle around $x$ axis, from nose to head $- \pi/4$             |
    | HeadYaw        | Angle around $-y$ axis, from $-z$ axis to nose                 |

    Note that these are only the angles for the upper body. The other joints and
    angles are ignored because they are not used here to generate gestures.
    
    #+BEGIN_SRC latex
      \begin{figure}
        \begin{tabular}{ >{\centering\arraybackslash} m{70mm} >{\centering\arraybackslash} m{70mm} }
          \includegraphics[width=65mm]{./img/nao-angles-arm-l.png} & \includegraphics[width=65mm]{./img/nao-angles-arm-r.png} \\
          (a) Left arm angles & (b) Right arm angles \\[18pt]
          \includegraphics[width=65mm]{./img/nao-angles-head.png} & \includegraphics[width=40mm]{./img/nao-axes.png} \\
          (c) Head angles & (d) Reference frame
        \end{tabular}
        \caption{\label{fig:nao-angles} Angle definitions and reference frame for
          Cartesian coordinates of the NAO robot. The 3D pose data is converted from
          Cartesian coordinates into a representation based on these angles.}
      \end{figure}
    #+END_SRC
    
    #+BEGIN_SRC latex :exports results
      \begin{figure}[htbp]
        \centering
        \adjustbox{max width=1.2\linewidth,center}{\includegraphics{./img/pose-vectors.png}}
        \caption{\label{fig:pose-angles} Visualization of the vector interpretation of
          body joints. Joints on the back side are shaded for clarity. The reference
          coordinate frame for the 3D Pose Baseline poses is also shown (not to
          scale); the $x$, $y$ and $z$ axes are colored in red, green and blue
          respectively.}
      \end{figure}
    #+END_SRC

**** TODO Figure: leaning
     :PROPERTIES:
     :Effort:   0:30
     :END:
**** DONE Figure: NAO skeleton and angles
     CLOSED: [2018-07-20 vr 15:19]
     :PROPERTIES:
     :Effort:   0:30
     :END:
**** DONE Figure: Vectors used in pose, for directions
     CLOSED: [2018-07-20 vr 17:09]
     :PROPERTIES:
     :Effort:   1:00
     :END:
**** DONE Figure: Difference in axes
     CLOSED: [2018-07-20 vr 17:15]
     :PROPERTIES:
     :Effort:   0:15
     :END:
** Finding Motion Primitives
   
   Even if pose data is stored as a limited set of angles, the output space is
   continuous and quite large. This makes it difficult to train a machine
   learning model with only a small amount of data. Would it be possible to
   vastly reduce the model complexity by turning it into a classification
   problem? How would the results compare?

   Gesture synthesis can be interpreted as a classification problem if the space
   of possible movements is reduced to a sequence of predefined /motion
   primitives/. Instead of producing a continuous sequence of angles, the model
   could classify a sentence under a motion primitive and then concatenate these
   motion primitives into a coherent, continuous whole. This approach poses two
   more questions:

   - Can we make this look continuous? Continuity was one of the main
     objectives but given the discrete nature of a sequence of motion
     primitives, this appears not to be trivial.
   - Is it possible to extract a set of these motion primitives from our
     dataset? I.e., can we cluster our dataset into motion patterns?

   The first question might have a straightforward answer as the NAOqi software
   has a built-in animation module that can interpolate between points. It is,
   however, difficult to evaluate beforehand if this results in (qualitatively)
   natural motion. The second question needs deeper investigation and
   experimentation.
   
*** Time Series Clustering
    :LOGBOOK:
    CLOCK: [2018-07-21 za 08:48]--[2018-07-21 za 09:01] =>  0:13
    :END:

    These motion primitives can be extracted from the captured gestures by
    performing unsupervised clustering on the dataset. The range of algorithms
    available is determined by the properties of this dataset:

    - It is a collection in which samples are time series
    - The samples have varying lengths
    - The samples are multi-dimensional (one dimension for each joint)
    - The desired clusters are subsequences of these samples

    Suitable algorithms to perform unsupervised, multi-dimensional clustering on
    subsets across multiple samples, with an implementation readily available,
    were not found by the author so the implementation in this project clusters
    across /whole/ samples instead of subsequences.

    As mentioned in autoref:sec-research-clustering, the ~dtwclust~ R package
    allows experimenting with different distance metrics. Those that support
    sequences of varying lengths are described below.

    *Dynamic Time Warping (DTW) distance.* To calculate the DTW distance between
    two sequences $a_i, i \in \{1,\ldots,n\}$ and $b_j, j \in \{1,\ldots,m\}$,
    the following steps are taken:

    1. Calculate the pairwise Euclidian distance between every pair of points
       $(a_i, b_j)$ and store it in a matrix $M_{i,j} = d(a_i, b_j)$, where $d:
       \mathbb{R}^k \times \mathbb{R}^k \rightarrow \mathbb{R}$ is the
       \(k\)-dimensional Euclidean distance function.
    2. Find the shortest path from $M_{0,0}$ to $M_{n,m}$, where the total weight
       of the path is the sum of the elements on this path. Every step in this
       path can only increase one of or both of the matrix' indices by one.

    This shortest path, in terms of \((i, j)\)-pairs, is called the /alignment/
    and the sum of the elements of this path is the DTW distance
    cite:sarda2017comparing.
    
    *Truangular Global Alignment Kernel (GAK) distance.* GAK methods interpret
    the distance measurement in a kernel space, similar to the process often
    used in Support Vector Machines. With a GAK, it is relatively simple to add
    a penalty to certain paths. In particular, the Triangular GAK with parameter
    $T$ weights elements of the alignment by their distance to the matrix
    diagonal and discards elements further than $T$ from the diagonal. This
    greatly reduces the computation complexity---with some loss of precision, of
    course. Still, the triangular GAK seems to perform well
    cite:Cuturi:2011:FGA:3104482.3104599,sarda2017comparing.
    
    The second element of a clustering algorithm is the method of defining a
    /prototype/ or centroid of a cluster. In this case, the Partition Around
    Medioids (PAM) method is used, which always uses an element of the data as
    centroid.
    
**** DONE Explain metrics in dtwclust
     CLOSED: [2018-07-20 vr 19:32]
     :PROPERTIES:
     :Effort:   0:30
     :END:

**** TODO Figure: Euclidian distance vs. DTW distance
     :PROPERTIES:
     :Effort:   0:30
     :END:

    The standard Euclidian distance compares distance on a point-by-point basis.
    For time series, however, this metric falls short because it cannot account
    for variations in the /length/ of recurring patterns that should be
    discovered. The Dynamic Time Warping (DTW) metric solves this issue by
    skipping or repeating points in time so that the distance between two time
    series is minimized.

*** Results
    :PROPERTIES:
    :Effort:   2:00
    :END:
    :LOGBOOK:
    CLOCK: [2018-07-23 ma 14:43]--[2018-07-23 ma 14:54] =>  0:11
    CLOCK: [2018-07-23 ma 07:56]--[2018-07-23 ma 11:54] =>  3:58
    CLOCK: [2018-07-21 za 09:01]--[2018-07-21 za 10:26] =>  1:25
    :END:

    The results described here were obtained using the GAK distance metric, PAM
    centroid method and a partition in eight clusters.
    autoref:fig:clustering-results-histogram shows the distribution of clusters
    across the dataset. Two of the clusters are very small, two are very big and
    the other four have a size roughly 1/8th of the dataset.
    
    #+call: pgf_figure(var_name="clustering-results-histogram", var_caption="Distribution of gesture clusters across the dataset.")
    
    #+call: pgf_figure(var_name="cluster-centers", var_caption="A single frame from each of the clustered gestures' centroids.")
    
    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}[!tbp]
        \adjustbox{max width=0.95\paperwidth,center}{%
        \begin{tabular}{m{35mm} m{35mm} m{35mm} m{35mm} m{35mm} m{35mm} }

          \multicolumn{2}{p{70mm}}{\input{./img/cluster-1-samples.pgf}} &
          \multicolumn{2}{p{70mm}}{\input{./img/cluster-2-samples.pgf}} &
          \multicolumn{2}{p{70mm}}{\input{./img/cluster-3-samples.pgf}} \\
          \multicolumn{2}{c}{(a) Cluster 1} &
          \multicolumn{2}{c}{(b) Cluster 2} &
          \multicolumn{2}{c}{(c) Cluster 3} \\

          \multicolumn{2}{p{70mm}}{\input{./img/cluster-4-samples.pgf}} &
          \multicolumn{2}{p{70mm}}{\input{./img/cluster-5-samples.pgf}} &
          \multicolumn{2}{p{70mm}}{\input{./img/cluster-6-samples.pgf}} \\
          \multicolumn{2}{c}{(d) Cluster 4} &
          \multicolumn{2}{c}{(e) Cluster 5} &
          \multicolumn{2}{c}{(f) Cluster 6} \\

          \multicolumn{3}{p{105mm}}{\adjustbox{center}{\input{./img/cluster-7-samples.pgf}}} &
          \multicolumn{3}{p{105mm}}{\adjustbox{center}{\input{./img/cluster-8-samples.pgf}}} \\
          \multicolumn{3}{c}{(g) Cluster 7} &
          \multicolumn{3}{c}{(h) Cluster 8}

        \end{tabular}}
        \caption{\label{fig:cluster-samples} Frames from four samples for each cluster.}
      \end{figure}

    #+END_SRC
    
    autoref:fig:cluster-centers shows a frame for the centroid of each of the
    clusters and autoref:fig:cluster-samples shows, for each of the eight
    clusters, a frame from four random samples in that cluster. While it is
    difficult to evaluate based on single frames, looking at the animated
    version of autoref:fig:cluster-samples reveals that thhe results from
    clustering are good in some cases and not that good in others. For example,
    one of the poses in cluster 3 (where the person's right upper arm is
    extended to the right and their lower arm is pointing downwards) would make
    more sense if it would belong to cluster 5, where two instances of similar
    gestures are present. The samples within cluster 4 and cluster 6 are all
    very similar but they could perhaps even be combined into a single cluster.
    
    The author suspects that much better results can be achieved with a larger
    dataset or if an algorithm could be implemented that can extract clusters
    from subsequences of the gestures, which would be real motion primitives.
    For now, these results will be used for the rest of the project since
    building the entire flow that collects and processes data and generates
    gestures from that is deemed more important. As explained in
    autoref:sec:video-picker, collecting more data is time-consuming but
    straightforward.

**** DONE Figure: examples of extracted clusters
     CLOSED: [2018-07-23 ma 14:51]
** Predicting Poses label:sec:pose-prediction
   
   Now the dataset is annotated with gesture classes, it is possible to build a
   machine learning model that can predict these. This section explains the
   network structure for a model that predicts classes based on an input
   sentence and for the alternative approach, which predicts gestures directly
   instead.
   
   In all cases, the network can be seen as having an encoder-decoder
   architecture. This section will explain two different encoders and two
   decoders.

   The encoder is responsible for reading the input text and interpreting that
   sequence into a thought vector. In one case, the encoder uses an \gls{rnn} and a
   vocabulary based on the input data while in the other case, the encoder uses
   a pretrained sequence encoder.

   The decoder then generates the desired output based on the results from the
   encoder. In one case, the decoder will return a set of probabilities for
   output classes while in the other case, the decoder will return a sequence of
   poses, i.e., a gesture.

*** DONE Figure: graph of the model(s)
    CLOSED: [2018-07-27 vr 08:36]
    :PROPERTIES:
    :Effort:   0:30
    :END:
    :LOGBOOK:
    CLOCK: [2018-07-22 zo 10:50]--[2018-07-22 zo 11:21] =>  0:31
    :END:
    
*** The Encoder Network
    :PROPERTIES:
    :Effort:   1:00
    :END:
    :LOGBOOK:
    CLOCK: [2018-07-22 zo 11:42]--[2018-07-22 zo 12:54] =>  1:12
    :END:

    The encoder network interprets the input sentence and returns an internal
    representation to be used by the decoder network. Note that the input of
    this network is a string, a variable-length sequence of characters, so the
    encoder must be able to able to process sequential data. If a plain neural
    network would be used (where sequences are padded up until the maximum
    length of an input sequence in the dataset), it would suffer from the
    following problems:

    - The size of the network and subsequently its complexity is dependent upon
      the maximum length. This means, for example, that the network needs to be
      rebuilt if longer sequences are desired.
    - Exactly the same amount of processing needs to happen whether the input is
      short or long.
    - The network cannot learn to exploit structure in parts of the input, e.g.,
      the words "this big" appearing at the beginning or at the end result in
      completely different computations while they should probably result in the
      same gesture, just at a different point in time.

    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}
        \centering
        \begin{tikzpicture}[semithick, font=\headingfont]
          \matrix [row sep=0.8cm, column sep=1.2cm] {
            \node (thought) [variable block, align=center] {Output\\{\small(Thought Vector)}}; \\
            \node (rnn) [block] {\Gls{rnn} Cell}; \\
            \node (embedder) [block] {Word Embedding Layer}; \\
            \node (lookup) [block] {Lookup}; \\
            \node (splitter) [block] {Splitter}; \\
            \node (sentence) [variable block, align=center] {Input\\{\small(Sentence)}}; \\
          };

          \path [->, thick]
            (sentence) edge (splitter)
            (splitter) edge node[right, xshift=12pt] {Normalized words} (lookup)
            (lookup) edge node[right, xshift=12pt] {Word Indices} (embedder)
            (embedder) edge node[right, xshift=12pt] {Word Embeddings} (rnn)
            (rnn) edge (thought)
            (rnn) edge [loop right] ()
          ;

          \foreach \x in {-10pt, 10pt} {
            \path [->, thick]
              (splitter.north) edge ([xshift=\x] lookup.south)
              ([xshift=\x] lookup.north)   edge ([xshift=\x] embedder.south)
              ([xshift=\x] embedder.north) edge ([xshift=\x] rnn.south)
              ;
          }

        \end{tikzpicture}
        \caption{\label{fig:text-encoder} The encoder network that processes an input
          sentence into a thought vector. Multiple arrows are shown where sequential
          data flows.}
      \end{figure}
    #+END_SRC

    #+BEGIN_EXPORT latex
    \paragraph{{\Gls{rnn}}-based encoder} The \gls{rnn}-based encoder processes the input
    text in four steps:
    #+END_EXPORT

    1. Split the sentence into text objects.
    2. Use a vocabulary file to encode them with one-hot encoding [fn:one-hot].
    3. Create an /embedding/ of these text objects, representing them as a
       (dense) vector in a lower-dimensional feature space.
    4. Read this sequence of embeddings with an \gls{rnn} and return the thought
       vector.

    The first design choice in the encoder is the definition of a /text object/.
    This is the unit of text to which the sentence will be reduced. Common
    examples are words and n-grams ($n$ characters). Larger text objects need
    higher-dimensionality neural network layers since each unique instance needs
    its own representation, but they can more readily be used to model time
    dependencies. Smaller text objects like single characters only need a small
    vocabulary (in this example, each letter of the alphabet and perhaps some
    punctuation characters) but a more complex network structure is needed to
    capture time dependencies. In this case, words were chosen as text object
    because of their interpretability and popularity.

    # there are bigger things in there. yesyes. you see. when its bigger it
    # needs more space hé. So yes. The first design choice from above is clearly
    # very good. (ahja, dazietiedereen). dus okay then. if we take a closer look
    # we'll see some interesting things there. euh ja. uhu.

[fn:one-hot] A one-hot encoding represents a word as an \(n\)-dimensional
    vector, where $n$ is the number of words in the vocabulary. A word is
    represented by the vector that is 1 at the index of that word in the
    vocabulary and 0 everywhere else.


    When splitting the sentence, the text is pre-processed by removing
    capitalization and special characters. The same transformation is used to
    create the vocabulary file, which contains the 512 words that appear most in
    the dataset.

    The embedding step is a simple linear layer that is trained along with the
    rest of the network. It can be expected that, after training, the embedded
    representations of words with similar gestures would be close to each other.
    
    The final step is a single-layer \gls{rnn} with Gated Recurrent Unit (GRU) cells,
    which perform better than plain \gls{rnn} cells on longer sequences and are less
    complex to train cite:chung14_empir_evaluat_gated_recur_neural. 
    
    \paragraph{Pretrained encoder} The pretrained encoder uses Google's
    Universal Sentence Encoder cite:cer18_univer_senten_encod available in
    pretrained form with TensorFlow Hub cite:tfhub. This module reads the entire
    sentence at once and returns a 512-dimensional vector.

    # A /recurrent/ neural network, however, is able to handle this kind of data.
    # This kind of network introduces a "time" dimension that can vary across
    # inputs.

    # In practice, though, there is a caveat. Training data is not processed
    # sample-per-sample, rather minibatches of samples are created and all elements
    # in the minibatch pass through the network at once. This improves training
    # speed cite:TODO but re-introduces some of the problems mentioned above. While
    # inputs of different length do not have an impact on the /result/ of the
    # network, batching inputs of different lengths can slow down the training
    # process.
    
    # Recurrent neural networks have difficulty with long-term dependencies (LSTM
    # networks fare somewhat better) because of the large amount of operations in
    # between the input data and the corresponding output: the data from one time
    # step is passed through all the data from later time steps and combined into
    # the intermediate state, which needs to be "unrolled" again.
    
    # An attentional network is one in which weights act as "keys" to select
    # specific time steps of the input, implemented as a weight vector that is
    # multiplied element-wise with the inputs. This weight vector is a variable
    # that is optimized.
    
*** The Decoder Network label:sec:decoder
    
    Now that hidden representation of the subtitle has been created, this can be
    decoded to predict a class or sequence of poses.

    \paragraph{Classification decoder} The simplest decoder returns a
    probability for each of the classes by adding the following layers to the
    hidden representation:

    1. A dropout layer for regularization
    2. An intermediate fully-connected layer with ReLU activation
    3. A fully-connected layer with ReLU activation, representing the classes'
       logits

    The loss function is the softmax cross entropy with the labels in a one-hot
    encoding.
    
    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}
        \centering
        \begin{tikzpicture}[semithick, font=\headingfont]
          \matrix [row sep=0.8cm, column sep=1.2cm] {
            \node (logits) [variable block, align=center] {Output\\{\small(Class Probabilities)}}; \\
            \node (dense2) [block] {Dense}; \\
            \node (dense1) [block] {Dense}; \\
            \node (dropout) [block] {Dropout}; \\
            \node (thought) [variable block, align=center] {Input\\{\small(Thought Vector)}}; \\
          };

          \path [->, thick]
            (thought) edge (dropout)
            (dropout) edge (dense1)
            (dense1) edge (dense2)
            (dense2) edge (logits)
          ;

        \end{tikzpicture}
        \caption{\label{fig:class-decoder} The class decoder decodes a thought vector
          into a probability for each class.}
      \end{figure}
    #+END_SRC
    
    
    \paragraph{Sequence decoder} 
    
    The other decoder directly generates gestures, sequences of poses. The
    challenges when implementing this decoder are

    - Generating sequential data which is of a different data type than the
      input sequence (gestures are sequences of vectors in a continuous space,
      while words are discrete).
    - Learning the length of the sequence to be generated.
    - Learning dependencies between the input sequence and output sequence, when
      their length scales are different.

    Previous work on models that input and output sequences, like the ~seq2seq~
    framework for TensorFlow built by Google engineers as a general-purpose
    encoder-decoder framework cite:Britz:2017 and the Seq2seq Library built-in
    to TensorFlow cite:tf-seq2seq-library are built to generate sequences of
    symbols (such as word objects) instead of sequences of continuous values.
    Thus, while they can easily be used for various text generation tasks like
    machine translation and image captioning, they cannot generate continuous
    values like poses.
    
    In that sense, gesture synthesis is more akin to speech synthesis than
    machine translation. One advantage of gesture synthesis over speech
    synthesis is the much smaller time resolution: Google's WaveNet
    text-to-speech synthesis network, for example, generates audio at 24,000
    frames per second cite:oord17_paral_waven. As opposed to sound, motion
    appears smooth to human perception at 25 frames per second and this might
    even be reduced if a good interpolation of poses can be found. Note though
    that poses have higher dimensionality than sound: a sound sample is a single
    value while a pose is, in this case, a value per joint. Still, the time
    dimensionality for gesture synthesis is two orders of magnitude less than
    for raw audio synthesis.
    
    With that in mind, neither the text synthesis or speech synthesis systems
    available can be easily adapted for this task but a specialized decoder
    shall be built. autoref:fig:sequence-decoder shows a diagram of this
    decoder. The data that flows through an \gls{rnn} is divided into two parts:
    the input/output and the state. Both of these are updated in every time step
    of processing but the state is internal. Intuitively, one can interpret the
    state as the context while the input represents the progress of decoding.

    In the sequence decoder, the initial state is given by the thought vector as
    created by the encoder. The input is the pose from the previous time step
    but in practice, this differs depending on the network structure. During the
    training step, the ground truth pose is passed as input and during
    inference, the generated pose from the previous time step is fed back into
    the cell.
    
    Similar to previous research that animated 3D face meshes based on audio
    input cite:karras17_audio_driven_facial_animat_by, the loss function used is
    a sum of two terms: the /position loss/ and the /motion loss/. The position
    loss is the squared error between the predicted pose and the ground truth
    pose, while the motion loss measures the squared error of the difference
    between consecutive frames. This way, the network is explicitly forced to
    learn the correct speed of motion as well as the position of the joints.
    These terms, defined in terms of the network input $x$ are, respectively:

    \begin{align*}
      P: x \mapsto &\sum_{t=0}^{T(x)-1}\sum_{i=0}^{n-1} \Big[ y_i^{(t)}(x) - \hat{y}_i^{(t)}(x) \Big]^2 \\
      M: x \mapsto &\sum_{t=0}^{T(x)-1}\sum_{i=0}^{n-1} \Big[ \big(y_i^{(t)}(x) - y_i^{(t-1)}(x)\big) - 
                   \big(\hat{y}_i^{(t)}(x) - \hat{y}_i^{(t-1)}(x)\big) \Big]^2.
    \end{align*}
    
    Here, we defined $y$ and $\hat{y}$ as the functions that map the input to
    the ground truth output and the network's prediction respectively. The
    output of both of these functions is a temporal sequence of /frames/
    $y^{(t)}, t \in \{0,\ldots,T(x)-1\}$, where $T$ is the length of the ground
    truth sequence and thus dependent on $x$, and where each frame is a vector
    of $n$ frames $y^{(t)}_i, i \in \{0,\ldots,n-1\}$.

    Since the length of the time sequence in the output cannot be directly
    determined from the input, the network needs to learn this as well. To
    achieve this, an extra dimension is added to the gesture data containing
    the percentage of time left until the end of the sequence. For example, when
    a gesture is 50 frames long, the value of the vector in the 11th timestep
    for this dimension is $1 - (1/50) \cdot 10 = 0.8$. The same loss function
    can be used to learn this dimension.
    
    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}
        \centering
        \begin{tikzpicture}[->, thick, x=\smallgridsize, y=\smallgridsize, font=\headingfont, center coordinate=(cell)]
          \node (cell) [block] {\Gls{rnn} Cell};

          \draw
            (cell.160)++(-2, 0) node(state) [variable block, left, align=center, yshift=0.25\smallgridsize] {Initial State\\{\small(Thought Vector)}}
            -- node(statejoincontrol)[pos=0.3]{}
               node(statejoin)[pos=1.0]{}
               (cell.160);

          \draw[->]
            (cell.200)++(-2, 0) node(input) [left] {Input}
            -- node(inputjoincontrol)[pos=0.3]{}
               node(inputjoin)[pos=1.0]{}
               (cell.200);

          \draw[->] 
            (cell.340)
            -- node(outputjoin)[pos=0]{}
               ++(2, 0) node(output)   [right] {Output};

          \draw[draw-blue]
            (input.east)++(0, -1)
            node (groundtruth) [left, align=right] {Ground Truth\\{\small(Poses)}} 
            .. controls +(1, 0) and (inputjoincontrol) .. (inputjoin.center);

          \draw[->,looseness=3] (cell.20) to[out=0, in=0] ([shift=(up:1)] cell.north) node [above] {State} to[out=180, in=180] (statejoin.center);
          \draw[looseness=3, draw-green] (outputjoin.center) to[out=0, in=0] ([shift=(down:1)] cell.south) to[out=180, in=180] (inputjoin.center);

        \end{tikzpicture}
        \caption{\label{fig:sequence-decoder} The sequence decoder decodes a thought
          vector into a sequence of poses. Green lines are connections present during
          inference, while blue lines are present during training.}
      \end{figure}
    #+END_SRC

    One property of this encoder-decoder structure that is at least intuitively
    restrictive is that the entire input sequence must be summarized as a single
    vector and then expanded again by the decoder---especially considering that
    people probably do not choose their gestures based on the summarized
    "meaning" of the sentence they are saying, but rather as a combination of
    gestures based on the words in that sentence.
    
    In response to this observation in the fields of image caption generation
    and machine translation, for example, /attention-based models/ were proposed
    as a solution
    cite:wu16_googl_neural_machin_trans_system,xu15_show_atten_tell,bahdanau14_neural_machin_trans_by_joint.
    These structures are most often used in combination with \glspl{rnn} and
    have more recently even been used on their own
    cite:vaswani17_atten_is_all_you_need. An attention model creates, for each
    time step in the decoder, a /context vector/ that is a weighted combination
    of the intermediate outputs from the \gls{rnn} in the encoder step (referred
    to as /memory/). These weights are trainable and based on the previous state
    from the \gls{rnn}. Then, the result is assigned to the new state of the
    recurrent network. autoref:fix:attention-decoder shows how this works
    conceptually.
    
    #+BEGIN_SRC latex :exports results :results output
      \begin{figure}
        \centering
        \begin{tikzpicture}[->, thick, x=\smallgridsize, y=\smallgridsize, node distance=\smallgridsize, font=\headingfont, center coordinate=(cell)]

          \node (cell) [block] {\Gls{rnn} Cell};
          \node (attention) [block, above=\smallgridsize of cell] {Attention Mechanism};
          \node (memory) [variable block, above=\smallgridsize of attention, align=center] {Memory\\{\small(Outputs from encoder)}};

          % Input Line
          \draw[->]
            (cell.200)++(-2, 0) node(input) [left] {Input}
            -- node(inputjoincontrol)[pos=0.3]{}
               node(inputjoin)[pos=1.0]{}
               (cell.200);

          % Output Line
          \draw[->] 
            (cell.340)
            -- node(outputjoin)[pos=0]{}
               ++(2, 0) node(output)   [right] {Output};

          % Right State Line
          \draw[->,looseness=3] (cell.20)
               to[out=0, in=0]
               node [right] {State} (attention.east);

          % Left State Line
          \draw[->,looseness=3] (attention.west)
               to[out=180, in=180]
               node [left] {State} (cell.160);

          % Input Loop
          \draw[looseness=3] (outputjoin.center) to[out=0, in=0] ([shift=(down:1)] cell.south) to[out=180, in=180] (inputjoin.center);

          % Memory Lines
          \draw[->] ([xshift=-0.75\smallgridsize] memory.south) -- ([xshift=-0.75\smallgridsize] attention.north);
          \draw[->] ([xshift=-0.5\smallgridsize] memory.south) -- ([xshift=-0.5\smallgridsize] attention.north);
          \draw[->] ([xshift=0.75\smallgridsize] memory.south) -- ([xshift=0.75\smallgridsize] attention.north);
          \draw[->] ([xshift=0.5\smallgridsize] memory.south) -- ([xshift=0.5\smallgridsize] attention.north);

          \node at ($(memory)!0.5!(attention)$) {$\cdots$};

        \end{tikzpicture}
        \caption{\label{fig:attention-decoder} The attention-based decoder calculates
          its new state by taking a weighted combination of all the outputs from the
          encoder. Details about training/inference differences and initial states
          omitted for clarity.}
      \end{figure}
    #+END_SRC

** Playing Back on Robot
   :PROPERTIES:
   :Effort:   2:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-27 vr 08:39]--[2018-07-27 vr 09:18] =>  0:39
   :END:
   
   When researching the dataset and data formats in
   autoref:sec:research-dataset, it was decided that the representation of poses
   throughout the project would be based on the joint angles of the NAO robot,
   so they can directly be used on this robot.
   
   During most of the project, the results were tested on a virtual robot.
   SoftBank provides the /Choregraphe/ application cite:softbank_tools which
   provides a visual programming interface and hosts a simulated robot with a 3D
   view. 

   Softbank's robots can be controlled over the local network, either wired or
   over Wi-Fi. They provide the Python NAOqi API that adds an abstraction layer
   to make controlling the robot straightforward. The simulation in Choregraphe
   creates a virtual robot on the local machine and behaves mostly similar to
   the real robot, except that:

   - There is no network involved outside the development machine.
   - The simulation does not include gravity or other environmental physics. The
     robot will thus never fall over and its torso is always displayed upright.
   - Some of NAO's capabilities cannot be simulated.
   
   Of these capabilities not supported in the simulation, though, two are
   critical for this project: the gesture synthesis and the text-to-speech
   engines. This means that to compare NAOqi's existing gesture synthesis system
   with the ones created here a physical robot is needed.
   
   Controlling the robot's pose is done by setting the value of each joint's
   angle through this API. Though these angles might contain positions that are
   not reachable by the robot or move too quickly, the NAO will move in a
   best-effort way. The commands to specify position are asynchronous so they
   can be sent frame-by-frame while the robot will try to keep up as best as he
   can. Thus, the robot's pose is sent in real time, frame-by-frame.
   
   In a production environment, this would not be a good solution. Spotty
   network connections and a dependency on a separate computer for gesture
   synthesis will make it diffifult in real life situations. Eventually, the
   best solution would be that the gesture synthesis system is part of the robot
   and has direct control over its pose. For this research, though, the method
   used here works well.

*** TODO Figure: screenshot of Choregraphe
   
*** TODO Figure: photo of NAO
* Evaluation label:sec:evaluation
  SCHEDULED: <2018-06-28 do>--<2018-06-29 vr>
  :LOGBOOK:
  CLOCK: [2018-07-27 vr 09:18]--[2018-07-27 vr 11:56] =>  2:38
  :END:
  
  While the metrics and many "sanity checks" used up until now give some
  confidence that the gesture generated look natural, that conclusion is clearly
  subjective and should be verified with a proper test. This section starts by
  covering the evaluation and optimization of the neural network discussed
  previously, after which it explains the setup and results for a Turing-like
  test that was performed to validate the results produced qualitatively by
  sending a survey out.

** Defining Good Results
   
   One of the biggest challenges in this project and machine learning in general
   is defining what a "good result" is. In this case it is especially
   ill-defined since human perception is involved and body language is by no
   means a formal language. The most "real" measure of success would be
   something like /"the majority of people agree that this robot gestures in a
   natural way"/---which is not a precise measure and is influenced by a large
   amount of factors we cannot control like culture differences, the physical
   shape of the robot and the text-to-speech engine it uses.
   
   Yet, without an accessible measure it is not possible to compare different
   results or have an idea about whether the outcome is actually something
   useful. Three different "levels" of metrics are used throughout the project:

   - Quick-and-dirty sanity checks :: The figures throughout autoref:sec:method
        provide a quick, qualitative pass-or-fail answer that make sure the
        results are at least somewhat meaningful. This was necessary because of
        the many steps in the pipeline.
   - Prediction loss functions :: The gesture synthesis models are trained and
        evaluated based on their loss functions as defined in
        autoref:sec:pose-prediction. These hard numbers drive the learning
        mechanisms but differ from the ideal perception-based metric. In
        autoref:sec:decoder-comparison, this difference is discussed some in
        more detail.
   - The survey :: By asking people's opinion, human perception is accounted for
                   and the final results can be evaluated. Unfortunately, this
                   is time-consuming for participants so the amount of
                   approaches that can be compared is very limited.

** Optimizing the Neural Network
  
   The gesture synthesis network has quite a few parameters, the first of which
   is the network structure itself. \Fref[plain]{sec:pose-prediction} presented
   the network as an encoder-decoder architecture and explained two encoders (a
   recurrent text encoder and a pre-trained black box encoder) and two decoders
   (a classification decoder and a sequence decoder). This results in four
   possible network architectures.
  
   There is one special case in these four architectures. Since the
   attention-based decoder model (see autoref:sec:decoder) requires access to
   the results from the intermediate results of the encoder, it cannot be used
   in combination with the black box pre-trained text encoder. In that case, the
   decoder falls back to a "regular" recurrent neural network that only uses the
   final output of the encoder to generate its predictions.
  
   The rest of this section shows the results from tweaking the hyperparameters
   of the encoder and decoder networks. In these results, the differences
   between these high-level network architectures will be shown as much as
   possible. In order to do that, we first need to find a metric that can
   compare the results from the two decoders who have different types of
   outputs.
  
** Classification-based vs. \glsentrytext{rnn} label:sec:decoder-comparison
   :PROPERTIES:
   :Effort:   2:00
   :END:

   For the neural network, the loss functions are described in
   autoref:sec:pose-prediction. The results from the two decoders can be
   compared by comparing the generated gestures, when the gestures for the
   classification decoder are generated by concatenating the centroids from the
   preicted clusters. This implies that both the learning model and the
   clustering method are evaluated simultaneously.
    
   Directly comparing the generated gestures with the ground truth data from the
   dataset implies that there is only a single gesture possible (perhaps with
   some noise) for each part of text, which is clearly not the case for humans.
   A better solution would be to use an adversarial network model that has two
   components: one that learns to predict gestures and another that learns what
   kind of gestures humans perceive as being "natural". Sadly this approach was
   determined as out of scope for the current project and is thus recommended as
   one of the next steps in future works towards gesture synthesis
   (autoref:sec:future-work).
    
   In order to compare the two decoders, this leaves us with comparing the
   generated gestures using the position-motion-loss from the sequence decoder.
   This is not a completely fair game since the minimum loss the classification
   decoder can achieve is the loss during the clustering step, which is also
   determined by the amount of clusters created.
    
   #+BEGIN_SRC latex :exports results :results output
     \begin{figure}
       \centering
       \begin{tikzpicture}
         \begin{axis}[
           xlabel = {Step},
           ylabel = {Gesture Loss},
           ymode = log,
           legend entries = {{Sequence Encoder, Classification Decoder}, {Sequence Encoder, Classification Decoder}, {Sequence Encoder, Sequence Decoder}}
         ]
           \addplot table [x=Step, y=Value, col sep=comma] {./img/data-decoder-comparison-classes.csv};
           \addplot table [x=Step, y=Value, col sep=comma] {./img/data-decoder-comparison-classes-sequence-encoder.csv};
           \addplot table [x=Step, y=Value, col sep=comma] {./img/data-decoder-comparison-sequence.csv};
         \end{axis}
       \end{tikzpicture}
       \caption{\label{fig:classification-vs-sequence} The difference in gesture
         error for the classification and sequence decoders.}
     \end{figure}
   #+END_SRC
    
   autoref:fig:classification-vs-sequence shows the results on the validation
   set. The model with the sequence decoder clearly learns to predict the
   gestures while the classification decoder does not learn (in terms of its
   own loss function this decoder immediately starts to overfit the training
   data).

** Survey label:sec:survey

   A better metric to validate this method is with qualitative results from
   people, obtained through a survey. This section desribes the survey that was
   created and its results.

   An online survey was created that includes the following sections:

   1. An introduction with an explanation of the project and the task
   2. An attention checker question, where users are given the simple question
      "what is your gender?" while they are instructed, in a long block of
      text, to answer "orange penguin" instead of the true answer.
   3. Six questions where
      - A video was shown with a NAO robot performing four different animations
        for the same subtitle: the ground truth, a baseline (the default NAO
        animation), the result from the classification-based prediction and the
        result from the sequence-based prediction.
      - Users were asked to rate the human-ness of each animation on a 5-point
        scale (from "stiff, robot-like" to "humanlike")
      - Users were asked which of the animations they prefer
   4. A final checker question that asked the color of the robot's shoulders

   A screenshot of this questionnaire is shown in
   autoref:fig:survey-screenshot.
    
   #+caption: label:fig:survey-screenshot Screenshot of one of the main questions from the survey used to qualitatively test the results of these gesture synthesis methods.
   #+attr_latex: :width 1.2\textwidth,center
   [[file:./img/survey-screenshot.png]]
    
* Conclusions and Future Work label:sec:conclusion label:sec:future-work
  :PROPERTIES:
  :Effort:   2:00
  :END:

** Checklist
   - [ ] Findings: What are the main results?
   - [ ] Take-home message: What should the reader remember?
   - [ ] Future work: what is the outlook?


bibliographystyle:unsrt
\bibliography{/home/marcel/org/bibliography/references.bib}

